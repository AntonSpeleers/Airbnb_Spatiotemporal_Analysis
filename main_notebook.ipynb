{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9cc1a1",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9056c3e3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\pysal\\lib\\cg\\alpha_shapes.py:33: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit\n",
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\pysal\\lib\\cg\\alpha_shapes.py:154: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit\n",
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\pysal\\lib\\cg\\alpha_shapes.py:187: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit\n",
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\pysal\\lib\\cg\\alpha_shapes.py:248: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit\n",
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\pysal\\viz\\mapclassify\\classifiers.py:409: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urbanaccess in c:\\users\\anton\\anaconda3\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: pandana in c:\\users\\anton\\anaconda3\\lib\\site-packages (0.7)\n",
      "Requirement already satisfied: requests>=2.9.1 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from urbanaccess) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from urbanaccess) (1.16.0)\n",
      "Requirement already satisfied: pandas>=0.17.0 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from urbanaccess) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from urbanaccess) (1.24.3)\n",
      "Requirement already satisfied: osmnet>=0.1.4 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from urbanaccess) (0.1.7)\n",
      "Requirement already satisfied: matplotlib>=2.0 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from urbanaccess) (3.7.2)\n",
      "Requirement already satisfied: geopy>=1.11.0 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from urbanaccess) (2.4.1)\n",
      "Requirement already satisfied: pyyaml>=3.11 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from urbanaccess) (6.0)\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from urbanaccess) (1.3.0)\n",
      "Requirement already satisfied: tables>=3.1 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from pandana) (3.8.0)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from geopy>=1.11.0->urbanaccess) (2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from matplotlib>=2.0->urbanaccess) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from matplotlib>=2.0->urbanaccess) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from matplotlib>=2.0->urbanaccess) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from matplotlib>=2.0->urbanaccess) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from matplotlib>=2.0->urbanaccess) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from matplotlib>=2.0->urbanaccess) (10.0.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from matplotlib>=2.0->urbanaccess) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from matplotlib>=2.0->urbanaccess) (2.8.2)\n",
      "Requirement already satisfied: geopandas>=0.11 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from osmnet>=0.1.4->urbanaccess) (0.14.2)\n",
      "Requirement already satisfied: shapely>=1.8 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from osmnet>=0.1.4->urbanaccess) (2.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from pandas>=0.17.0->urbanaccess) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from pandas>=0.17.0->urbanaccess) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from requests>=2.9.1->urbanaccess) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from requests>=2.9.1->urbanaccess) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from requests>=2.9.1->urbanaccess) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from requests>=2.9.1->urbanaccess) (2023.11.17)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from scikit-learn>=0.17.1->urbanaccess) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from scikit-learn>=0.17.1->urbanaccess) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from scikit-learn>=0.17.1->urbanaccess) (2.2.0)\n",
      "Requirement already satisfied: cython>=0.29.21 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from tables>=3.1->pandana) (3.0.8)\n",
      "Requirement already satisfied: numexpr>=2.6.2 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from tables>=3.1->pandana) (2.8.4)\n",
      "Requirement already satisfied: blosc2~=2.0.0 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from tables>=3.1->pandana) (2.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\anton\\anaconda3\\lib\\site-packages (from tables>=3.1->pandana) (8.0.0)\n",
      "Requirement already satisfied: msgpack in c:\\users\\anton\\anaconda3\\lib\\site-packages (from blosc2~=2.0.0->tables>=3.1->pandana) (1.0.3)\n",
      "Requirement already satisfied: fiona>=1.8.21 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from geopandas>=0.11->osmnet>=0.1.4->urbanaccess) (1.9.5)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from geopandas>=0.11->osmnet>=0.1.4->urbanaccess) (3.6.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from fiona>=1.8.21->geopandas>=0.11->osmnet>=0.1.4->urbanaccess) (22.1.0)\n",
      "Requirement already satisfied: click~=8.0 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from fiona>=1.8.21->geopandas>=0.11->osmnet>=0.1.4->urbanaccess) (8.0.4)\n",
      "Requirement already satisfied: click-plugins>=1.0 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from fiona>=1.8.21->geopandas>=0.11->osmnet>=0.1.4->urbanaccess) (1.1.1)\n",
      "Requirement already satisfied: cligj>=0.5 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from fiona>=1.8.21->geopandas>=0.11->osmnet>=0.1.4->urbanaccess) (0.7.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\anton\\anaconda3\\lib\\site-packages (from fiona>=1.8.21->geopandas>=0.11->osmnet>=0.1.4->urbanaccess) (68.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\anton\\anaconda3\\lib\\site-packages (from click~=8.0->fiona>=1.8.21->geopandas>=0.11->osmnet>=0.1.4->urbanaccess) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "#import all packages needed \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "from datetime import datetime \n",
    "import ast\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import geopandas as gpd\n",
    "from scipy.stats import ttest_ind\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from pysal.lib.weights import KNN\n",
    "from pysal.explore import esda\n",
    "!pip install urbanaccess pandana\n",
    "\n",
    "\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Data/2. airbnb_data/Paris/Paris all detailed listings/Paris_Q1_detailed_listings.xlsx'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "dataframe1= pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b29ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quickly looking with which data types we are dealing\n",
    "print(dataframe1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64073ce2",
   "metadata": {},
   "source": [
    "# Data cleaning listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2841a740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decided which columns will not be relevant for the first initial analysis\n",
    "columns_to_drop =[\"name\",\"description\",\"host_location\",\"host_thumbnail_url\",\"host_name\",\"bathrooms\",\n",
    "                  \"listing_url\",\"scrape_id\",\"last_scraped\", \"host_picture_url\",\"host_url\", \"host_has_profile_pic\", \n",
    "                  \"host_verifications\",\"source\",\"calendar_last_scraped\",\"license\",\"picture_url\",\"host_about\",\n",
    "                             \"neighbourhood\",\"neighbourhood_group_cleansed\",\"minimum_minimum_nights\",\n",
    "                             \"maximum_minimum_nights\",\"minimum_maximum_nights\",\"maximum_maximum_nights\",\n",
    "                             \"minimum_nights_avg_ntm\",\"maximum_nights_avg_ntm\",\"calendar_updated\",\n",
    "                             \"neighborhood_overview\",\"host_neighbourhood\", \"host_acceptance_rate\"]\n",
    "dataframe1 = dataframe1.drop(columns_to_drop, axis=1)\n",
    "\n",
    "#solving trailing white space problem\n",
    "string_columns = dataframe1.select_dtypes(include='object').columns.tolist()\n",
    "for i in string_columns:\n",
    "    dataframe1[i] = dataframe1[i].str.strip()\n",
    "\n",
    "#missing values have always been checked during this cleaning with the function: print(dataframe1.isnull().mean()) \n",
    "\n",
    "#host_repsonse_time, filling in the empty ones with \"unknown\"\n",
    "dataframe1.host_response_time.fillna(\"unknown\", inplace=True)\n",
    "dataframe1.host_response_time.value_counts(normalize=True)\n",
    "\n",
    "#adapting the host_response_rate column to better fit\n",
    "# Removing the % sign from the host_response_rate string and converting to an integer\n",
    "dataframe1.host_response_rate = dataframe1.host_response_rate.str[:-1].astype('float64')\n",
    "# Bin into four categories\n",
    "dataframe1.host_response_rate = pd.cut(dataframe1.host_response_rate, bins=[0, 50, 90, 99, 100], labels=['0-49%', '50-89%', '90-99%', '100%'], include_lowest=True)\n",
    "# Converting to string\n",
    "dataframe1.host_response_rate = dataframe1.host_response_rate.astype('str')\n",
    "# Replace nulls with 'unknown'\n",
    "dataframe1.host_response_rate.replace('nan', 'unknown', inplace=True)\n",
    "\n",
    "#these rows do not have that big of a missing value amount so we just delete the missing value rows here\n",
    "col = [\"host_since\",\"host_identity_verified\",\"host_listings_count\",\"host_total_listings_count\",\"host_is_superhost\"]\n",
    "for column in col:\n",
    "    dataframe1.dropna(subset=[column], inplace=True)\n",
    "\n",
    "# Category counts\n",
    "dataframe1.host_response_rate.value_counts()\n",
    "#fill out all NaN within string columns with ''\n",
    "for column in string_columns:\n",
    "    dataframe1[column] = dataframe1[column].fillna(\"\")   \n",
    "\n",
    "#fixing the \"bathroom_text\" column to only numbers and renaming it\n",
    "def extract_numeric(value):\n",
    "    numeric_part = re.search(r'\\d+\\.\\d+|\\d+', str(value))\n",
    "    return float(numeric_part.group()) if numeric_part else None\n",
    "dataframe1['bathrooms_text'] = dataframe1['bathrooms_text'].apply(extract_numeric)\n",
    "dataframe1.rename(columns={'bathrooms_text': 'bathrooms'}, inplace=True)\n",
    "\n",
    "#adding a column with the information if a listing has a review or not\n",
    "dataframe1['zero_reviews'] = dataframe1['number_of_reviews'] == 0\n",
    "    \n",
    "#delete those that have no information about both beds and bedrooms\n",
    "dataframe1.dropna(subset=['beds', 'bedrooms'], how='all', inplace=True)\n",
    "\n",
    "#dropping those with +4 bedrooms (outliers) and no information about bedroom and between 1-4 beds is a studio so 0 bedrooms\n",
    "dataframe1.loc[(dataframe1['bedrooms'].isnull()) & (dataframe1['beds'].between(1, 4)), 'bedrooms'] = 0\n",
    "dataframe1.drop(dataframe1[(dataframe1['bedrooms'].isnull()) & (dataframe1['beds'] > 4)].index, inplace=True)\n",
    "\n",
    "#fill in all the other empty values with the amount of bedrooms \n",
    "dataframe1['beds'].fillna(dataframe1['bedrooms'], inplace=True)\n",
    "\n",
    "#dropping those that still have no bedroom amount \n",
    "dataframe1.dropna(subset=[\"bedrooms\"], inplace=True)\n",
    "\n",
    "#dropping those that still have no bathroom amount \n",
    "dataframe1.dropna(subset=[\"bathrooms\"], inplace=True)\n",
    "\n",
    "dataframe1['first_review'] = pd.to_datetime(dataframe1['first_review']) \n",
    "# Calculating the number of days\n",
    "dataframe1['days_since_first_review'] = (datetime(2024, 1, 20) - dataframe1['first_review']).dt.days\n",
    "# Printing descriptives\n",
    "#dataframe1.hist(['days_since_first_review'], figsize=(15,5), bins=[0, 1*365, 2*365, 3*365, 4*365, 5*365, 6*365, 7*365, 8*365, 10*365, 11*365]), 9*365\n",
    "#DOING Binning time since last review\n",
    "dataframe1['days_since_first_first_review'] = pd.qcut(dataframe1['days_since_first_review'], q=5,\n",
    "                              labels=['Extremely active', 'Very active', 'Active', 'Inactive', 'Slumbering'])\n",
    "\n",
    "dataframe1['last_review'] = pd.to_datetime(dataframe1['last_review']) \n",
    "# Calculating the number of days\n",
    "dataframe1['days_since_last_review'] = (datetime(2024, 1, 20) - dataframe1['last_review']).dt.days\n",
    "# Printing descriptives\n",
    "#dataframe1.hist(['days_since_last_review'], figsize=(15,5), bins=[0, 1*365, 2*365, 3*365, 4*365, 5*365, 6*365, 7*365, 8*365, 10*365, 11*365]), 9*365\n",
    "#DOING Binning time since last review\n",
    "dataframe1['days_since_last_last_review'] = pd.qcut(dataframe1['days_since_last_review'], q=5,\n",
    "                              labels=['Extremely active', 'Very active', 'Active', 'Inactive', 'Slumbering'])\n",
    "\n",
    "dataframe1['host_since'] = pd.to_datetime(dataframe1['host_since']) \n",
    "# Calculating the number of days\n",
    "dataframe1['days_since_host'] = (datetime(2024, 1, 20) - dataframe1['host_since']).dt.days\n",
    "# Printing descriptives\n",
    "#dataframe1.hist(['days_since_host'], figsize=(15,5), bins=[0, 1*365, 2*365, 3*365, 4*365, 5*365, 6*365, 7*365, 8*365, 10*365, 11*365]), 9*365\n",
    "#DOING Binning time since last review\n",
    "dataframe1['days_since_host_host'] = pd.qcut(dataframe1['days_since_host'], q=5,\n",
    "                              labels=['Extremely active', 'Very active', 'Active', 'Inactive', 'Slumbering'])\n",
    "\n",
    "#Simplifying the property_types in to 4 categories\n",
    "dataframe1.property_type.replace({\n",
    "    'Barn': 'House',\n",
    "    'Boat': 'Other',\n",
    "    'Bus': 'Other',\n",
    "    'Camper/RV': 'Other',\n",
    "    'Casa particular': 'House',\n",
    "    'Cave': 'Other',\n",
    "    'Dome': 'Other',\n",
    "    'Earthen home': 'House',\n",
    "    'Entire bed and breakfast': 'Hotel',    \n",
    "    'Entire bungalow': 'House',\n",
    "    'Entire condo': 'Apartmen',\n",
    "    'Entire guesthouse': 'House',\n",
    "    'Entire home': 'House',\n",
    "    'Entire guest suite': 'Apartment',\n",
    "    'Entire rental unit': 'Apartment',\n",
    "    'Entire loft': 'Apartment',\n",
    "    'Entire home/apt': 'House',\n",
    "    'Entire place': 'House',\n",
    "    'Entire serviced apartment': 'Apartment',\n",
    "    'Entire townhouse': 'House',\n",
    "    'Entire villa': 'House',\n",
    "    'Entire vacation home': 'House',\n",
    "    'Floor': 'Other',\n",
    "    'Houseboat': 'Other',\n",
    "    'Private room': 'Apartment',\n",
    "    'Island': 'Other',\n",
    "    'Private room in bed and breakfast': 'Hotel',    \n",
    "    'Private room in boat': 'Other',\n",
    "    'Private room in casa particular': 'House',\n",
    "    'Private room in condo': 'Apartment',\n",
    "    'Private room in guest suite': 'Apartment',\n",
    "    'Private room in earthen home': 'House',\n",
    "    'Private room in home': 'House',\n",
    "    'Private room in guesthouse': 'House',\n",
    "    'Private room in loft': 'Apartment',\n",
    "    'Private room in hostel': 'Hotel',\n",
    "    'Private room in rental unit': 'Apartment',\n",
    "    'Private room in townhouse': 'House',\n",
    "    'Private room in tiny home': 'House',\n",
    "    'Private room in serviced apartment': 'Apartment',\n",
    "    'Room in bed and breakfast': 'Hotel',\n",
    "    'Private room in villa': 'House',\n",
    "    'Room in serviced apartment': 'Apartment',\n",
    "    'Room in boutique hotel': 'Hotel',\n",
    "    'Room in hotel': 'Hotel',\n",
    "    'Room in hostel': 'Hotel',\n",
    "    'Shared room in bed and breakfast': 'Hotel',\n",
    "    'Shared room in boutique hotel': 'Hotel',\n",
    "    'Shared room in cabin': 'Other',\n",
    "    'Shared room in boat': 'Other',\n",
    "    'Shared room in condo': 'Apartment',\n",
    "    'Shared room in farm stay': 'Other',\n",
    "    'Shared room in guesthouse': 'House',\n",
    "    'Shared room in casa particular': 'House',\n",
    "    'Shared room in ice dome': 'Other',\n",
    "    'Shared room in home': 'House',\n",
    "    'Shared room in hostel': 'Hotel',\n",
    "    'Shared room in hotel': 'Hotel',\n",
    "    'Shared room in rental unit': 'Apartment',\n",
    "    'Tiny home': 'House',\n",
    "    'Shared room in loft': 'Apartment',\n",
    "    'Shared room in townhouse': 'House',\n",
    "    'Shared room in tiny home': 'House',\n",
    "    }, inplace=True)\n",
    "\n",
    "# Replacing other categories with 'other'\n",
    "dataframe1.loc[~dataframe1.property_type.isin(['House', 'Apartment','Hotel']), 'property_type'] = 'Other'\n",
    "#dataframe1['property_type'].value_counts()\n",
    "\n",
    "#changing the name from neighbourhoud_cleansed to neighbourhood\n",
    "dataframe1.rename(columns={'neighbourhood_cleansed': 'neighbourhood'}, inplace=True)\n",
    "\n",
    "#convert the necessary columns to a boolean type, which is easier to use\n",
    "columns_to_convert = ['host_is_superhost', 'instant_bookable', 'host_identity_verified',\"has_availability\"] \n",
    "for column in columns_to_convert:\n",
    "    dataframe1[column] = dataframe1[column].replace({'f': False, 't': True}).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "066f3e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the string representation of lists to actual lists\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a1419ad",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Iron', 'Room-darkening shades', 'Safe', 'Blender', 'Outdoor dining area', 'Cleaning products', 'sound system', 'Elevator', 'toys children', 'bbq', 'conditioner', 'Security cameras on property', 'Self check-in', 'Free dryer – In unit', 'oven', 'High chair', 'Indoor fireplace', 'Free washer – In unit', 'Dryer', 'Pets allowed', 'Baking sheet', 'exercise equipment', 'Heating', 'Hair dryer', 'Cleaning available during stay', 'Outdoor furniture', 'Toaster', 'Bed linens', 'Hangers', 'Microwave', 'pool', 'tv', 'Piano', 'Smoke alarm', 'broadcast', 'First aid kit', 'Rice maker', 'Dishwasher', 'Smoking allowed', 'Lockbox', 'Babysitter recommendations', 'Drying rack for clothing', 'Wine glasses', 'stove', 'Dishes and silverware', 'Freezer', 'Private entrance', 'Dining table', 'coffee', 'Patio or balcony', 'Portable fans', 'Board games', 'Ethernet connection', 'Sound system', 'Central heating', 'game console', 'hot tub', 'Bathtub', 'wifi', 'soap', 'clothing storage', 'backyard', 'Fire extinguisher', 'Air conditioning', 'Cooking basics', 'Extra pillows and blankets', 'Washer', 'Private patio or balcony', 'gym', 'shampoo', 'Hot water kettle', 'Hot water', 'view', 'refrigerator', 'Host greets you', 'Lock on bedroom door', 'Portable heater', 'Building staff', 'Essentials', 'parking', 'Dedicated workspace', 'Luggage dropoff allowed', 'Carbon monoxide alarm', 'Breakfast', 'Books and reading material', 'Single level home', 'Laundromat nearby', 'Mini fridge', 'Radiant heating', 'Bread maker', 'Long term stays allowed', 'Shower gel', 'Kitchen', 'crib'}\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "#making a unique list out of all the different kind of amenities that there are\n",
    "unique_items_set = set.union(*dataframe1[\"amenities\"].apply(set))\n",
    "print(unique_items_set)\n",
    "print(len(unique_items_set)) #first we come to  5829 different amenities\n",
    "#the one that is shown here is taken after the adjustments you can see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca1b0f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with the code down below we will simplify the amenities to reduce the amount of amenities\n",
    "\n",
    "oven_items_set = {item for item in unique_items_set if 'oven' in item.lower()}\n",
    "def replace_oven_items(item_list):\n",
    "    return ['oven' if item in oven_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_oven_items)\n",
    "\n",
    "soap_items_set = {item for item in unique_items_set if 'soap' in item.lower()}\n",
    "def replace_soap_items(item_list):\n",
    "    return ['soap' if item in soap_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_soap_items)\n",
    "\n",
    "shampoo_items_set = {item for item in unique_items_set if 'shampoo' in item.lower()}\n",
    "def replace_shampoo_items(item_list):\n",
    "    return ['shampoo' if item in shampoo_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_shampoo_items)\n",
    "\n",
    "wifi_items_set = {item for item in unique_items_set if 'wifi' in item.lower()}\n",
    "def replace_wifi_items(item_list):\n",
    "    return ['wifi' if item in wifi_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_wifi_items)\n",
    "\n",
    "special_items_set = {item for item in unique_items_set if any(keyword in item for keyword in ['Netflix', 'Disney+', 'Amazon Prime'])}\n",
    "def replace_and_add_broadcast(item_list):\n",
    "    # Replace items from special_items_set with 'Broadcast'\n",
    "    item_list = ['broadcast' if item in special_items_set else item for item in item_list]\n",
    "    # Add 'TV' to the list if modified\n",
    "    modified = any(item == 'broadcast' for item in item_list)\n",
    "    if modified:\n",
    "        item_list.append('TV')\n",
    "    return item_list\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_and_add_broadcast)\n",
    "\n",
    "tv_items_set = {item for item in unique_items_set if 'tv' in item.lower()}\n",
    "def replace_tv_items(item_list):\n",
    "    return ['tv' if item in tv_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_tv_items)\n",
    "\n",
    "ref_items_set = {item for item in unique_items_set if 'refrigerator' in item.lower()}\n",
    "def replace_ref_items(item_list):\n",
    "    return ['refrigerator' if item in ref_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_ref_items)\n",
    "\n",
    "coffee_items_set = {item for item in unique_items_set if 'coffee' in item.lower()}\n",
    "def replace_coffee_items(item_list):\n",
    "    return ['coffee' if item in coffee_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_coffee_items)\n",
    "\n",
    "sound_items_set = {item for item in unique_items_set if any(keyword in item for keyword in ['sound system', 'Bluetooth'])}\n",
    "def replace_sound_items(item_list):\n",
    "    item_list = ['sound system' if item in sound_items_set else item for item in item_list]\n",
    "    return item_list\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_sound_items)\n",
    "\n",
    "stove_items_set = {item for item in unique_items_set if 'stove' in item.lower()}\n",
    "def replace_stove_items(item_list):\n",
    "    return ['stove' if item in stove_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_stove_items)\n",
    "\n",
    "cond_items_set = {item for item in unique_items_set if 'conditioner' in item.lower()}\n",
    "def replace_cond_items(item_list):\n",
    "    return ['conditioner' if item in cond_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_cond_items)\n",
    "\n",
    "park_items_set = {item for item in unique_items_set if 'parking' in item.lower()}\n",
    "def replace_park_items(item_list):\n",
    "    return ['parking' if item in park_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_park_items)\n",
    "\n",
    "clothing_items_set = {item for item in unique_items_set if 'clothing storage' in item.lower()}\n",
    "def replace_clothing_items(item_list):\n",
    "    return ['clothing storage' if item in clothing_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_clothing_items)\n",
    "\n",
    "exercise_items_set = {item for item in unique_items_set if 'exercise equipment' in item.lower()}\n",
    "def replace_exercise_items(item_list):\n",
    "    return ['exercise equipment' if item in exercise_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_exercise_items)\n",
    "\n",
    "children_items_set = {item for item in unique_items_set if 'children' in item.lower()}\n",
    "def replace_children_items(item_list):\n",
    "    return ['toys children' if item in children_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_children_items)\n",
    "\n",
    "bbq_items_set = {item for item in unique_items_set if 'bbq' in item.lower()}\n",
    "def replace_bbq_items(item_list):\n",
    "    return ['bbq' if item in bbq_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_bbq_items)\n",
    "\n",
    "pool_items_set = {item for item in unique_items_set if 'pool' in item.lower()}\n",
    "def replace_pool_items(item_list):\n",
    "    return ['pool' if item in pool_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_pool_items)\n",
    "\n",
    "hot_items_set = {item for item in unique_items_set if 'hot tub' in item.lower()}\n",
    "def replace_hot_items(item_list):\n",
    "    return ['hot tub' if item in hot_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_hot_items)\n",
    "\n",
    "backyard_items_set = {item for item in unique_items_set if 'backyard' in item.lower()}\n",
    "def replace_backyard_items(item_list):\n",
    "    return ['backyard' if item in backyard_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_backyard_items)\n",
    "\n",
    "gym_items_set = {item for item in unique_items_set if 'gym' in item.lower()}\n",
    "def replace_gym_items(item_list):\n",
    "    return ['gym' if item in gym_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_gym_items)\n",
    "\n",
    "view_items_set = {item for item in unique_items_set if 'view' in item.lower()}\n",
    "def replace_view_items(item_list):\n",
    "    return ['view' if item in view_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_view_items)\n",
    "\n",
    "crib_items_set = {item for item in unique_items_set if 'crib' in item.lower()}\n",
    "def replace_crib_items(item_list):\n",
    "    return ['crib' if item in crib_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_crib_items)\n",
    "\n",
    "gc_items_set = {item for item in unique_items_set if 'game console' in item.lower()}\n",
    "def replace_gc_items(item_list):\n",
    "    return ['game console' if item in gc_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_gc_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8adfd144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#last step to clean the amenities is removing the ones that do not appear 1000 times or more in the column\n",
    "flat_list = [word for sublist in dataframe1[\"amenities\"] for word in sublist]\n",
    "word_counts = Counter(flat_list)\n",
    "filtered_word_set = {word for word, count in word_counts.items() if count < 1000}\n",
    "common_elements = list(filtered_word_set & unique_items_set)\n",
    "\n",
    "def remove_common_elements(item_list):\n",
    "    return [item for item in item_list if item not in common_elements]\n",
    "dataframe1['amenities'] = dataframe1['amenities'].apply(remove_common_elements)\n",
    "\n",
    "#only 94 unique items left, which is an acceptable amount and now we can make columns out of these ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8c35ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making new columns for the amenities indivually with a 0 or 1 inside of them \n",
    "for item in unique_items_set:\n",
    "    dataframe1[item] = dataframe1[\"amenities\"].apply(lambda x: int(item in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6afc7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will be able to remove the 'amenities' column\n",
    "dataframe1 = dataframe1.drop('amenities', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbd53e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of amenities for each listing and make this a new column\n",
    "dataframe1['total_amenities'] = dataframe1.iloc[:, 51:-1].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a5af036",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of listings with 1.0 bedroom(s) is: 62.33%\n",
      "The percentage of listings with 0.0 bedroom(s) is: 15.70%\n",
      "The percentage of listings with 2.0 bedroom(s) is: 15.69%\n",
      "The percentage of listings with 3.0 bedroom(s) is: 4.81%\n",
      "The percentage of listings with 4.0 bedroom(s) is: 1.17%\n",
      "The percentage of listings with 5.0 bedroom(s) is: 0.22%\n",
      "The percentage of listings with 6.0 bedroom(s) is: 0.06%\n",
      "The percentage of listings with 7.0 bedroom(s) is: 0.02%\n",
      "The percentage of listings with 50.0 bedroom(s) is: 0.01%\n",
      "The percentage of listings with 1.0 bathroom(s) is: 86.00%\n",
      "The percentage of listings with 2.0 bathroom(s) is: 6.14%\n",
      "The percentage of listings with 1.5 bathroom(s) is: 5.38%\n",
      "The percentage of listings with 2.5 bathroom(s) is: 1.11%\n",
      "The percentage of listings with 3.0 bathroom(s) is: 0.77%\n",
      "The percentage of listings with 0.0 bathroom(s) is: 0.22%\n",
      "The percentage of listings with 3.5 bathroom(s) is: 0.15%\n",
      "The percentage of listings with 4.0 bathroom(s) is: 0.11%\n",
      "The percentage of listings with 5.0 bathroom(s) is: 0.05%\n",
      "The percentage of listings with 4.5 bathroom(s) is: 0.03%\n",
      "The percentage of listings with 6.0 bathroom(s) is: 0.02%\n",
      "The percentage of listings with 50.0 bathroom(s) is: 0.01%\n",
      "The percentage of listings with 1.0 bed(s) is: 55.34%\n",
      "The percentage of listings with 2.0 bed(s) is: 29.34%\n",
      "The percentage of listings with 3.0 bed(s) is: 9.23%\n",
      "The percentage of listings with 4.0 bed(s) is: 3.76%\n",
      "The percentage of listings with 5.0 bed(s) is: 1.39%\n",
      "The percentage of listings with 6.0 bed(s) is: 0.57%\n",
      "The percentage of listings with 7.0 bed(s) is: 0.18%\n",
      "The percentage of listings with 8.0 bed(s) is: 0.12%\n",
      "The percentage of listings with 9.0 bed(s) is: 0.05%\n",
      "The percentage of listings with 10.0 bed(s) is: 0.02%\n",
      "The percentage of listings with 11.0 bed(s) is: 0.01%\n"
     ]
    }
   ],
   "source": [
    "#with the following code we are going to eliminate the outliers regarding the columns: bedrooms, beds and bathrooms\n",
    "\n",
    "x = \"bedrooms\"\n",
    "\n",
    "# Calculate the percentage of each bedroom count relative to all listings\n",
    "bedrooms_percentage = dataframe1[x].value_counts(normalize=True) * 100\n",
    "\n",
    "# Identify bedroom counts with percentage <= 0.01%\n",
    "bedrooms_to_drop = bedrooms_percentage[bedrooms_percentage <= 0.01].index\n",
    "\n",
    "# Drop rows with bedrooms counts <= 0.01%\n",
    "dataframe1 = dataframe1[~dataframe1[x].isin(bedrooms_to_drop)]\n",
    "\n",
    "# Display the percentage of each bedroom count after filtering\n",
    "filtered_bedroom_percentage = dataframe1[x].value_counts(normalize=True) * 100\n",
    "for bedrooms, percentage in filtered_bedroom_percentage.items():\n",
    "    print(f'The percentage of listings with {bedrooms} bedroom(s) is: {percentage:.2f}%')\n",
    "    \n",
    "y = \"bathrooms\"\n",
    "\n",
    "# Calculate the percentage of each bathroom count relative to all listings\n",
    "bathrooms_percentage = dataframe1[y].value_counts(normalize=True) * 100\n",
    "\n",
    "# Identify bathroom counts with percentage <= 0.01%\n",
    "bathrooms_to_drop = bathrooms_percentage[bathrooms_percentage <= 0.01].index\n",
    "\n",
    "# Drop rows with bathrooms counts <= 0.01%\n",
    "dataframe1 = dataframe1[~dataframe1[y].isin(bathrooms_to_drop)]\n",
    "\n",
    "# Display the percentage of each bathroom count after filtering\n",
    "filtered_bathroom_percentage = dataframe1[y].value_counts(normalize=True) * 100\n",
    "for bathrooms, percentage in filtered_bathroom_percentage.items():\n",
    "    print(f'The percentage of listings with {bathrooms} bathroom(s) is: {percentage:.2f}%')\n",
    "    \n",
    "z = \"beds\"\n",
    "\n",
    "# Calculate the percentage of each bed count relative to all listings\n",
    "beds_percentage = dataframe1[z].value_counts(normalize=True) * 100\n",
    "\n",
    "# Identify bed counts with percentage <= 0.01%\n",
    "beds_to_drop = beds_percentage[beds_percentage <= 0.01].index\n",
    "\n",
    "# Drop rows with bed counts <= 0.01%\n",
    "dataframe1 = dataframe1[~dataframe1[z].isin(beds_to_drop)]\n",
    "\n",
    "# Display the percentage of each bed count after filtering\n",
    "filtered_beds_percentage = dataframe1[z].value_counts(normalize=True) * 100\n",
    "for beds, percentage in filtered_beds_percentage.items():\n",
    "    print(f'The percentage of listings with {beds} bed(s) is: {percentage:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ca898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading the cleaned_dataset\n",
    "dataframe1.to_excel('cleaned_dataset.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c2af1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/cleaned_dataset.xlsx'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "dataframe1= pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ac62b5",
   "metadata": {},
   "source": [
    "# Data cleaning Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7283fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "#reading in the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Data/2. airbnb_data/Paris/Q1 airbnb_data Paris/calendar/calendar.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2976d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['price'] = data['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "data['adjusted_price'] = data['adjusted_price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "data['date']=pd.to_datetime(data['date'])\n",
    "data['weekday'] = pd.Series(data.date).dt.dayofweek\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c896ac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the missing values from the dataset after seeing that there are not that many missing values\n",
    "data.dropna(inplace=True)\n",
    "print(data.isnull().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2476941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'price' and 'adjusted_price' are always the same\n",
    "are_equal = (data['price'] == data['adjusted_price']).all()\n",
    "if are_equal:\n",
    "    print(\"The 'price' and 'adjusted_price' columns always have the same values.\")\n",
    "else:\n",
    "    print(\"There are differences between 'price' and 'adjusted_price' values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c41084",
   "metadata": {},
   "source": [
    "# Data exploration Calendar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdc39d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import csv\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Taking average/median values as well as plotting\n",
    "date = []\n",
    "avg_price = []\n",
    "median_price = []\n",
    "avg_adjusted_price = []\n",
    "median_adjusted_price = []\n",
    "\n",
    "for i in data['date'].unique():\n",
    "    date.append(i)\n",
    "    avg_price.append(data[data['date'] == i]['price'].mean())\n",
    "    median_price.append(data[data['date'] == i]['price'].median())\n",
    "    avg_adjusted_price.append(data[data['date'] == i]['adjusted_price'].mean())\n",
    "    median_adjusted_price.append(data[data['date'] == i]['adjusted_price'].median())\n",
    "\n",
    "plt.plot(range(len(avg_price)), avg_price, label=\"Average Price\")\n",
    "plt.plot(range(len(avg_price)), median_price, color='red', label=\"Median Price\")\n",
    "plt.plot(range(len(avg_adjusted_price)), avg_adjusted_price, label=\"Average Adjusted Price\")\n",
    "plt.plot(range(len(avg_adjusted_price)), median_adjusted_price, color='green', label=\"Median Adjusted Price\")\n",
    "\n",
    "plt.ylabel('Price($)')\n",
    "plt.xlabel('Day of Q1 2023 - Q1 2024')\n",
    "plt.title('Average and Median Price of Q1 2023 - Q1 2024 AirBnB Listings Paris')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1672079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing the same but only for the first 70 days\n",
    "plt.plot(range(len(avg_price[0:70])), avg_price[0:70], label=\"Average Price\")\n",
    "plt.plot(range(len(avg_price[0:70])), median_price[0:70], color='red', label=\"Median Price\")\n",
    "plt.plot(range(len(avg_adjusted_price[0:70])), avg_adjusted_price[0:70], label=\"Average Adjusted Price\")\n",
    "plt.plot(range(len(avg_adjusted_price[0:70])), median_adjusted_price[0:70], color='green', label=\"Median Adjusted Price\")\n",
    "\n",
    "plt.ylabel('Price($)')\n",
    "plt.xlabel('Day of Q1 2023 - Q1 2024')\n",
    "plt.title('Average and Median Price of First 70 Days of AirBnB Listings')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968e8f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at which day is the most expensive on average and median\n",
    "daily_avg_price = []\n",
    "daily_median_price = []\n",
    "b = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "for i in range(7):\n",
    "    daily_avg_price.append(data[data['weekday'] == i]['price'].mean())\n",
    "    daily_median_price.append(data[data['weekday'] == i]['price'].median())\n",
    "\n",
    "plt.bar(range(len(daily_avg_price)), daily_avg_price, width=1 / 1.5, label=\"Average Price\")\n",
    "plt.bar(range(len(daily_median_price)), daily_median_price, width=1 / 1.5, color='red', label=\"Median Price\")\n",
    "\n",
    "plt.xticks(range(len(daily_avg_price)), b)\n",
    "plt.ylabel('Price($)')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.title('Average and Median Price per Day of the Week')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for i in range(7):\n",
    "    avg_price = data[data['weekday'] == i]['price'].mean()\n",
    "    median_price = data[data['weekday'] == i]['price'].median()\n",
    "    \n",
    "    daily_avg_price.append(avg_price)\n",
    "    daily_median_price.append(median_price)\n",
    "\n",
    "    print(f\"{b[i]} - Average Price: ${avg_price:.2f}, Median Price: ${median_price:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f9de38",
   "metadata": {},
   "source": [
    "# Data exploration listings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc94cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distributions of the review ratings columns\n",
    "variables_to_plot = list(dataframe1.columns[dataframe1.columns.str.startswith(\"review_scores\") == True])\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "for i, var_name in enumerate(variables_to_plot):\n",
    "    ax = fig.add_subplot(3,3,i+1)\n",
    "    dataframe1[var_name].hist(bins=10,ax=ax)\n",
    "    ax.set_title(var_name)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#out of these we see that most of them are rated quite positive, with the biggest amount around 4-5/5 stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a3d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Paris neighbourhood  GeoJSON file as a dataframe in geopandas\n",
    "map_dataframe1 = gpd.read_file('C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Data/2. airbnb_data/Paris/Q1 airbnb_data Paris/neighbourhoods.geojson')\n",
    "map_dataframe1.drop('neighbourhood_group', axis = 1, inplace = True)\n",
    "map_dataframe1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d97c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the characteristics for the following plot\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.rcParams['figure.figsize'] = 280, 280\n",
    "\n",
    "\n",
    "# Creating a dataframe of listing counts and median price by neighbourhood\n",
    "neighbourhood_dataframe1 = pd.DataFrame(dataframe1.groupby('neighbourhood').size())\n",
    "neighbourhood_dataframe1.rename(columns={0: 'number_of_listings'}, inplace=True)\n",
    "neighbourhood_dataframe1['median_price'] = dataframe1.groupby('neighbourhood').price.median().values\n",
    "neighbourhood_dataframe1['mean_price'] = dataframe1.groupby('neighbourhood').price.mean().values\n",
    "\n",
    "# Putting the dataframes together\n",
    "neighbourhood_map_dataframe1 = map_dataframe1.set_index('neighbourhood').join(neighbourhood_dataframe1)\n",
    "                  \n",
    "# Plotting the number of listings in each neighbourhood\n",
    "fig1, ax1 = plt.subplots(1, figsize=(15, 6)) #deciding on plot size\n",
    "neighbourhood_map_dataframe1.plot(column='number_of_listings', cmap='Reds', ax=ax1, rasterized=True) \n",
    "#rasterized to makes it easier for big data sets + ax1 earlier defined\n",
    "\n",
    "ax1.axis('off') #disabling the axis components, including axis labels, ticks, and the frame surrounding the plot\n",
    "ax1.set_title('Amount of listings per neighbourhood in Paris', fontsize=14)\n",
    "cax1 = fig1.add_axes([0.9, 0.1, 0.03, 0.8]) # Adjusted the position and size as needed for the side axis [left, bottom, width, height]\n",
    "#ScalarMappable to make a color map + norm, to normalize the coloring within the graph \n",
    "sm1 = plt.cm.ScalarMappable(cmap='Reds', norm=plt.Normalize(vmin=0, vmax=9000))\n",
    "sm1._A = [] # The primary reason for doing this is to provide an empty array that will later be populated with the data range when the plot is created. The colorbar uses this array to decide the color scaling for the colormap.\n",
    "cbar1 = fig1.colorbar(sm1, cax=cax1) \n",
    "plt.show()\n",
    "\n",
    "# Plotting the median price of listings in each neighbourhood\n",
    "fig2, ax2 = plt.subplots(1, figsize=(15, 6))\n",
    "neighbourhood_map_dataframe1.plot(column='median_price', cmap='Reds', ax=ax2)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Median price per neighbourhood in Paris', fontsize=14)\n",
    "cax2 = fig2.add_axes([0.9, 0.1, 0.03, 0.8])  # Adjusted the position and size as needed for the side axis [left, bottom, width, height]\n",
    "#ScalarMappable to make a color map + norm, to normalize the coloring within the graph \n",
    "sm2 = plt.cm.ScalarMappable(cmap='Reds', norm=plt.Normalize(vmin=min(neighbourhood_map_dataframe1.median_price), vmax=max(neighbourhood_map_dataframe1.median_price))) \n",
    "sm2._A = [] # The primary reason for doing this is to provide an empty array that will later be populated with the data range when the plot is created. The colorbar uses this array to decide the color scaling for the colormap.\n",
    "cbar2 = fig2.colorbar(sm2, cax=cax2)\n",
    "plt.show()\n",
    "\n",
    "# Plotting the mean price of listings in each neighbourhood\n",
    "fig3, ax3 = plt.subplots(1, figsize=(15, 6))\n",
    "neighbourhood_map_dataframe1.plot(column='mean_price', cmap='Reds', ax=ax3)\n",
    "ax3.axis('off')\n",
    "ax3.set_title('Mean price per neighbourhood in Paris', fontsize=14)\n",
    "cax3 = fig3.add_axes([0.9, 0.1, 0.03, 0.8])  # Adjusted the position and size as needed for the side axis [left, bottom, width, height]\n",
    "#ScalarMappable to make a color map + norm, to normalize the coloring within the graph \n",
    "sm3 = plt.cm.ScalarMappable(cmap='Reds', norm=plt.Normalize(vmin=min(neighbourhood_map_dataframe1.mean_price), vmax=max(neighbourhood_map_dataframe1.mean_price))) \n",
    "sm3._A = [] # The primary reason for doing this is to provide an empty array that will later be populated with the data range when the plot is created. The colorbar uses this array to decide the color scaling for the colormap.\n",
    "cbar3 = fig3.colorbar(sm3, cax=cax3)\n",
    "plt.show()\n",
    "\n",
    "print(neighbourhood_dataframe1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad82985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot number of listings and median price in one bar plot\n",
    "plt.rcParams['font.size'] = 15\n",
    "plt.rcParams['figure.figsize'] = 25, 5\n",
    "selected_columns = ['number_of_listings', 'median_price']\n",
    "neighbourhood_dataframe1_selected = neighbourhood_dataframe1[selected_columns]\n",
    "neighbourhood_dataframe1_sorted = neighbourhood_dataframe1_selected.sort_values('median_price', ascending=False)\n",
    "neighbourhood_dataframe1_sorted.plot( kind= 'bar' , secondary_y= 'median_price' , rot= 90 )\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "# Plot number of listings and mean price in one bar plot\n",
    "plt.rcParams['font.size'] = 15\n",
    "plt.rcParams['figure.figsize'] = 25, 5\n",
    "selected_columns = ['number_of_listings', 'mean_price']\n",
    "neighbourhood_dataframe1_selected = neighbourhood_dataframe1[selected_columns]\n",
    "neighbourhood_dataframe1_sorted = neighbourhood_dataframe1_selected.sort_values('mean_price', ascending=False)\n",
    "neighbourhood_dataframe1_sorted.plot( kind= 'bar' , secondary_y= 'mean_price' , rot= 90 )\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f63c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_count_and_price_plot(column_name,alpha=0.05):\n",
    "    # Assuming dataframe1 is your DataFrame\n",
    "\n",
    "    # Plot count distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.countplot(x=column_name, data=dataframe1)\n",
    "    plt.title(f'Count distribution of {column_name}')\n",
    "\n",
    "    # Plot median price distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    median_prices = dataframe1.groupby(column_name)['price'].median()\n",
    "    median_prices.plot(kind='bar', color=['blue', 'orange'])\n",
    "    plt.title(f'Median price distribution by {column_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate t-test for median prices\n",
    "    true_prices = dataframe1[dataframe1[column_name] == True]['price']\n",
    "    false_prices = dataframe1[dataframe1[column_name] == False]['price']\n",
    "    t_stat, p_value = ttest_ind(true_prices, false_prices)\n",
    "   \n",
    "    # Display additional information\n",
    "    percent_split = dataframe1[column_name].value_counts(normalize=True) * 100\n",
    "    print(f'% Wise Split of {column_name}:\\n{percent_split}')\n",
    "    t_statistic, p_value = ttest_ind(true_prices, false_prices, equal_var=False)  # assuming unequal variances\n",
    "    print(f'\\nP-value for {column_name}: {p_value}')\n",
    "    \n",
    "    # Check if p-value is greater than or equal to alpha\n",
    "    if p_value >= alpha:\n",
    "        print(f\"{column_name} is not statistically significant (p-value >= {alpha})\")\n",
    "        return column_name\n",
    "    else:\n",
    "        print(f\"{column_name} is statistically significant (p-value < {alpha})\")\n",
    "        return None  # or any other value if needed\n",
    "\n",
    "superhost = binary_count_and_price_plot('host_is_superhost')\n",
    "print(dataframe1.host_is_superhost.value_counts(normalize=True))\n",
    "\n",
    "test2 = binary_count_and_price_plot('host_identity_verified')\n",
    "print(dataframe1.host_identity_verified.value_counts(normalize=True))\n",
    "\n",
    "test3 = binary_count_and_price_plot('instant_bookable')\n",
    "print(dataframe1.instant_bookable.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db725883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names and their indices\n",
    "column_info = list(enumerate(dataframe1.columns))\n",
    "# Print column names and their indices\n",
    "for index, column_name in column_info:\n",
    "    print(f\"Column {index}: {column_name}\")\n",
    "    \n",
    "#now we know that the amenity columns start from index 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590ff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing the analysis for all the different amenities\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['figure.figsize'] = 20, 20\n",
    "result = [binary_count_and_price_plot(col) for col in dataframe1.iloc[:,51:-2].columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e109df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we are making a comparison against the rating and seeing if having the amenity might impact the rating\n",
    "\n",
    "def binary_count_and_rating_plot(column_name,alpha=0.05):\n",
    "    # Only taking the ones with reviews into consideration\n",
    "    data = dataframe1[[column_name, 'review_scores_rating']].dropna()\n",
    "\n",
    "    # Plot count distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.countplot(x=column_name, data=data)\n",
    "    plt.title(f'Count distribution of {column_name}')\n",
    "\n",
    "    # Plot median price distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    rating_mean = data.groupby(column_name)['review_scores_rating'].mean()\n",
    "    rating_mean.plot(kind='bar', color=['blue', 'orange'])\n",
    "    plt.title(f'Average rating distribution by {column_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate t-test for median prices\n",
    "    true_ratings = data[data[column_name] == True]['review_scores_rating']\n",
    "    false_ratings = data[data[column_name] == False]['review_scores_rating']\n",
    "    t_stat, p_value = ttest_ind(true_ratings, false_ratings)\n",
    "   \n",
    "    # Display additional information\n",
    "    percent_split = data[column_name].value_counts(normalize=True) * 100\n",
    "    print(f'% Wise Split of {column_name}:\\n{percent_split}')\n",
    "    t_statistic, p_value = ttest_ind(true_ratings, false_ratings, equal_var=False)  # assuming unequal variances\n",
    "    print(f'\\nP-value for {column_name}: {p_value}')\n",
    "    \n",
    "    # Check if p-value is greater than or equal to alpha\n",
    "    if p_value >= alpha:\n",
    "        print(f\"{column_name} is not statistically significant (p-value >= {alpha})\")\n",
    "        return column_name\n",
    "    else:\n",
    "        print(f\"{column_name} is statistically significant (p-value < {alpha})\")\n",
    "        return None  # or any other value if needed\n",
    "    \n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['figure.figsize'] = 20, 20\n",
    "result = [binary_count_and_rating_plot(col) for col in dataframe1.iloc[:,51:-2].columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0bb6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def collect_pvalues(columns_of_interest):\n",
    "    all_pvalues = {}\n",
    "\n",
    "    for column_name in columns_of_interest:\n",
    "        # Drop rows with missing values in the specified columns\n",
    "        data = dataframe1[[column_name, 'price']].dropna()\n",
    "\n",
    "        # Calculate t-test for median prices\n",
    "        true_prices = data[data[column_name] == True]['price']\n",
    "        false_prices = data[data[column_name] == False]['price']\n",
    "        t_statistic, p_value = ttest_ind(true_prices, false_prices, equal_var=False)  # assuming unequal variances\n",
    "        all_pvalues[column_name] = {'p_value': p_value, 't_statistic': t_statistic}\n",
    "\n",
    "    return all_pvalues\n",
    "\n",
    "columns_of_interest = dataframe1.iloc[:, 51:-2].columns.values\n",
    "result = collect_pvalues(columns_of_interest)\n",
    "# Extract p-values and create a dictionary\n",
    "\n",
    "all_pvalues = {}\n",
    "for column in result:\n",
    "    p_value = result[column]['p_value']\n",
    "    if isinstance(p_value, np.float64):\n",
    "        all_pvalues[column] = p_value\n",
    "    else:\n",
    "        all_pvalues[column] = [float(val) for val in p_value]\n",
    "\n",
    "import math\n",
    "#P-values of effects of each amanity on price can be visualized using the 'Manhattan-plot' approach: taking the minus log of the P-value:\n",
    "# -log Pvalues and order\n",
    "all_pvalues.update({k: -1*np.log(v) for k, v in all_pvalues.items()})\n",
    "ordereddict = OrderedDict(sorted(all_pvalues.items(), key=lambda t: t[1]))\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['figure.figsize'] = 12, 30\n",
    "plt.barh(list(ordereddict.keys()), list(ordereddict.values()),color='g', height=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ec67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['figure.figsize'] = 16, 8\n",
    "\n",
    "print(f\"Of the Airbnb hosts that are still listing on the site, the first joined on {min(dataframe1.host_since).strftime('%d %B %Y')}, and the most recent joined on {max(dataframe1.host_since).strftime('%d %B %Y')}.\")\n",
    "\n",
    "\n",
    "dataframe1.set_index('host_since').resample('MS').size().plot(label='Hosts joining Airbnb', color='orange')\n",
    "dataframe1.set_index('first_review').resample('MS').size().plot(label='Listings getting their first review', color='green')\n",
    "plt.title('Paris hosts registration on Airbnb, numbers of first reviews per accommodation per month')\n",
    "plt.legend()\n",
    "plt.xlim('2008-01-01', '2019-11-30') # Limiting to whole months\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae5a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "\n",
    "def decompose_time_series(ts, title=''):\n",
    "    decomposition = seasonal_decompose(ts)\n",
    "    trend = decomposition.trend\n",
    "    seasonal = decomposition.seasonal\n",
    "    residual = decomposition.resid\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(411)\n",
    "    plt.plot(ts, label='Original')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(f'{title} - Original')\n",
    "\n",
    "    plt.subplot(412)\n",
    "    plt.plot(trend, label='Trend')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(f'{title} - Trend')\n",
    "\n",
    "    plt.subplot(413)\n",
    "    plt.plot(seasonal, label='Seasonal')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(f'{title} - Seasonal')\n",
    "\n",
    "    plt.subplot(414)\n",
    "    plt.plot(residual, label='Residual')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(f'{title} - Residual')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Creating dataframes for time series analysis\n",
    "ts_host_since = pd.DataFrame(dataframe1.set_index('host_since').resample('MS').size())\n",
    "ts_first_review = pd.DataFrame(dataframe1.set_index('first_review').resample('MS').size())\n",
    "\n",
    "# Renaming columns\n",
    "ts_host_since = ts_host_since.rename(columns={0: 'hosts'})\n",
    "ts_host_since.index.rename('month', inplace=True)\n",
    "ts_first_review = ts_first_review.rename(columns={0: 'reviews'})\n",
    "ts_first_review.index.rename('month', inplace=True)\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['figure.figsize'] = 50, 70\n",
    "decompose_time_series(ts_host_since, title='Number of registrations on Airbnb by month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7dccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(dataframe1['reviews_per_month'].dropna(), bins=50, kde=True)\n",
    "plt.xlabel('Reviews Per Month')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Reviews Per Month Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074a60f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Create a base map using the mean latitude and longitude values\n",
    "mean_lat = dataframe1['latitude'].mean()\n",
    "mean_lon = dataframe1['longitude'].mean()\n",
    "\n",
    "m = folium.Map(location=[mean_lat, mean_lon], zoom_start=12)\n",
    "\n",
    "# Create a HeatMap layer with price as the intensity\n",
    "heat_data = [[row['latitude'], row['longitude'], row['price']] for index, row in dataframe1.iterrows()]\n",
    "HeatMap(heat_data, radius=15, blur=10).add_to(m)\n",
    "\n",
    "# Display the map in Jupyter Notebook\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out the neighborhoods that correspond with the rules we've established.\n",
    "top_6 = (dataframe1.groupby(['neighbourhood'])['id'].count()\n",
    "        .sort_values(ascending = False).head(6))\n",
    "\n",
    "# Let's make a list containing all the 'top_6' Series neighborhoods.\n",
    "desirable_neighborhoods = list(top_6.index)\n",
    "\n",
    "# Here, we'll perform a group by. It will list all neighborhoods \n",
    "# and their property types available. Also, it will count the number of \n",
    "# dwellings each residence kind has.\n",
    "property_types_count = dataframe1.groupby(['neighbourhood','property_type'], as_index = False)['id'].count()\n",
    "property_types_count = (property_types_count.sort_values(by = 'id', ascending = False)) \n",
    "\n",
    "# Let's make a list containing all the 'top_6' Series neighborhoods.\n",
    "desirable_neighborhoods = list(top_6.index)\n",
    "\n",
    "# We'll  filter the DF so that it only contains residences registered \n",
    "# in the 'desirable_neighborhoods' list.\n",
    "property_types_count = (property_types_count[property_types_count['neighbourhood']\n",
    "                                             .isin(desirable_neighborhoods)])\n",
    "\n",
    "# And that's it! We are set to plot the chart!\n",
    "import plotly.express as px\n",
    "\n",
    "# Unfortunately plotly has a bug for the sunburst plot \n",
    "# in which it does not recognize the 'id' column for the chart creation. \n",
    "\n",
    "# Hence we'll have to make a second column with the same data.\n",
    "# We are going to call the new column 'Number of Dwellings'\n",
    "property_types_count['Number of Dwellings'] = property_types_count['id']\n",
    "\n",
    "# Finally plotting the sunburst chart!\n",
    "figure = px.sunburst(property_types_count, values = 'Number of Dwellings', \n",
    "                     path = ['neighbourhood','property_type'], width = 600, height = 600, \n",
    "                     title = 'Dwelling Type Composition')\n",
    "\n",
    "figure.update_traces(textinfo=\"label+percent parent\")\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0959a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Cut the 'review_scores_value' into categories\n",
    "categories = pd.cut(dataframe1['review_scores_value'].dropna(), 4, labels=['Poor', 'Average', 'Good', 'Excellent'])\n",
    "dataframe1['Lodging Quality'] = categories\n",
    "\n",
    "# Filter out residences with no customer evaluation\n",
    "no_nan = dataframe1[~dataframe1['review_scores_value'].isnull()]\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Group by 'Lodging Quality' and calculate the average price for each category\n",
    "groups = no_nan.groupby('Lodging Quality')['price'].mean()\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the bar chart with adjusted ylim\n",
    "chart = groups.plot(kind='bar', ylim=(0, 300), xlabel='', yticks=[], fontsize=12, color=['grey', 'grey', 'darkred', 'grey'])\n",
    "\n",
    "# Display average prices on top of the bars\n",
    "for i, price in enumerate(groups):\n",
    "    plt.text(i, price + 10, f'${price:.2f}', ha='center', va='bottom', fontdict={'size': 12})\n",
    "\n",
    "# Set the graph title\n",
    "plt.title('Average Price per Dwelling Quality', fontdict={'size': 15})\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc42025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing the distribution between price and amount of bedrooms\n",
    "figsize = (10, 5)\n",
    "x = 'bedrooms'\n",
    "\n",
    "# Create a grid with 2 rows and 1 column\n",
    "fig = plt.figure(figsize=figsize)\n",
    "gs = gridspec.GridSpec(1, 1)\n",
    "\n",
    "# Bar plot for price distribution\n",
    "ax1 = plt.subplot(gs[0])\n",
    "sns.barplot(data=dataframe1, x=x, y='price', ax=ax1)\n",
    "ax1.set_title(f'Distribution of Price based on {x}')\n",
    "\n",
    "# Calculate the percentage of each bedroom count relative to all listings\n",
    "bedroom_percentage = dataframe1[x].value_counts(normalize=True) * 100\n",
    "\n",
    "# Display the percentage of each bedroom count\n",
    "for bedrooms, percentage in bedroom_percentage.items():\n",
    "    print(f'The percentage of listings with {bedrooms} bedroom(s) is: {percentage:.2f}%')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d55010",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing the distribution between price and amount of beds\n",
    "figsize = (10, 5)\n",
    "x = 'beds'\n",
    "\n",
    "# Create a grid with 2 rows and 1 column\n",
    "fig = plt.figure(figsize=figsize)\n",
    "gs = gridspec.GridSpec(1, 1)\n",
    "\n",
    "# Bar plot for price distribution\n",
    "ax1 = plt.subplot(gs[0])\n",
    "sns.barplot(data=dataframe1, x=x, y='price', ax=ax1)\n",
    "ax1.set_title(f'Distribution of Price based on {x}')\n",
    "\n",
    "# Calculate the percentage of each bedroom count relative to all listings\n",
    "bed_percentage = dataframe1[x].value_counts(normalize=True) * 100\n",
    "\n",
    "# Display the percentage of each bedroom count\n",
    "for beds, percentage in bed_percentage.items():\n",
    "    print(f'The percentage of listings with {beds} bed(s) is: {percentage:.2f}%')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2c9e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing the distribution between price and amount of bathrooms\n",
    "figsize = (10, 5)\n",
    "x = 'bathrooms'\n",
    "\n",
    "# Create a grid with 2 rows and 1 column\n",
    "fig = plt.figure(figsize=figsize)\n",
    "gs = gridspec.GridSpec(1, 1)\n",
    "\n",
    "# Bar plot for price distribution\n",
    "ax1 = plt.subplot(gs[0])\n",
    "sns.barplot(data=dataframe1, x=x, y='price', ax=ax1)\n",
    "ax1.set_title(f'Distribution of Price based on {x}')\n",
    "\n",
    "# Calculate the percentage of each bedroom count relative to all listings\n",
    "bathroom_percentage = dataframe1[x].value_counts(normalize=True) * 100\n",
    "\n",
    "# Display the percentage of each bedroom count\n",
    "for bathrooms, percentage in bathroom_percentage.items():\n",
    "    print(f'The percentage of listings with {bathrooms} bathroom(s) is: {percentage:.2f}%')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8448ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing the distribution between price and amount of bathrooms\n",
    "figsize = (10, 5)\n",
    "x = 'room_type'\n",
    "\n",
    "# Create a grid with 2 rows and 1 column\n",
    "fig = plt.figure(figsize=figsize)\n",
    "gs = gridspec.GridSpec(1, 1)\n",
    "\n",
    "# Bar plot for price distribution\n",
    "ax1 = plt.subplot(gs[0])\n",
    "sns.barplot(data=dataframe1, x=x, y='price', ax=ax1)\n",
    "ax1.set_title(f'Distribution of Price based on {x}')\n",
    "\n",
    "# Calculate the percentage of each bedroom count relative to all listings\n",
    "room_type_percentage = dataframe1[x].value_counts(normalize=True) * 100\n",
    "\n",
    "# Display the percentage of each bedroom count\n",
    "for room_type, percentage in room_type_percentage.items():\n",
    "    print(f'The percentage of listings with {room_type} room type(s) is: {percentage:.2f}%')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a3819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing the distribution between price and amount of bathrooms\n",
    "figsize = (10, 5)\n",
    "x = 'accommodates'\n",
    "\n",
    "# Create a grid with 2 rows and 1 column\n",
    "fig = plt.figure(figsize=figsize)\n",
    "gs = gridspec.GridSpec(1, 1)\n",
    "\n",
    "# Bar plot for price distribution\n",
    "ax1 = plt.subplot(gs[0])\n",
    "sns.barplot(data=dataframe1, x=x, y='price', ax=ax1)\n",
    "ax1.set_title(f'Distribution of Price based on {x}')\n",
    "\n",
    "# Calculate the percentage of each bedroom count relative to all listings\n",
    "accommodates_percentage = dataframe1[x].value_counts(normalize=True) * 100\n",
    "\n",
    "# Display the percentage of each bedroom count\n",
    "for accommodates, percentage in accommodates_percentage.items():\n",
    "    print(f'The percentage of listings with {accommodates} accommodates is: {percentage:.2f}%')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6bcd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of amenities for each listing\n",
    "dataframe1['total_amenities'] = dataframe1.iloc[:, 51:-1].sum(axis=1)\n",
    "\n",
    "#showing the distribution between price and amount of bathrooms\n",
    "figsize = (10, 5)\n",
    "x = 'total_amenities'\n",
    "\n",
    "# Create a grid with 2 rows and 1 column\n",
    "fig = plt.figure(figsize=figsize)\n",
    "gs = gridspec.GridSpec(1, 1)\n",
    "\n",
    "# Bar plot for price distribution\n",
    "ax1 = plt.subplot(gs[0])\n",
    "sns.barplot(data=dataframe1, x=x, y='price', ax=ax1)\n",
    "ax1.set_title(f'Distribution of Price based on {x}')\n",
    "\n",
    "# Calculate the percentage of each bedroom count relative to all listings\n",
    "total_amenities_percentage = dataframe1[x].value_counts(normalize=True) * 100\n",
    "\n",
    "# Display the percentage of each bedroom count\n",
    "for total_amenities, percentage in total_amenities_percentage.items():\n",
    "    print(f'The percentage of listings with {total_amenities} total_amenities is: {percentage:.2f}%')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c858ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "#Trying to avoid the memory leak of kmeans KML\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "# Set up the map centered around Paris\n",
    "paris_map = folium.Map(location=[48.8566, 2.3522], zoom_start=12)\n",
    "# Add cluster centers to the map using MarkerCluster\n",
    "marker_cluster = MarkerCluster().add_to(paris_map)\n",
    "\n",
    "#making a new dataframe for the locations of the listings\n",
    "data = {\n",
    "    'latitude': dataframe1['latitude'].head(100).tolist(),\n",
    "    'longitude': dataframe1['longitude'].head(100).tolist(),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Reverse geocoding to extract postal codes and ranks\n",
    "geolocator = Nominatim(user_agent=\"geo_analyzer\")\n",
    "\n",
    "df['location'] = df.apply(lambda row: geolocator.reverse((row['latitude'], row['longitude'])), axis=1)\n",
    "df['rank'] = df['location'].apply(lambda loc: loc.raw.get('importance', None))\n",
    "\n",
    "def rank_to_color(rank):\n",
    "    if rank is not None:\n",
    "        rank = float(rank)  # Convert rank to float for better precision\n",
    "        if rank > 0.5:\n",
    "            return 'darkgreen'\n",
    "        elif 0.3 < rank <= 0.5:\n",
    "            return 'green'\n",
    "        elif 0.1 < rank <= 0.3:\n",
    "            return 'lightgreen'\n",
    "        elif 0.05 < rank <= 0.1:\n",
    "            return 'lightorange'\n",
    "        elif rank <= 0.05:\n",
    "            return 'darkorange'\n",
    "    return 'gray'\n",
    "\n",
    "# Step 3: KMeans Clustering with Euclidean distance metric\n",
    "your_desired_clusters = dataframe1['neighbourhood'].nunique()\n",
    "kmeans = KMeans(n_clusters=your_desired_clusters, n_init=10)\n",
    "df['cluster_kmeans'] = kmeans.fit_predict(df[['latitude', 'longitude']])\n",
    "\n",
    "# Calculate average rank for each cluster\n",
    "cluster_avg_rank = df.groupby('cluster_kmeans')['rank'].mean().to_dict()\n",
    "df['cluster_avg_rank'] = df['cluster_kmeans'].map(cluster_avg_rank)\n",
    "\n",
    "# Create a dictionary to store cluster information\n",
    "clusters_info = {}\n",
    "\n",
    "# Iterate through each row to populate the clusters_info dictionary\n",
    "for idx, row in df.iterrows():\n",
    "    cluster_label = row['cluster_kmeans']\n",
    "    \n",
    "    # If cluster_label is not in clusters_info, create an entry\n",
    "    if cluster_label not in clusters_info:\n",
    "        clusters_info[cluster_label] = {\n",
    "            'latitude_sum': row['latitude'],\n",
    "            'longitude_sum': row['longitude'],\n",
    "            'avg_rank': cluster_avg_rank[cluster_label],\n",
    "            'count': 1\n",
    "        }\n",
    "    else:\n",
    "        # Update sum of coordinates, average rank, and count for existing cluster\n",
    "        clusters_info[cluster_label]['latitude_sum'] += row['latitude']\n",
    "        clusters_info[cluster_label]['longitude_sum'] += row['longitude']\n",
    "        clusters_info[cluster_label]['avg_rank'] += cluster_avg_rank[cluster_label]\n",
    "        clusters_info[cluster_label]['count'] += 1\n",
    "\n",
    "# Add each cluster's information to the map\n",
    "for cluster_label, info in clusters_info.items():\n",
    "    # Calculate centroid (middle point) of the cluster\n",
    "    centroid_latitude = info['latitude_sum'] / info['count']\n",
    "    centroid_longitude = info['longitude_sum'] / info['count']\n",
    "    \n",
    "    color = rank_to_color(info['avg_rank'] / info['count'])  # Calculate average rank\n",
    "    folium.CircleMarker(\n",
    "        location=[centroid_latitude, centroid_longitude],\n",
    "        radius=10,  # Adjust the radius based on the count\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_color=color,\n",
    "        fill_opacity=0.7,\n",
    "        popup=f\"Avg Rank: {info['avg_rank'] / info['count']:.2f}, Cluster KMeans: {cluster_label}\"\n",
    "    ).add_to(paris_map)\n",
    "    \n",
    "# Plotting the cluster centers on the map of Paris\n",
    "paris_map\n",
    "\n",
    "#not yet to the point, so not accurate yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e983dd2",
   "metadata": {},
   "source": [
    "# RF MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b76a0",
   "metadata": {},
   "source": [
    "## Feature Selection/Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47661981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with NaN values: ['first_review', 'last_review', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'reviews_per_month', 'days_since_first_review', 'days_since_first_first_review', 'days_since_last_review', 'days_since_last_last_review']\n"
     ]
    }
   ],
   "source": [
    "# Creating a copy of the dataset to work with\n",
    "data_copy = dataframe1.copy()\n",
    "#looking into which columns currently still have empty values\n",
    "columns_with_nan = data_copy.columns[data_copy.isna().any()].tolist()\n",
    "# Print the columns\n",
    "print(\"Columns with NaN values:\", columns_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0996c017",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"id\", \"host_id\", \"days_since_host_host\",\"host_since\"]\n",
    "data_copy = data_copy.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "361907d0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datetime Columns: ['first_review', 'last_review']\n"
     ]
    }
   ],
   "source": [
    "# Display columns with datetime type\n",
    "datetime_columns = data_copy.select_dtypes(include=['datetime64[ns]']).columns\n",
    "print(\"Datetime Columns:\", datetime_columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbc0f041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with NaN values: []\n"
     ]
    }
   ],
   "source": [
    "#to many missing values in these columns, thus removing them\n",
    "columns_to_drop = ['first_review', 'last_review','days_since_first_review', 'days_since_first_first_review', 'days_since_last_review', \n",
    "                   'days_since_last_last_review']\n",
    "\n",
    "#filling in 0 in the fields where nothing is filled in\n",
    "columns_to_fill = ['review_scores_rating', 'review_scores_accuracy', \n",
    "                   'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', \n",
    "                   'review_scores_location', 'review_scores_value', 'reviews_per_month']\n",
    "\n",
    "# Fill NaN values with 0 in the specified columns\n",
    "for column in columns_to_fill:\n",
    "    data_copy[column] = data_copy[column].fillna(0)\n",
    "\n",
    "data_copy_clean = data_copy.drop(columns_to_drop, axis=1)\n",
    "\n",
    "#looking into which columns currently still have empty values\n",
    "columns_with_nan = data_copy_clean.columns[data_copy_clean.isna().any()].tolist()\n",
    "# Print the columns\n",
    "print(\"Columns with NaN values:\", columns_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ed12f82",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['host_response_time', 'host_response_rate', 'neighbourhood', 'property_type', 'room_type']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#looking which columns are strings\n",
    "string_columns = [col for col in data_copy_clean.columns if data_copy_clean[col].dtype == 'object' or isinstance(data_copy_clean[col].dtype, pd.StringDtype)]\n",
    "print(string_columns)\n",
    "\n",
    "#looking which columns are categories\n",
    "category_columns = [col for col in data_copy_clean.columns if data_copy_clean[col].dtype == 'category' or isinstance(data_copy_clean[col].dtype, pd.CategoricalDtype)]\n",
    "print(category_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0ed5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy_clean[\"days_since_host\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72419fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Splitting the columns up in one-hot and label encoding\n",
    "categorical_columns_one_hot = ['neighbourhood', 'property_type', 'room_type']  # For one-hot encoding\n",
    "categorical_columns_label = ['host_response_time', 'host_response_rate']  # For label encoding\n",
    "\n",
    "# One-Hot Encoding\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(data_copy_clean[categorical_columns_one_hot])\n",
    "\n",
    "# Manually create feature names for the one-hot encoded columns\n",
    "one_hot_feature_names = []\n",
    "for i, column in enumerate(categorical_columns_one_hot):\n",
    "    categories = one_hot_encoder.categories_[i]\n",
    "    one_hot_feature_names.extend([f\"{column}_{category}\" for category in categories])\n",
    "\n",
    "one_hot_encoded_df = pd.DataFrame(one_hot_encoded, columns=one_hot_feature_names, index=data_copy_clean.index)\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_columns_label:\n",
    "    data_copy_clean[col] = label_encoder.fit_transform(data_copy_clean[col])\n",
    "\n",
    "# Concatenate the one-hot encoded columns back to the original dataframe\n",
    "data_copy_clean = pd.concat([data_copy_clean, one_hot_encoded_df], axis=1)\n",
    "\n",
    "# Drop the original string columns\n",
    "data_copy_clean.drop(categorical_columns_one_hot, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f6273db",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 56145 entries, 0 to 56725\n",
      "Columns: 161 entries, host_response_time to room_type_Shared room\n",
      "dtypes: bool(5), float64(43), int32(2), int64(111)\n",
      "memory usage: 67.1 MB\n"
     ]
    }
   ],
   "source": [
    "data_copy_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3bf761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns with exclusively 0s and 1s to boolean\n",
    "for col in data_copy_clean.columns:\n",
    "    unique_values = data_copy_clean[col].unique()\n",
    "    if set(unique_values).issubset({0, 1}):\n",
    "        data_copy_clean[col] = data_copy_clean[col].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "041fc591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cap all the outliers (the ones that are below 0.05 or above 0.95 percentile) to 0.05 percentile and 0.95 percentile\n",
    "from scipy.stats.mstats import winsorize\n",
    "for col in data_copy_clean.select_dtypes(include=np.number).columns:\n",
    "    data_copy_clean[col] = winsorize(data_copy_clean[col], limits=[0.05, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe99e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy_clean.to_excel('data_RF_073.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d423926f",
   "metadata": {},
   "source": [
    "## 1. RF without further selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a69fcb34",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error:, 2647.7587\n",
      "R-squared: 0.7306\n",
      "Adjusted R-squared: 0.7280\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "X = data_copy_clean.drop(\"price\", axis=1)\n",
    "y = data_copy_clean['price']\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "#looking at MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error:, {mse:.4f}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "\n",
    "# Calculate Adjusted R-squared\n",
    "n = X_test.shape[0]  # Number of observations\n",
    "p = X_test.shape[1]  # Number of predictor variables\n",
    "adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "print(f\"Adjusted R-squared: {adjusted_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366ce76c",
   "metadata": {},
   "source": [
    "## 2. RF with feature importance score (0.01 threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f45c2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "!pip install --upgrade joblib\n",
    "\n",
    "import os\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"4\"\n",
    "\n",
    "X = data_copy_clean.drop(\"price\",axis=1)\n",
    "Y = data_copy_clean['price']\n",
    "\n",
    "importances = mutual_info_classif(X,Y)\n",
    "feat_importances = pd.Series(importances, X.columns)\n",
    "feat_importances.plot(kind='barh', color= \"teal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c799cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "X = data_copy_clean.drop(\"price\", axis=1)\n",
    "y = data_copy_clean['price']\n",
    "\n",
    "# Calculate feature importances\n",
    "importances = mutual_info_classif(X, y)\n",
    "feat_importances = pd.Series(importances, X.columns)\n",
    "\n",
    "# Set a threshold for feature importance (this is arbitrary and can be adjusted)\n",
    "threshold = 0.01  # Example threshold\n",
    "\n",
    "# Select features above the importance threshold\n",
    "selected_features = feat_importances[feat_importances > threshold].index\n",
    "\n",
    "# Rebuild the feature set with only selected features\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "#looking at MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculate R-squared\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Calculate Adjusted R-squared\n",
    "n = X_test.shape[0]  # Number of observations\n",
    "p = X_test.shape[1]  # Number of predictor variables\n",
    "adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R-squared:\", adjusted_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a182941",
   "metadata": {},
   "source": [
    "## 3. RF with Backward Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86d3e23a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Start Backward Feature Elimination with a simpler model\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 23\u001b[0m     selection_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     24\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m selection_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     25\u001b[0m     current_r_squared \u001b[38;5;241m=\u001b[39m r2_score(y_test, predictions)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \n\u001b[0;32m   1294\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1320\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m   1321\u001b[0m         X,\n\u001b[0;32m   1322\u001b[0m         y,\n\u001b[0;32m   1323\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1324\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[0;32m   1325\u001b[0m     )\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load your data\n",
    "X = data_copy_clean.drop('price', axis=1)\n",
    "y = data_copy_clean['price']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize a simpler model for feature selection\n",
    "selection_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Initialize variables for tracking the best R-squared and corresponding feature set\n",
    "best_r_squared = 0\n",
    "best_features_set = X_train.columns.tolist()\n",
    "\n",
    "# Start Backward Feature Elimination with a simpler model\n",
    "for _ in range(len(X_train.columns) - 1):\n",
    "    selection_model.fit(X_train, y_train)\n",
    "    predictions = selection_model.predict(X_test)\n",
    "    current_r_squared = r2_score(y_test, predictions)\n",
    "\n",
    "    if current_r_squared > best_r_squared:\n",
    "        best_r_squared = current_r_squared\n",
    "        best_features_set = X_train.columns.tolist()\n",
    "    \n",
    "    # Remove the least important feature\n",
    "    importances = selection_model.feature_importances_\n",
    "    least_important = np.argmin(importances)\n",
    "    X_train = X_train.drop(X_train.columns[least_important], axis=1)\n",
    "    X_test = X_test.drop(X_test.columns[least_important], axis=1)\n",
    "\n",
    "# Train final model with Random Forest using the best feature set\n",
    "final_model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "X_train_best = X_train\n",
    "X_test_best = X_test\n",
    "final_model.fit(X_train_best, y_train)\n",
    "\n",
    "# Evaluate the final model\n",
    "final_predictions = final_model.predict(X_test_best)\n",
    "final_performance = mean_squared_error(y_test, final_predictions)\n",
    "final_r_squared = r2_score(y_test, final_predictions)\n",
    "adjusted_r2 = 1 - (1 - final_r_squared) * (len(y_test) - 1) / (len(y_test) - X_train_best.shape[1] - 1)\n",
    "\n",
    "print(f\"Final performance: MSE: {final_performance:.4f}, R-squared: {final_r_squared:.4f}, Adjusted R-squared: {adjusted_r2:.4f}\")\n",
    "print(\"Best selected features:\", best_features_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8b3e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names and their indices\n",
    "column_info = list(enumerate(data_copy_clean.columns))\n",
    "# Print column names and their indices\n",
    "for index, column_name in column_info:\n",
    "    print(f\"Column {index}: {column_name}\")\n",
    "    \n",
    "#now we know that the amenity columns start from index 51"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b99b17a",
   "metadata": {},
   "source": [
    "## 4. RF with selection based on correlation and VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4348d092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "# Select only the numerical columns for VIF computation\n",
    "# Assuming your DataFrame is named 'data'\n",
    "numerical_data = data_copy_clean.select_dtypes(include=[float, int])\n",
    "\n",
    "# Adding a constant column for intercept\n",
    "numerical_data_with_constant = add_constant(numerical_data)\n",
    "\n",
    "#because of performance issues we are taking a subset of the data to make this calculation (only taking 40% of the data)\n",
    "sampled_data = numerical_data_with_constant.sample(frac=0.4, random_state=42)\n",
    "\n",
    "# Calculating VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = sampled_data.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(sampled_data.values, i) for i in range(sampled_data.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70258bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and display features with VIF greater than 5\n",
    "high_vif_features = vif_data[vif_data[\"VIF\"] > 5]\n",
    "print(\"Features with VIF greater than 5:\\n\", high_vif_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15598bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "data = data_copy_clean.copy()\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = data.corr()\n",
    "\n",
    "# Set a threshold for high correlation (for example, 0.8)\n",
    "high_corr_threshold = 0.8\n",
    "\n",
    "# Identify features highly correlated with the target variable\n",
    "target_variable = 'price'  # Replace with your actual target variable name\n",
    "high_corr_with_target = corr_matrix[target_variable].abs().sort_values(ascending=False)\n",
    "\n",
    "# Selecting features with high correlation to the target\n",
    "selected_features = high_corr_with_target[high_corr_with_target > high_corr_threshold].index.tolist()\n",
    "selected_features.remove(target_variable)  # Remove the target variable itself from the list\n",
    "\n",
    "# Identify and remove redundant features\n",
    "redundant_features = set()\n",
    "for i in range(len(selected_features)):\n",
    "    for j in range(i+1, len(selected_features)):\n",
    "        if abs(corr_matrix.loc[selected_features[i], selected_features[j]]) > high_corr_threshold:\n",
    "            # Add one of the highly correlated features to the redundant set\n",
    "            redundant_features.add(selected_features[j])\n",
    "\n",
    "# Remove redundant features from selected features\n",
    "final_features = [feature for feature in selected_features if feature not in redundant_features]\n",
    "\n",
    "# Now, final_features contains the features you want to keep\n",
    "print(\"Selected features:\", final_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a5feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create a new DataFrame with only the selected features\n",
    "X = data[final_features]\n",
    "y = data[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "adjusted_r2 = 1 - (1 - final_r_squared) * (len(y_test) - 1) / (len(y_test) - X_train_best.shape[1] - 1)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "print(f\"R-squared adjusted: {adjusted_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb315194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('your_data.csv')  # Replace with your actual file path\n",
    "X = data.drop('target', axis=1)  # Replace 'target' with your actual target column name\n",
    "\n",
    "# Function to calculate VIF for each feature\n",
    "def calculate_vif(df):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = df.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# Initial VIF calculation\n",
    "vif_df = calculate_vif(X)\n",
    "\n",
    "# Set a threshold for high VIF\n",
    "high_vif_threshold = 5\n",
    "\n",
    "# Remove features with high VIF\n",
    "while vif_df['VIF'].max() > high_vif_threshold:\n",
    "    highest_vif_feature = vif_df.sort_values('VIF', ascending=False)['Feature'].iloc[0]\n",
    "    X = X.drop(highest_vif_feature, axis=1)\n",
    "    vif_df = calculate_vif(X)\n",
    "\n",
    "# Print the final set of features and their VIF values\n",
    "print(vif_df)\n",
    "print(\"\\nRemaining Features after VIF reduction:\", X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41179bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming 'target' is your target variable and it's in the original 'data' DataFrame\n",
    "y = data['target']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "adjusted_r2 = 1 - (1 - final_r_squared) * (len(y_test) - 1) / (len(y_test) - X_train_best.shape[1] - 1)\n",
    "\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "print(f\"R-squared adjusted: {adjusted_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcba5a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is loaded into 'data'\n",
    "# Replace 'target_column' with your actual target variable\n",
    "target = 'price'\n",
    "\n",
    "# Correlation for numerical features\n",
    "numerical_features = numerical_data.select_dtypes(include=['int64', 'float64'])\n",
    "correlation_matrix = numerical_features.corr()\n",
    "correlation_with_target = correlation_matrix[target].sort_values(ascending=False)\n",
    "\n",
    "# Display the correlations\n",
    "print(correlation_with_target)\n",
    "\n",
    "# Visualize the correlations with a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b7c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace 'target_column' with your actual target variable\n",
    "target = 'price'\n",
    "# Calculate the correlation matrix\n",
    "copied_data = data.copy()\n",
    "correlation_matrix = copied_data.corr()\n",
    "\n",
    "# Get the correlation of each feature with the target variable\n",
    "# and sort them by the absolute value of the correlation\n",
    "sorted_correlations = correlation_matrix[target].abs().sort_values()\n",
    "\n",
    "# Identify the five features with the lowest correlation\n",
    "lowest_corr_features = sorted_correlations.head(5).index.tolist()\n",
    "\n",
    "# Print out the features to be removed\n",
    "print(\"Features to be removed:\", lowest_corr_features)\n",
    "\n",
    "# Remove these features from the dataset\n",
    "data = copied_data.drop(columns=lowest_corr_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c28a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = numerical_data.corr()\n",
    "\n",
    "# Visualize the correlation matrix using a heatmap\n",
    "plt.figure(figsize=(100, 40))  # You can adjust the size of the figure\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd49fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for identifying high correlations\n",
    "threshold = 0.8\n",
    "\n",
    "# Find pairs of highly correlated features\n",
    "highly_correlated_pairs = []\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "            colname = corr_matrix.columns[i]\n",
    "            rowname = corr_matrix.columns[j]\n",
    "            highly_correlated_pairs.append((rowname, colname))\n",
    "\n",
    "# Print the highly correlated pairs\n",
    "print(\"Highly correlated pairs (above threshold of {}):\".format(threshold))\n",
    "for pair in highly_correlated_pairs:\n",
    "    print(pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a71905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading the cleaned_dataset\n",
    "numerical_data.to_excel('data_RF.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67e744d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
