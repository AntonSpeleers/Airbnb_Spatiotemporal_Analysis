{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9cc1a1",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9056c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all packages needed \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "from datetime import datetime \n",
    "import ast\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import geopandas as gpd\n",
    "from scipy.stats import ttest_ind\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from pysal.lib.weights import KNN\n",
    "from pysal.explore import esda\n",
    "!pip install urbanaccess pandana\n",
    "\n",
    "\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Data/2. airbnb_data/Paris/Paris all detailed listings/Paris_Q1_detailed_listings.xlsx'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "dataframe1= pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b29ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quickly looking with which data types we are dealing\n",
    "print(dataframe1.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148792a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64073ce2",
   "metadata": {},
   "source": [
    "# Data cleaning listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2841a740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decided which columns will not be relevant for the first initial analysis\n",
    "columns_to_drop =[\"name\",\"description\",\"host_location\",\"host_thumbnail_url\",\"host_name\",\"bathrooms\",\n",
    "                  \"listing_url\",\"scrape_id\",\"last_scraped\", \"host_picture_url\",\"host_url\", \"host_has_profile_pic\", \n",
    "                  \"host_verifications\",\"source\",\"calendar_last_scraped\",\"license\",\"picture_url\",\"host_about\",\n",
    "                             \"neighbourhood\",\"neighbourhood_group_cleansed\",\"minimum_minimum_nights\",\n",
    "                             \"maximum_minimum_nights\",\"minimum_maximum_nights\",\"maximum_maximum_nights\",\n",
    "                             \"minimum_nights_avg_ntm\",\"maximum_nights_avg_ntm\",\"calendar_updated\",\n",
    "                             \"neighborhood_overview\",\"host_neighbourhood\", \"host_acceptance_rate\"]\n",
    "dataframe1 = dataframe1.drop(columns_to_drop, axis=1)\n",
    "\n",
    "#solving trailing white space problem\n",
    "string_columns = dataframe1.select_dtypes(include='object').columns.tolist()\n",
    "for i in string_columns:\n",
    "    dataframe1[i] = dataframe1[i].str.strip()\n",
    "\n",
    "#missing values have always been checked during this cleaning with the function: print(dataframe1.isnull().mean()) \n",
    "\n",
    "#host_repsonse_time, filling in the empty ones with \"unknown\"\n",
    "dataframe1.host_response_time.fillna(\"unknown\", inplace=True)\n",
    "dataframe1.host_response_time.value_counts(normalize=True)\n",
    "\n",
    "#adapting the host_response_rate column to better fit\n",
    "# Removing the % sign from the host_response_rate string and converting to an integer\n",
    "dataframe1.host_response_rate = dataframe1.host_response_rate.str[:-1].astype('float64')\n",
    "# Bin into four categories\n",
    "dataframe1.host_response_rate = pd.cut(dataframe1.host_response_rate, bins=[0, 50, 90, 99, 100], labels=['0-49%', '50-89%', '90-99%', '100%'], include_lowest=True)\n",
    "# Converting to string\n",
    "dataframe1.host_response_rate = dataframe1.host_response_rate.astype('str')\n",
    "# Replace nulls with 'unknown'\n",
    "dataframe1.host_response_rate.replace('nan', 'unknown', inplace=True)\n",
    "\n",
    "#these rows do not have that big of a missing value amount so we just delete the missing value rows here\n",
    "col = [\"host_since\",\"host_identity_verified\",\"host_listings_count\",\"host_total_listings_count\",\"host_is_superhost\"]\n",
    "for column in col:\n",
    "    dataframe1.dropna(subset=[column], inplace=True)\n",
    "\n",
    "# Category counts\n",
    "dataframe1.host_response_rate.value_counts()\n",
    "#fill out all NaN within string columns with ''\n",
    "for column in string_columns:\n",
    "    dataframe1[column] = dataframe1[column].fillna(\"\")   \n",
    "\n",
    "#fixing the \"bathroom_text\" column to only numbers and renaming it\n",
    "def extract_numeric(value):\n",
    "    numeric_part = re.search(r'\\d+\\.\\d+|\\d+', str(value))\n",
    "    return float(numeric_part.group()) if numeric_part else None\n",
    "dataframe1['bathrooms_text'] = dataframe1['bathrooms_text'].apply(extract_numeric)\n",
    "dataframe1.rename(columns={'bathrooms_text': 'bathrooms'}, inplace=True)\n",
    "\n",
    "#dropping those that still have no bathroom amount \n",
    "dataframe1.dropna(subset=[\"bathrooms\"], inplace=True)\n",
    "    \n",
    "#delete those that have no information about both beds and bedrooms\n",
    "dataframe1.dropna(subset=['beds', 'bedrooms'], how='all', inplace=True)\n",
    "\n",
    "#dropping those with +4 bedrooms (outliers) and no information about bedroom and between 1-4 beds is a studio so 0 bedrooms\n",
    "dataframe1.loc[(dataframe1['bedrooms'].isnull()) & (dataframe1['beds'].between(1, 4)), 'bedrooms'] = 0\n",
    "dataframe1.drop(dataframe1[(dataframe1['bedrooms'].isnull()) & (dataframe1['beds'] > 4)].index, inplace=True)\n",
    "\n",
    "#fill in all the other empty values with the amount of bedrooms \n",
    "dataframe1['beds'].fillna(dataframe1['bedrooms'], inplace=True)\n",
    "\n",
    "#dropping those that still have no bedroom amount \n",
    "dataframe1.dropna(subset=[\"bedrooms\"], inplace=True)\n",
    "\n",
    "dataframe1['first_review'] = pd.to_datetime(dataframe1['first_review']) \n",
    "# Calculating the number of days\n",
    "dataframe1['days_since_first_review'] = (datetime(2024, 1, 20) - dataframe1['first_review']).dt.days\n",
    "# Printing descriptives\n",
    "#dataframe1.hist(['days_since_first_review'], figsize=(15,5), bins=[0, 1*365, 2*365, 3*365, 4*365, 5*365, 6*365, 7*365, 8*365, 10*365, 11*365]), 9*365\n",
    "#DOING Binning time since last review\n",
    "dataframe1['days_since_first_first_review'] = pd.qcut(dataframe1['days_since_first_review'], q=5,\n",
    "                              labels=['Extremely active', 'Very active', 'Active', 'Inactive', 'Slumbering'])\n",
    "\n",
    "dataframe1['last_review'] = pd.to_datetime(dataframe1['last_review']) \n",
    "# Calculating the number of days\n",
    "dataframe1['days_since_last_review'] = (datetime(2024, 1, 20) - dataframe1['last_review']).dt.days\n",
    "# Printing descriptives\n",
    "#dataframe1.hist(['days_since_last_review'], figsize=(15,5), bins=[0, 1*365, 2*365, 3*365, 4*365, 5*365, 6*365, 7*365, 8*365, 10*365, 11*365]), 9*365\n",
    "#DOING Binning time since last review\n",
    "dataframe1['days_since_last_last_review'] = pd.qcut(dataframe1['days_since_last_review'], q=5,\n",
    "                              labels=['Extremely active', 'Very active', 'Active', 'Inactive', 'Slumbering'])\n",
    "\n",
    "\n",
    "dataframe1['host_since'] = pd.to_datetime(dataframe1['host_since']) \n",
    "# Calculating the number of days\n",
    "dataframe1['days_since_host'] = (datetime(2024, 1, 20) - dataframe1['host_since']).dt.days\n",
    "# Printing descriptives\n",
    "#dataframe1.hist(['days_since_host'], figsize=(15,5), bins=[0, 1*365, 2*365, 3*365, 4*365, 5*365, 6*365, 7*365, 8*365, 10*365, 11*365]), 9*365\n",
    "#DOING Binning time since last review\n",
    "dataframe1['days_since_host_host'] = pd.qcut(dataframe1['days_since_host'], q=5,\n",
    "                              labels=['Extremely active', 'Very active', 'Active', 'Inactive', 'Slumbering'])\n",
    "\n",
    "#Simplifying the property_types in to 4 categories\n",
    "dataframe1.property_type.replace({\n",
    "    'Barn': 'House',\n",
    "    'Boat': 'Other',\n",
    "    'Bus': 'Other',\n",
    "    'Camper/RV': 'Other',\n",
    "    'Casa particular': 'House',\n",
    "    'Cave': 'Other',\n",
    "    'Dome': 'Other',\n",
    "    'Earthen home': 'House',\n",
    "    'Entire bed and breakfast': 'Hotel',    \n",
    "    'Entire bungalow': 'House',\n",
    "    'Entire condo': 'Apartmen',\n",
    "    'Entire guesthouse': 'House',\n",
    "    'Entire home': 'House',\n",
    "    'Entire guest suite': 'Apartment',\n",
    "    'Entire rental unit': 'Apartment',\n",
    "    'Entire loft': 'Apartment',\n",
    "    'Entire home/apt': 'House',\n",
    "    'Entire place': 'House',\n",
    "    'Entire serviced apartment': 'Apartment',\n",
    "    'Entire townhouse': 'House',\n",
    "    'Entire villa': 'House',\n",
    "    'Entire vacation home': 'House',\n",
    "    'Floor': 'Other',\n",
    "    'Houseboat': 'Other',\n",
    "    'Private room': 'Apartment',\n",
    "    'Island': 'Other',\n",
    "    'Private room in bed and breakfast': 'Hotel',    \n",
    "    'Private room in boat': 'Other',\n",
    "    'Private room in casa particular': 'House',\n",
    "    'Private room in condo': 'Apartment',\n",
    "    'Private room in guest suite': 'Apartment',\n",
    "    'Private room in earthen home': 'House',\n",
    "    'Private room in home': 'House',\n",
    "    'Private room in guesthouse': 'House',\n",
    "    'Private room in loft': 'Apartment',\n",
    "    'Private room in hostel': 'Hotel',\n",
    "    'Private room in rental unit': 'Apartment',\n",
    "    'Private room in townhouse': 'House',\n",
    "    'Private room in tiny home': 'House',\n",
    "    'Private room in serviced apartment': 'Apartment',\n",
    "    'Room in bed and breakfast': 'Hotel',\n",
    "    'Private room in villa': 'House',\n",
    "    'Room in serviced apartment': 'Apartment',\n",
    "    'Room in boutique hotel': 'Hotel',\n",
    "    'Room in hotel': 'Hotel',\n",
    "    'Room in hostel': 'Hotel',\n",
    "    'Shared room in bed and breakfast': 'Hotel',\n",
    "    'Shared room in boutique hotel': 'Hotel',\n",
    "    'Shared room in cabin': 'Other',\n",
    "    'Shared room in boat': 'Other',\n",
    "    'Shared room in condo': 'Apartment',\n",
    "    'Shared room in farm stay': 'Other',\n",
    "    'Shared room in guesthouse': 'House',\n",
    "    'Shared room in casa particular': 'House',\n",
    "    'Shared room in ice dome': 'Other',\n",
    "    'Shared room in home': 'House',\n",
    "    'Shared room in hostel': 'Hotel',\n",
    "    'Shared room in hotel': 'Hotel',\n",
    "    'Shared room in rental unit': 'Apartment',\n",
    "    'Tiny home': 'House',\n",
    "    'Shared room in loft': 'Apartment',\n",
    "    'Shared room in townhouse': 'House',\n",
    "    'Shared room in tiny home': 'House',\n",
    "    }, inplace=True)\n",
    "\n",
    "# Replacing other categories with 'other'\n",
    "dataframe1.loc[~dataframe1.property_type.isin(['House', 'Apartment','Hotel']), 'property_type'] = 'Other'\n",
    "#dataframe1['property_type'].value_counts()\n",
    "\n",
    "#changing the name from neighbourhoud_cleansed to neighbourhood\n",
    "dataframe1.rename(columns={'neighbourhood_cleansed': 'neighbourhood'}, inplace=True)\n",
    "\n",
    "#convert the necessary columns to a boolean type, which is easier to use\n",
    "columns_to_convert = ['host_is_superhost', 'instant_bookable', 'host_identity_verified',\"has_availability\"] \n",
    "for column in columns_to_convert:\n",
    "    dataframe1[column] = dataframe1[column].replace({'f': False, 't': True}).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f3e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the string representation of lists to actual lists\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1419ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a unique list out of all the different kind of amenities that there are\n",
    "unique_items_set = set.union(*dataframe1[\"amenities\"].apply(set))\n",
    "print(unique_items_set)\n",
    "print(len(unique_items_set)) #first we come to  5829 different amenities\n",
    "#the one that is shown here is taken after the adjustments you can see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1b0f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with the code down below we will simplify the amenities to reduce the amount of amenities\n",
    "\n",
    "oven_items_set = {item for item in unique_items_set if 'oven' in item.lower()}\n",
    "def replace_oven_items(item_list):\n",
    "    return ['oven' if item in oven_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_oven_items)\n",
    "\n",
    "soap_items_set = {item for item in unique_items_set if 'soap' in item.lower()}\n",
    "def replace_soap_items(item_list):\n",
    "    return ['soap' if item in soap_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_soap_items)\n",
    "\n",
    "shampoo_items_set = {item for item in unique_items_set if 'shampoo' in item.lower()}\n",
    "def replace_shampoo_items(item_list):\n",
    "    return ['shampoo' if item in shampoo_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_shampoo_items)\n",
    "\n",
    "wifi_items_set = {item for item in unique_items_set if 'wifi' in item.lower()}\n",
    "def replace_wifi_items(item_list):\n",
    "    return ['wifi' if item in wifi_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_wifi_items)\n",
    "\n",
    "special_items_set = {item for item in unique_items_set if any(keyword in item for keyword in ['Netflix', 'Disney+', 'Amazon Prime'])}\n",
    "def replace_and_add_broadcast(item_list):\n",
    "    # Replace items from special_items_set with 'Broadcast'\n",
    "    item_list = ['broadcast' if item in special_items_set else item for item in item_list]\n",
    "    # Add 'TV' to the list if modified\n",
    "    modified = any(item == 'broadcast' for item in item_list)\n",
    "    if modified:\n",
    "        item_list.append('TV')\n",
    "    return item_list\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_and_add_broadcast)\n",
    "\n",
    "tv_items_set = {item for item in unique_items_set if 'tv' in item.lower()}\n",
    "def replace_tv_items(item_list):\n",
    "    return ['tv' if item in tv_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_tv_items)\n",
    "\n",
    "ref_items_set = {item for item in unique_items_set if 'refrigerator' in item.lower()}\n",
    "def replace_ref_items(item_list):\n",
    "    return ['refrigerator' if item in ref_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_ref_items)\n",
    "\n",
    "coffee_items_set = {item for item in unique_items_set if 'coffee' in item.lower()}\n",
    "def replace_coffee_items(item_list):\n",
    "    return ['coffee' if item in coffee_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_coffee_items)\n",
    "\n",
    "sound_items_set = {item for item in unique_items_set if any(keyword in item for keyword in ['sound system', 'Bluetooth'])}\n",
    "def replace_sound_items(item_list):\n",
    "    item_list = ['sound system' if item in sound_items_set else item for item in item_list]\n",
    "    return item_list\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_sound_items)\n",
    "\n",
    "stove_items_set = {item for item in unique_items_set if 'stove' in item.lower()}\n",
    "def replace_stove_items(item_list):\n",
    "    return ['stove' if item in stove_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_stove_items)\n",
    "\n",
    "cond_items_set = {item for item in unique_items_set if 'conditioner' in item.lower()}\n",
    "def replace_cond_items(item_list):\n",
    "    return ['conditioner' if item in cond_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_cond_items)\n",
    "\n",
    "park_items_set = {item for item in unique_items_set if 'parking' in item.lower()}\n",
    "def replace_park_items(item_list):\n",
    "    return ['parking' if item in park_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_park_items)\n",
    "\n",
    "clothing_items_set = {item for item in unique_items_set if 'clothing storage' in item.lower()}\n",
    "def replace_clothing_items(item_list):\n",
    "    return ['clothing storage' if item in clothing_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_clothing_items)\n",
    "\n",
    "exercise_items_set = {item for item in unique_items_set if 'exercise equipment' in item.lower()}\n",
    "def replace_exercise_items(item_list):\n",
    "    return ['exercise equipment' if item in exercise_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_exercise_items)\n",
    "\n",
    "children_items_set = {item for item in unique_items_set if 'children' in item.lower()}\n",
    "def replace_children_items(item_list):\n",
    "    return ['toys children' if item in children_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_children_items)\n",
    "\n",
    "bbq_items_set = {item for item in unique_items_set if 'bbq' in item.lower()}\n",
    "def replace_bbq_items(item_list):\n",
    "    return ['bbq' if item in bbq_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_bbq_items)\n",
    "\n",
    "pool_items_set = {item for item in unique_items_set if 'pool' in item.lower()}\n",
    "def replace_pool_items(item_list):\n",
    "    return ['pool' if item in pool_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_pool_items)\n",
    "\n",
    "hot_items_set = {item for item in unique_items_set if 'hot tub' in item.lower()}\n",
    "def replace_hot_items(item_list):\n",
    "    return ['hot tub' if item in hot_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_hot_items)\n",
    "\n",
    "backyard_items_set = {item for item in unique_items_set if 'backyard' in item.lower()}\n",
    "def replace_backyard_items(item_list):\n",
    "    return ['backyard' if item in backyard_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_backyard_items)\n",
    "\n",
    "gym_items_set = {item for item in unique_items_set if 'gym' in item.lower()}\n",
    "def replace_gym_items(item_list):\n",
    "    return ['gym' if item in gym_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_gym_items)\n",
    "\n",
    "view_items_set = {item for item in unique_items_set if 'view' in item.lower()}\n",
    "def replace_view_items(item_list):\n",
    "    return ['view' if item in view_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_view_items)\n",
    "\n",
    "crib_items_set = {item for item in unique_items_set if 'crib' in item.lower()}\n",
    "def replace_crib_items(item_list):\n",
    "    return ['crib' if item in crib_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_crib_items)\n",
    "\n",
    "gc_items_set = {item for item in unique_items_set if 'game console' in item.lower()}\n",
    "def replace_gc_items(item_list):\n",
    "    return ['game console' if item in gc_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_gc_items)\n",
    "\n",
    "sauna_items_set = {item for item in unique_items_set if 'sauna' in item.lower()}\n",
    "def replace_sauna_items(item_list):\n",
    "    return ['sauna' if item in sauna_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_sauna_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adfd144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#last step to clean the amenities is removing the ones that do not appear 1000 times or more in the column\n",
    "flat_list = [word for sublist in dataframe1[\"amenities\"] for word in sublist]\n",
    "word_counts = Counter(flat_list)\n",
    "filtered_word_set = {word for word, count in word_counts.items() if count < 1000}\n",
    "common_elements = list(filtered_word_set & unique_items_set)\n",
    "\n",
    "def remove_common_elements(item_list):\n",
    "    return [item for item in item_list if item not in common_elements]\n",
    "dataframe1['amenities'] = dataframe1['amenities'].apply(remove_common_elements)\n",
    "\n",
    "unique_items_set = set.union(*dataframe1[\"amenities\"].apply(set))\n",
    "#only 95 unique items left, which is an acceptable amount and now we can make columns out of these ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c35ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making new columns for the amenities indivually with a 0 or 1 inside of them \n",
    "for item in unique_items_set:\n",
    "    dataframe1[item] = dataframe1[\"amenities\"].apply(lambda x: int(item in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6afc7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will be able to remove the 'amenities' column\n",
    "dataframe1 = dataframe1.drop('amenities', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd53e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of amenities for each listing and make this a new column\n",
    "dataframe1['total_amenities'] = dataframe1.iloc[:, 50:-1].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77638734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns with exclusively 0s and 1s to boolean\n",
    "for col in dataframe1.columns:\n",
    "    unique_values = dataframe1[col].unique()\n",
    "    if set(unique_values).issubset({0, 1}):\n",
    "        dataframe1[col] = dataframe1[col].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5af036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming your dataset is stored in a DataFrame named \n",
    "\n",
    "# Define the columns for which you want to calculate z-scores\n",
    "columns_of_interest = ['beds', 'bedrooms', 'bathrooms', 'price']\n",
    "\n",
    "# Calculate z-scores for the specified columns\n",
    "z_scores = (dataframe1[columns_of_interest] - dataframe1[columns_of_interest].mean()) / dataframe1[columns_of_interest].std()\n",
    "\n",
    "# Define a threshold for the z-score (e.g., 3)\n",
    "threshold = 3\n",
    "\n",
    "# Filter out rows where any z-score exceeds the threshold\n",
    "dataframe1 = dataframe1[(np.abs(z_scores) < threshold).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80866990",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ca898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading the cleaned_dataset\n",
    "dataframe1.to_excel('cleaned_listing_dataset.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ac62b5",
   "metadata": {},
   "source": [
    "# Data cleaning Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7283fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "#reading in the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Data/2. airbnb_data/Paris/Q1 airbnb_data Paris/calendar/calendar.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28da4a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2976d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['price'] = data['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "data['adjusted_price'] = data['adjusted_price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "data['date']=pd.to_datetime(data['date'])\n",
    "data['weekday'] = pd.Series(data.date).dt.dayofweek\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1c2c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c896ac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the missing values from the dataset after seeing that there are not that many missing values\n",
    "data.dropna(inplace=True)\n",
    "print(data.isnull().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a625dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the columns for which you want to calculate z-scores\n",
    "columns_of_interest = ['price']\n",
    "\n",
    "# Calculate z-scores for the specified columns\n",
    "z_scores = (data[columns_of_interest] - data[columns_of_interest].mean()) / data[columns_of_interest].std()\n",
    "\n",
    "# Define a threshold for the z-score (e.g., 3)\n",
    "threshold = 3\n",
    "\n",
    "# Filter out rows where any z-score exceeds the threshold\n",
    "data = data[(np.abs(z_scores) < threshold).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e2cfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12091262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading the cleaned_dataset\n",
    "data.to_excel('cleaned_calendar_dataset.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c41084",
   "metadata": {},
   "source": [
    "# Data exploration Calendar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdc39d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import csv\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Taking average/median values as well as plotting\n",
    "date = []\n",
    "avg_price = []\n",
    "median_price = []\n",
    "avg_adjusted_price = []\n",
    "median_adjusted_price = []\n",
    "\n",
    "for i in data['date'].unique():\n",
    "    date.append(i)\n",
    "    avg_price.append(data[data['date'] == i]['price'].mean())\n",
    "    median_price.append(data[data['date'] == i]['price'].median())\n",
    "    avg_adjusted_price.append(data[data['date'] == i]['adjusted_price'].mean())\n",
    "    median_adjusted_price.append(data[data['date'] == i]['adjusted_price'].median())\n",
    "\n",
    "plt.plot(range(len(avg_price)), avg_price, label=\"Average Price\")\n",
    "plt.plot(range(len(avg_price)), median_price, color='red', label=\"Median Price\")\n",
    "plt.plot(range(len(avg_adjusted_price)), avg_adjusted_price, label=\"Average Adjusted Price\")\n",
    "plt.plot(range(len(avg_adjusted_price)), median_adjusted_price, color='green', label=\"Median Adjusted Price\")\n",
    "\n",
    "plt.ylabel('Price($)')\n",
    "plt.xlabel('Day of Q1 2023 - Q1 2024')\n",
    "plt.title('Average and Median Price of Q1 2023 - Q1 2024 AirBnB Listings Paris')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1672079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing the same but only for the first 70 days\n",
    "plt.plot(range(len(avg_price[0:70])), avg_price[0:70], label=\"Average Price\")\n",
    "plt.plot(range(len(avg_price[0:70])), median_price[0:70], color='red', label=\"Median Price\")\n",
    "plt.plot(range(len(avg_adjusted_price[0:70])), avg_adjusted_price[0:70], label=\"Average Adjusted Price\")\n",
    "plt.plot(range(len(avg_adjusted_price[0:70])), median_adjusted_price[0:70], color='green', label=\"Median Adjusted Price\")\n",
    "\n",
    "plt.ylabel('Price($)')\n",
    "plt.xlabel('Day of Q1 2023 - Q1 2024')\n",
    "plt.title('Average and Median Price of First 70 Days of AirBnB Listings')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968e8f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at which day is the most expensive on average and median\n",
    "daily_avg_price = []\n",
    "daily_median_price = []\n",
    "b = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "for i in range(7):\n",
    "    daily_avg_price.append(data[data['weekday'] == i]['price'].mean())\n",
    "    daily_median_price.append(data[data['weekday'] == i]['price'].median())\n",
    "\n",
    "plt.bar(range(len(daily_avg_price)), daily_avg_price, width=1 / 1.5, label=\"Average Price\")\n",
    "plt.bar(range(len(daily_median_price)), daily_median_price, width=1 / 1.5, color='red', label=\"Median Price\")\n",
    "\n",
    "plt.xticks(range(len(daily_avg_price)), b)\n",
    "plt.ylabel('Price($)')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.title('Average and Median Price per Day of the Week')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for i in range(7):\n",
    "    avg_price = data[data['weekday'] == i]['price'].mean()\n",
    "    median_price = data[data['weekday'] == i]['price'].median()\n",
    "    \n",
    "    daily_avg_price.append(avg_price)\n",
    "    daily_median_price.append(median_price)\n",
    "\n",
    "    print(f\"{b[i]} - Average Price: ${avg_price:.2f}, Median Price: ${median_price:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f9de38",
   "metadata": {},
   "source": [
    "# Data exploration listings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc94cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distributions of the review ratings columns\n",
    "variables_to_plot = list(dataframe1.columns[dataframe1.columns.str.startswith(\"review_scores\") == True])\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "for i, var_name in enumerate(variables_to_plot):\n",
    "    ax = fig.add_subplot(3,3,i+1)\n",
    "    dataframe1[var_name].hist(bins=10,ax=ax)\n",
    "    ax.set_title(var_name)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#out of these we see that most of them are rated quite positive, with the biggest amount around 4-5/5 stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a3d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Paris neighbourhood  GeoJSON file as a dataframe in geopandas\n",
    "map_dataframe1 = gpd.read_file('C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Data/2. airbnb_data/Paris/Q1 airbnb_data Paris/neighbourhoods.geojson')\n",
    "map_dataframe1.drop('neighbourhood_group', axis = 1, inplace = True)\n",
    "map_dataframe1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d97c2c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#setting the characteristics for the following plot\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.rcParams['figure.figsize'] = 280, 280\n",
    "\n",
    "\n",
    "# Creating a dataframe of listing counts and median price by neighbourhood\n",
    "neighbourhood_dataframe1 = pd.DataFrame(dataframe1.groupby('neighbourhood').size())\n",
    "neighbourhood_dataframe1.rename(columns={0: 'number_of_listings'}, inplace=True)\n",
    "neighbourhood_dataframe1['median_price'] = dataframe1.groupby('neighbourhood').price.median().values\n",
    "neighbourhood_dataframe1['mean_price'] = dataframe1.groupby('neighbourhood').price.mean().values\n",
    "\n",
    "# Putting the dataframes together\n",
    "neighbourhood_map_dataframe1 = map_dataframe1.set_index('neighbourhood').join(neighbourhood_dataframe1)\n",
    "                  \n",
    "# Plotting the number of listings in each neighbourhood\n",
    "fig1, ax1 = plt.subplots(1, figsize=(15, 6)) #deciding on plot size\n",
    "neighbourhood_map_dataframe1.plot(column='number_of_listings', cmap='Reds', ax=ax1, rasterized=True) \n",
    "#rasterized to makes it easier for big data sets + ax1 earlier defined\n",
    "\n",
    "ax1.axis('off') #disabling the axis components, including axis labels, ticks, and the frame surrounding the plot\n",
    "ax1.set_title('Amount of listings per neighbourhood in Paris', fontsize=14)\n",
    "cax1 = fig1.add_axes([0.9, 0.1, 0.03, 0.8]) # Adjusted the position and size as needed for the side axis [left, bottom, width, height]\n",
    "#ScalarMappable to make a color map + norm, to normalize the coloring within the graph \n",
    "sm1 = plt.cm.ScalarMappable(cmap='Reds', norm=plt.Normalize(vmin=0, vmax=9000))\n",
    "sm1._A = [] # The primary reason for doing this is to provide an empty array that will later be populated with the data range when the plot is created. The colorbar uses this array to decide the color scaling for the colormap.\n",
    "cbar1 = fig1.colorbar(sm1, cax=cax1) \n",
    "plt.show()\n",
    "\n",
    "# Plotting the median price of listings in each neighbourhood\n",
    "fig2, ax2 = plt.subplots(1, figsize=(15, 6))\n",
    "neighbourhood_map_dataframe1.plot(column='median_price', cmap='Reds', ax=ax2)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Median price per neighbourhood in Paris', fontsize=14)\n",
    "cax2 = fig2.add_axes([0.9, 0.1, 0.03, 0.8])  # Adjusted the position and size as needed for the side axis [left, bottom, width, height]\n",
    "#ScalarMappable to make a color map + norm, to normalize the coloring within the graph \n",
    "sm2 = plt.cm.ScalarMappable(cmap='Reds', norm=plt.Normalize(vmin=min(neighbourhood_map_dataframe1.median_price), vmax=max(neighbourhood_map_dataframe1.median_price))) \n",
    "sm2._A = [] # The primary reason for doing this is to provide an empty array that will later be populated with the data range when the plot is created. The colorbar uses this array to decide the color scaling for the colormap.\n",
    "cbar2 = fig2.colorbar(sm2, cax=cax2)\n",
    "plt.show()\n",
    "\n",
    "# Plotting the mean price of listings in each neighbourhood\n",
    "fig3, ax3 = plt.subplots(1, figsize=(15, 6))\n",
    "neighbourhood_map_dataframe1.plot(column='mean_price', cmap='Reds', ax=ax3)\n",
    "ax3.axis('off')\n",
    "ax3.set_title('Mean price per neighbourhood in Paris', fontsize=14)\n",
    "cax3 = fig3.add_axes([0.9, 0.1, 0.03, 0.8])  # Adjusted the position and size as needed for the side axis [left, bottom, width, height]\n",
    "#ScalarMappable to make a color map + norm, to normalize the coloring within the graph \n",
    "sm3 = plt.cm.ScalarMappable(cmap='Reds', norm=plt.Normalize(vmin=min(neighbourhood_map_dataframe1.mean_price), vmax=max(neighbourhood_map_dataframe1.mean_price))) \n",
    "sm3._A = [] # The primary reason for doing this is to provide an empty array that will later be populated with the data range when the plot is created. The colorbar uses this array to decide the color scaling for the colormap.\n",
    "cbar3 = fig3.colorbar(sm3, cax=cax3)\n",
    "plt.show()\n",
    "\n",
    "print(neighbourhood_dataframe1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad82985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot number of listings and median price in one bar plot\n",
    "plt.rcParams['font.size'] = 15\n",
    "plt.rcParams['figure.figsize'] = 25, 5\n",
    "selected_columns = ['number_of_listings', 'median_price']\n",
    "neighbourhood_dataframe1_selected = neighbourhood_dataframe1[selected_columns]\n",
    "neighbourhood_dataframe1_sorted = neighbourhood_dataframe1_selected.sort_values('median_price', ascending=False)\n",
    "neighbourhood_dataframe1_sorted.plot( kind= 'bar' , secondary_y= 'median_price' , rot= 90 )\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "# Plot number of listings and mean price in one bar plot\n",
    "plt.rcParams['font.size'] = 15\n",
    "plt.rcParams['figure.figsize'] = 25, 5\n",
    "selected_columns = ['number_of_listings', 'mean_price']\n",
    "neighbourhood_dataframe1_selected = neighbourhood_dataframe1[selected_columns]\n",
    "neighbourhood_dataframe1_sorted = neighbourhood_dataframe1_selected.sort_values('mean_price', ascending=False)\n",
    "neighbourhood_dataframe1_sorted.plot( kind= 'bar' , secondary_y= 'mean_price' , rot= 90 )\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f63c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_count_and_price_plot(column_name,alpha=0.05):\n",
    "    # Assuming dataframe1 is your DataFrame\n",
    "    \n",
    "    # Define color palette with hex color codes\n",
    "    blue_color = '#1f77b4'\n",
    "    orange_color = '#ff7f0e'\n",
    "    \n",
    "    # Plot count distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.countplot(x=column_name, data=dataframe1,palette=[blue_color,orange_color])\n",
    "    plt.title(f'Count distribution of {column_name}')\n",
    "\n",
    "    # Plot median price distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    median_prices = dataframe1.groupby(column_name)['price'].median()\n",
    "    median_prices.plot(kind='bar', color=[blue_color,orange_color])\n",
    "    plt.title(f'Median price distribution by {column_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate t-test for median prices\n",
    "    true_prices = dataframe1[dataframe1[column_name] == True]['price']\n",
    "    false_prices = dataframe1[dataframe1[column_name] == False]['price']\n",
    "    t_stat, p_value = ttest_ind(true_prices, false_prices)\n",
    "   \n",
    "    # Display additional information\n",
    "    percent_split = dataframe1[column_name].value_counts(normalize=True) * 100\n",
    "    print(f'% Wise Split of {column_name}:\\n{percent_split}')\n",
    "    t_statistic, p_value = ttest_ind(true_prices, false_prices, equal_var=False)  # assuming unequal variances\n",
    "    print(f'\\nP-value for {column_name}: {p_value}')\n",
    "    \n",
    "    # Check if p-value is greater than or equal to alpha\n",
    "    if p_value >= alpha:\n",
    "        print(f\"{column_name} is not statistically significant (p-value >= {alpha})\")\n",
    "        return column_name\n",
    "    else:\n",
    "        print(f\"{column_name} is statistically significant (p-value < {alpha})\")\n",
    "        return None  # or any other value if needed\n",
    "\n",
    "superhost = binary_count_and_price_plot('host_is_superhost')\n",
    "print(dataframe1.host_is_superhost.value_counts(normalize=True))\n",
    "\n",
    "test2 = binary_count_and_price_plot('host_identity_verified')\n",
    "print(dataframe1.host_identity_verified.value_counts(normalize=True))\n",
    "\n",
    "test3 = binary_count_and_price_plot('instant_bookable')\n",
    "print(dataframe1.instant_bookable.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db725883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names and their indices\n",
    "column_info = list(enumerate(dataframe1.columns))\n",
    "# Print column names and their indices\n",
    "for index, column_name in column_info:\n",
    "    print(f\"Column {index}: {column_name}\")\n",
    "    \n",
    "#now we know that the amenity columns start from index 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590ff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing the analysis for all the different amenities\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['figure.figsize'] = 20, 20\n",
    "result = [binary_count_and_price_plot(col) for col in dataframe1.iloc[:,51:-2].columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e109df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we are making a comparison against the rating and seeing if having the amenity might impact the rating\n",
    "\n",
    "def binary_count_and_rating_plot(column_name,alpha=0.05):\n",
    "    # Only taking the ones with reviews into consideration\n",
    "    data = dataframe1[[column_name, 'review_scores_rating']].dropna()\n",
    "\n",
    "    # Plot count distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.countplot(x=column_name, data=data)\n",
    "    plt.title(f'Count distribution of {column_name}')\n",
    "\n",
    "    # Plot median price distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    rating_mean = data.groupby(column_name)['review_scores_rating'].mean()\n",
    "    rating_mean.plot(kind='bar', color=['blue', 'orange'])\n",
    "    plt.title(f'Average rating distribution by {column_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate t-test for median prices\n",
    "    true_ratings = data[data[column_name] == True]['review_scores_rating']\n",
    "    false_ratings = data[data[column_name] == False]['review_scores_rating']\n",
    "    t_stat, p_value = ttest_ind(true_ratings, false_ratings)\n",
    "   \n",
    "    # Display additional information\n",
    "    percent_split = data[column_name].value_counts(normalize=True) * 100\n",
    "    print(f'% Wise Split of {column_name}:\\n{percent_split}')\n",
    "    t_statistic, p_value = ttest_ind(true_ratings, false_ratings, equal_var=False)  # assuming unequal variances\n",
    "    print(f'\\nP-value for {column_name}: {p_value}')\n",
    "    \n",
    "    # Check if p-value is greater than or equal to alpha\n",
    "    if p_value >= alpha:\n",
    "        print(f\"{column_name} is not statistically significant (p-value >= {alpha})\")\n",
    "        return column_name\n",
    "    else:\n",
    "        print(f\"{column_name} is statistically significant (p-value < {alpha})\")\n",
    "        return None  # or any other value if needed\n",
    "    \n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['figure.figsize'] = 20, 20\n",
    "result = [binary_count_and_rating_plot(col) for col in dataframe1.iloc[:,51:-2].columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0bb6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def collect_pvalues(columns_of_interest):\n",
    "    all_pvalues = {}\n",
    "\n",
    "    for column_name in columns_of_interest:\n",
    "        # Drop rows with missing values in the specified columns\n",
    "        data = dataframe1[[column_name, 'price']].dropna()\n",
    "\n",
    "        # Calculate t-test for median prices\n",
    "        true_prices = data[data[column_name] == True]['price']\n",
    "        false_prices = data[data[column_name] == False]['price']\n",
    "        t_statistic, p_value = ttest_ind(true_prices, false_prices, equal_var=False)  # assuming unequal variances\n",
    "        all_pvalues[column_name] = {'p_value': p_value, 't_statistic': t_statistic}\n",
    "\n",
    "    return all_pvalues\n",
    "\n",
    "columns_of_interest = dataframe1.iloc[:, 51:-2].columns.values\n",
    "result = collect_pvalues(columns_of_interest)\n",
    "# Extract p-values and create a dictionary\n",
    "\n",
    "all_pvalues = {}\n",
    "for column in result:\n",
    "    p_value = result[column]['p_value']\n",
    "    if isinstance(p_value, np.float64):\n",
    "        all_pvalues[column] = p_value\n",
    "    else:\n",
    "        all_pvalues[column] = [float(val) for val in p_value]\n",
    "\n",
    "import math\n",
    "#P-values of effects of each amanity on price can be visualized using the 'Manhattan-plot' approach: taking the minus log of the P-value:\n",
    "# -log Pvalues and order\n",
    "all_pvalues.update({k: -1*np.log(v) for k, v in all_pvalues.items()})\n",
    "ordereddict = OrderedDict(sorted(all_pvalues.items(), key=lambda t: t[1]))\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['figure.figsize'] = 12, 30\n",
    "plt.barh(list(ordereddict.keys()), list(ordereddict.values()),color='g', height=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ec67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['figure.figsize'] = 16, 8\n",
    "\n",
    "print(f\"Of the Airbnb hosts that are still listing on the site, the first joined on {min(dataframe1.host_since).strftime('%d %B %Y')}, and the most recent joined on {max(dataframe1.host_since).strftime('%d %B %Y')}.\")\n",
    "\n",
    "\n",
    "dataframe1.set_index('host_since').resample('MS').size().plot(label='Hosts joining Airbnb', color='orange')\n",
    "dataframe1.set_index('first_review').resample('MS').size().plot(label='Listings getting their first review', color='green')\n",
    "plt.title('Paris hosts registration on Airbnb, numbers of first reviews per accommodation per month')\n",
    "plt.legend()\n",
    "plt.xlim('2008-01-01', '2019-11-30') # Limiting to whole months\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae5a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "\n",
    "def decompose_time_series(ts, title=''):\n",
    "    decomposition = seasonal_decompose(ts)\n",
    "    trend = decomposition.trend\n",
    "    seasonal = decomposition.seasonal\n",
    "    residual = decomposition.resid\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(411)\n",
    "    plt.plot(ts, label='Original')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(f'{title} - Original')\n",
    "\n",
    "    plt.subplot(412)\n",
    "    plt.plot(trend, label='Trend')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(f'{title} - Trend')\n",
    "\n",
    "    plt.subplot(413)\n",
    "    plt.plot(seasonal, label='Seasonal')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(f'{title} - Seasonal')\n",
    "\n",
    "    plt.subplot(414)\n",
    "    plt.plot(residual, label='Residual')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(f'{title} - Residual')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Creating dataframes for time series analysis\n",
    "ts_host_since = pd.DataFrame(dataframe1.set_index('host_since').resample('MS').size())\n",
    "ts_first_review = pd.DataFrame(dataframe1.set_index('first_review').resample('MS').size())\n",
    "\n",
    "# Renaming columns\n",
    "ts_host_since = ts_host_since.rename(columns={0: 'hosts'})\n",
    "ts_host_since.index.rename('month', inplace=True)\n",
    "ts_first_review = ts_first_review.rename(columns={0: 'reviews'})\n",
    "ts_first_review.index.rename('month', inplace=True)\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['figure.figsize'] = 50, 70\n",
    "decompose_time_series(ts_host_since, title='Number of registrations on Airbnb by month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7dccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(dataframe1['reviews_per_month'].dropna(), bins=50, kde=True)\n",
    "plt.xlabel('Reviews Per Month')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Reviews Per Month Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074a60f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Create a base map using the mean latitude and longitude values\n",
    "mean_lat = dataframe1['latitude'].mean()\n",
    "mean_lon = dataframe1['longitude'].mean()\n",
    "\n",
    "m = folium.Map(location=[mean_lat, mean_lon], zoom_start=12)\n",
    "\n",
    "# Create a HeatMap layer with price as the intensity\n",
    "heat_data = [[row['latitude'], row['longitude'], row['price']] for index, row in dataframe1.iterrows()]\n",
    "HeatMap(heat_data, radius=15, blur=10).add_to(m)\n",
    "\n",
    "# Display the map in Jupyter Notebook\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f680a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Finding out the neighborhoods that correspond with the rules we've established.\n",
    "top_6 = (dataframe1.groupby(['neighbourhood'])['id'].count()\n",
    "        .sort_values(ascending = False).head(6))\n",
    "\n",
    "# Let's make a list containing all the 'top_6' Series neighborhoods.\n",
    "desirable_neighborhoods = list(top_6.index)\n",
    "\n",
    "# Here, we'll perform a group by. It will list all neighborhoods \n",
    "# and their property types available. Also, it will count the number of \n",
    "# dwellings each residence kind has.\n",
    "property_types_count = dataframe1.groupby(['neighbourhood','property_type'], as_index = False)['id'].count()\n",
    "property_types_count = (property_types_count.sort_values(by = 'id', ascending = False)) \n",
    "\n",
    "# Let's make a list containing all the 'top_6' Series neighborhoods.\n",
    "desirable_neighborhoods = list(top_6.index)\n",
    "\n",
    "# We'll  filter the DF so that it only contains residences registered \n",
    "# in the 'desirable_neighborhoods' list.\n",
    "property_types_count = (property_types_count[property_types_count['neighbourhood']\n",
    "                                             .isin(desirable_neighborhoods)])\n",
    "\n",
    "# And that's it! We are set to plot the chart!\n",
    "import plotly.express as px\n",
    "\n",
    "# Unfortunately plotly has a bug for the sunburst plot \n",
    "# in which it does not recognize the 'id' column for the chart creation. \n",
    "\n",
    "# Hence we'll have to make a second column with the same data.\n",
    "# We are going to call the new column 'Number of Dwellings'\n",
    "property_types_count['Number of Dwellings'] = property_types_count['id']\n",
    "\n",
    "# Finally plotting the sunburst chart!\n",
    "figure = px.sunburst(property_types_count, values = 'Number of Dwellings', \n",
    "                     path = ['neighbourhood','property_type'], width = 600, height = 600, \n",
    "                     title = 'Dwelling Type Composition')\n",
    "\n",
    "figure.update_traces(textinfo=\"label+percent parent\")\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0959a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Cut the 'review_scores_value' into categories\n",
    "categories = pd.cut(dataframe1['review_scores_value'].dropna(), 4, labels=['Poor', 'Average', 'Good', 'Excellent'])\n",
    "dataframe1['Lodging Quality'] = categories\n",
    "\n",
    "# Filter out residences with no customer evaluation\n",
    "no_nan = dataframe1[~dataframe1['review_scores_value'].isnull()]\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Group by 'Lodging Quality' and calculate the average price for each category\n",
    "groups = no_nan.groupby('Lodging Quality')['price'].mean()\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the bar chart with adjusted ylim\n",
    "chart = groups.plot(kind='bar', ylim=(0, 300), xlabel='', yticks=[], fontsize=12, color=['grey', 'grey', 'darkred', 'grey'])\n",
    "\n",
    "# Display average prices on top of the bars\n",
    "for i, price in enumerate(groups):\n",
    "    plt.text(i, price + 10, f'${price:.2f}', ha='center', va='bottom', fontdict={'size': 12})\n",
    "\n",
    "# Set the graph title\n",
    "plt.title('Average Price per Dwelling Quality', fontdict={'size': 15})\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc42025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing the distribution between price and amount of bedrooms\n",
    "figsize = (10, 5)\n",
    "x = 'bedrooms'\n",
    "\n",
    "# Create a grid with 2 rows and 1 column\n",
    "fig = plt.figure(figsize=figsize)\n",
    "gs = gridspec.GridSpec(1, 1)\n",
    "\n",
    "# Bar plot for price distribution\n",
    "ax1 = plt.subplot(gs[0])\n",
    "sns.barplot(data=dataframe1, x=x, y='price', ax=ax1)\n",
    "ax1.set_title(f'Distribution of Price based on {x}')\n",
    "\n",
    "# Calculate the percentage of each bedroom count relative to all listings\n",
    "bedroom_percentage = dataframe1[x].value_counts(normalize=True) * 100\n",
    "\n",
    "# Display the percentage of each bedroom count\n",
    "for bedrooms, percentage in bedroom_percentage.items():\n",
    "    print(f'The percentage of listings with {bedrooms} bedroom(s) is: {percentage:.2f}%')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d55010",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing the distribution between price and amount of beds\n",
    "figsize = (10, 5)\n",
    "x = 'beds'\n",
    "\n",
    "# Create a grid with 2 rows and 1 column\n",
    "fig = plt.figure(figsize=figsize)\n",
    "gs = gridspec.GridSpec(1, 1)\n",
    "\n",
    "# Bar plot for price distribution\n",
    "ax1 = plt.subplot(gs[0])\n",
    "sns.barplot(data=dataframe1, x=x, y='price', ax=ax1)\n",
    "ax1.set_title(f'Distribution of Price based on {x}')\n",
    "\n",
    "# Calculate the percentage of each bedroom count relative to all listings\n",
    "bed_percentage = dataframe1[x].value_counts(normalize=True) * 100\n",
    "\n",
    "# Display the percentage of each bedroom count\n",
    "for beds, percentage in bed_percentage.items():\n",
    "    print(f'The percentage of listings with {beds} bed(s) is: {percentage:.2f}%')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2c9e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing the distribution between price and amount of bathrooms\n",
    "figsize = (10, 5)\n",
    "x = 'bathrooms'\n",
    "\n",
    "# Create a grid with 2 rows and 1 column\n",
    "fig = plt.figure(figsize=figsize)\n",
    "gs = gridspec.GridSpec(1, 1)\n",
    "\n",
    "# Bar plot for price distribution\n",
    "ax1 = plt.subplot(gs[0])\n",
    "sns.barplot(data=dataframe1, x=x, y='price', ax=ax1)\n",
    "ax1.set_title(f'Distribution of Price based on {x}')\n",
    "\n",
    "# Calculate the percentage of each bedroom count relative to all listings\n",
    "bathroom_percentage = dataframe1[x].value_counts(normalize=True) * 100\n",
    "\n",
    "# Display the percentage of each bedroom count\n",
    "for bathrooms, percentage in bathroom_percentage.items():\n",
    "    print(f'The percentage of listings with {bathrooms} bathroom(s) is: {percentage:.2f}%')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8448ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing the distribution between price and amount of bathrooms\n",
    "figsize = (10, 5)\n",
    "x = 'room_type'\n",
    "\n",
    "# Create a grid with 2 rows and 1 column\n",
    "fig = plt.figure(figsize=figsize)\n",
    "gs = gridspec.GridSpec(1, 1)\n",
    "\n",
    "# Bar plot for price distribution\n",
    "ax1 = plt.subplot(gs[0])\n",
    "sns.barplot(data=dataframe1, x=x, y='price', ax=ax1)\n",
    "ax1.set_title(f'Distribution of Price based on {x}')\n",
    "\n",
    "# Calculate the percentage of each bedroom count relative to all listings\n",
    "room_type_percentage = dataframe1[x].value_counts(normalize=True) * 100\n",
    "\n",
    "# Display the percentage of each bedroom count\n",
    "for room_type, percentage in room_type_percentage.items():\n",
    "    print(f'The percentage of listings with {room_type} room type(s) is: {percentage:.2f}%')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a3819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing the distribution between price and amount of bathrooms\n",
    "figsize = (10, 5)\n",
    "x = 'accommodates'\n",
    "\n",
    "# Create a grid with 2 rows and 1 column\n",
    "fig = plt.figure(figsize=figsize)\n",
    "gs = gridspec.GridSpec(1, 1)\n",
    "\n",
    "# Bar plot for price distribution\n",
    "ax1 = plt.subplot(gs[0])\n",
    "sns.barplot(data=dataframe1, x=x, y='price', ax=ax1)\n",
    "ax1.set_title(f'Distribution of Price based on {x}')\n",
    "\n",
    "# Calculate the percentage of each bedroom count relative to all listings\n",
    "accommodates_percentage = dataframe1[x].value_counts(normalize=True) * 100\n",
    "\n",
    "# Display the percentage of each bedroom count\n",
    "for accommodates, percentage in accommodates_percentage.items():\n",
    "    print(f'The percentage of listings with {accommodates} accommodates is: {percentage:.2f}%')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6bcd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of amenities for each listing\n",
    "dataframe1['total_amenities'] = dataframe1.iloc[:, 51:-1].sum(axis=1)\n",
    "\n",
    "#showing the distribution between price and amount of bathrooms\n",
    "figsize = (10, 5)\n",
    "x = 'total_amenities'\n",
    "\n",
    "# Create a grid with 2 rows and 1 column\n",
    "fig = plt.figure(figsize=figsize)\n",
    "gs = gridspec.GridSpec(1, 1)\n",
    "\n",
    "# Bar plot for price distribution\n",
    "ax1 = plt.subplot(gs[0])\n",
    "sns.barplot(data=dataframe1, x=x, y='price', ax=ax1)\n",
    "ax1.set_title(f'Distribution of Price based on {x}')\n",
    "\n",
    "# Calculate the percentage of each bedroom count relative to all listings\n",
    "total_amenities_percentage = dataframe1[x].value_counts(normalize=True) * 100\n",
    "\n",
    "# Display the percentage of each bedroom count\n",
    "for total_amenities, percentage in total_amenities_percentage.items():\n",
    "    print(f'The percentage of listings with {total_amenities} total_amenities is: {percentage:.2f}%')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c858ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "#Trying to avoid the memory leak of kmeans KML\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "# Set up the map centered around Paris\n",
    "paris_map = folium.Map(location=[48.8566, 2.3522], zoom_start=12)\n",
    "# Add cluster centers to the map using MarkerCluster\n",
    "marker_cluster = MarkerCluster().add_to(paris_map)\n",
    "\n",
    "#making a new dataframe for the locations of the listings\n",
    "data = {\n",
    "    'latitude': dataframe1['latitude'].head(100).tolist(),\n",
    "    'longitude': dataframe1['longitude'].head(100).tolist(),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Reverse geocoding to extract postal codes and ranks\n",
    "geolocator = Nominatim(user_agent=\"geo_analyzer\")\n",
    "\n",
    "df['location'] = df.apply(lambda row: geolocator.reverse((row['latitude'], row['longitude'])), axis=1)\n",
    "df['rank'] = df['location'].apply(lambda loc: loc.raw.get('importance', None))\n",
    "\n",
    "def rank_to_color(rank):\n",
    "    if rank is not None:\n",
    "        rank = float(rank)  # Convert rank to float for better precision\n",
    "        if rank > 0.5:\n",
    "            return 'darkgreen'\n",
    "        elif 0.3 < rank <= 0.5:\n",
    "            return 'green'\n",
    "        elif 0.1 < rank <= 0.3:\n",
    "            return 'lightgreen'\n",
    "        elif 0.05 < rank <= 0.1:\n",
    "            return 'lightorange'\n",
    "        elif rank <= 0.05:\n",
    "            return 'darkorange'\n",
    "    return 'gray'\n",
    "\n",
    "# Step 3: KMeans Clustering with Euclidean distance metric\n",
    "your_desired_clusters = dataframe1['neighbourhood'].nunique()\n",
    "kmeans = KMeans(n_clusters=your_desired_clusters, n_init=10)\n",
    "df['cluster_kmeans'] = kmeans.fit_predict(df[['latitude', 'longitude']])\n",
    "\n",
    "# Calculate average rank for each cluster\n",
    "cluster_avg_rank = df.groupby('cluster_kmeans')['rank'].mean().to_dict()\n",
    "df['cluster_avg_rank'] = df['cluster_kmeans'].map(cluster_avg_rank)\n",
    "\n",
    "# Create a dictionary to store cluster information\n",
    "clusters_info = {}\n",
    "\n",
    "# Iterate through each row to populate the clusters_info dictionary\n",
    "for idx, row in df.iterrows():\n",
    "    cluster_label = row['cluster_kmeans']\n",
    "    \n",
    "    # If cluster_label is not in clusters_info, create an entry\n",
    "    if cluster_label not in clusters_info:\n",
    "        clusters_info[cluster_label] = {\n",
    "            'latitude_sum': row['latitude'],\n",
    "            'longitude_sum': row['longitude'],\n",
    "            'avg_rank': cluster_avg_rank[cluster_label],\n",
    "            'count': 1\n",
    "        }\n",
    "    else:\n",
    "        # Update sum of coordinates, average rank, and count for existing cluster\n",
    "        clusters_info[cluster_label]['latitude_sum'] += row['latitude']\n",
    "        clusters_info[cluster_label]['longitude_sum'] += row['longitude']\n",
    "        clusters_info[cluster_label]['avg_rank'] += cluster_avg_rank[cluster_label]\n",
    "        clusters_info[cluster_label]['count'] += 1\n",
    "\n",
    "# Add each cluster's information to the map\n",
    "for cluster_label, info in clusters_info.items():\n",
    "    # Calculate centroid (middle point) of the cluster\n",
    "    centroid_latitude = info['latitude_sum'] / info['count']\n",
    "    centroid_longitude = info['longitude_sum'] / info['count']\n",
    "    \n",
    "    color = rank_to_color(info['avg_rank'] / info['count'])  # Calculate average rank\n",
    "    folium.CircleMarker(\n",
    "        location=[centroid_latitude, centroid_longitude],\n",
    "        radius=10,  # Adjust the radius based on the count\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_color=color,\n",
    "        fill_opacity=0.7,\n",
    "        popup=f\"Avg Rank: {info['avg_rank'] / info['count']:.2f}, Cluster KMeans: {cluster_label}\"\n",
    "    ).add_to(paris_map)\n",
    "    \n",
    "# Plotting the cluster centers on the map of Paris\n",
    "paris_map\n",
    "\n",
    "#not yet to the point, so not accurate yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85079434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import FloatImage\n",
    "import pandas as pd\n",
    "from branca.colormap import LinearColormap\n",
    "\n",
    "# Assuming dataframe1 is loaded with appropriate 'longitude' and 'latitude' columns and 'price' column\n",
    "sub_6 = dataframe1[dataframe1.price < 600]\n",
    "\n",
    "# Create a map centered around an average location\n",
    "paris_map = folium.Map(location=[sub_6['latitude'].mean(), sub_6['longitude'].mean()], zoom_start=13)\n",
    "\n",
    "# Define color scale\n",
    "color_scale = LinearColormap(['green', 'yellow', 'orange', 'pink', 'purple', 'brown', 'red', 'blue'],\n",
    "                             vmin=sub_6['price'].min(), vmax=sub_6['price'].max(),\n",
    "                             caption='Price Scale')  # This caption will be displayed on the legend\n",
    "\n",
    "# Add points to the map\n",
    "for idx, row in sub_6.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=3,\n",
    "        color=color_scale(row['price']),\n",
    "        fill=True,\n",
    "        fill_color=color_scale(row['price']),\n",
    "        fill_opacity=0.7\n",
    "    ).add_to(paris_map)\n",
    "\n",
    "# Add the color scale to the map\n",
    "paris_map.add_child(color_scale)\n",
    "\n",
    "# Save or display the map\n",
    "#paris_map.save('paris_price_map.html')\n",
    "\n",
    "# If you want to display the image in a Jupyter notebook:\n",
    "paris_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3588bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the average scores and price for each neighborhood\n",
    "neighbourhood_reviews = dataframe1.groupby('neighbourhood').agg({\n",
    "    'review_scores_rating': 'mean',\n",
    "    'price': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Normalize scores and prices for comparison (optional)\n",
    "max_rating = neighbourhood_reviews['review_scores_rating'].max()\n",
    "neighbourhood_reviews['review_scores_rating'] = neighbourhood_reviews['review_scores_rating'] / max_rating\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=neighbourhood_reviews, x='price', y='review_scores_rating')\n",
    "plt.title('Average Review Score vs. Price by Neighbourhood')\n",
    "plt.xlabel('Normalized Price')\n",
    "plt.ylabel('Normalized Review Score')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f4ffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plot of availability_365 vs price without neighborhood distinction\n",
    "#plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Using a single color for all points\n",
    "#plt.scatter(dataframe1['availability_365'], dataframe1['price'], alpha=0.5)\n",
    "\n",
    "#plt.xlabel('Availability over 365 days')\n",
    "#plt.ylabel('Price')\n",
    "#plt.title('Availability vs Price')\n",
    "#plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Using a higher gridsize for more detail\n",
    "# Applying a logarithmic scale with 'norm' and setting a minimum count threshold for display with 'mincnt'\n",
    "plt.hexbin(dataframe1['availability_365'], dataframe1['price'], gridsize=100, cmap='Blues', bins='log', mincnt=1)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.xlabel('Availability over 365 days')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Availability vs Price - Hexbin Density with Log Scale')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f7b6f5",
   "metadata": {},
   "source": [
    "# Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95352200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is loaded into 'data'\n",
    "# Replace 'target_column' with your actual target variable\n",
    "target = 'price'\n",
    "\n",
    "# Correlation for numerical features\n",
    "correlation_matrix = data_copy.corr()\n",
    "correlation_with_target = correlation_matrix[target].sort_values(ascending=False)\n",
    "\n",
    "# Display the correlations\n",
    "print(correlation_with_target)\n",
    "\n",
    "# Visualize the correlations with a heatmap\n",
    "plt.figure(figsize=(100, 40))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1849af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "# Assuming data_copy_clean is your DataFrame\n",
    "# Ensure that only numeric data is used for VIF calculation\n",
    "numeric_data = data_copy.select_dtypes(include=[np.number])\n",
    "\n",
    "# Adding a constant column for intercept\n",
    "numeric_data_with_constant = add_constant(numeric_data)\n",
    "\n",
    "# Sampling 40% of the data for performance reasons\n",
    "sampled_data = numeric_data_with_constant.sample(frac=0.4, random_state=42)\n",
    "\n",
    "# Calculating VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = sampled_data.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(sampled_data.values, i) for i in range(sampled_data.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd316513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and display features with VIF greater than 5\n",
    "high_vif_features = vif_data[vif_data[\"VIF\"] > 5]\n",
    "print(\"Features with VIF greater than 5:\\n\", high_vif_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df28cfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "data = data_copy.copy()\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = data.corr()\n",
    "\n",
    "# Identify features correlated with the target variable\n",
    "target_variable = 'price'  # Replace with your actual target variable name\n",
    "correlation_with_target = corr_matrix[target_variable].abs().sort_values(ascending=False)\n",
    "\n",
    "# Selecting the top 10 features most correlated to the target, excluding the target itself\n",
    "top_10_correlated_features = correlation_with_target[1:11]  # Skip the first one as it's the target itself\n",
    "\n",
    "# Displaying the top 10 correlated features\n",
    "print(\"Top 10 features most correlated with the target variable:\")\n",
    "print(top_10_correlated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c5ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "data = data_copy.copy()\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = data.corr()\n",
    "\n",
    "# Identify features correlated with the target variable\n",
    "target_variable = 'price'  # Replace with your actual target variable name\n",
    "correlation_with_target = corr_matrix[target_variable].abs().sort_values(ascending=True)\n",
    "\n",
    "# Selecting the 10 features least correlated to the target, excluding the target itself\n",
    "bottom_10_correlated_features = correlation_with_target[1:11]  # Skip the first one as it's the target itself\n",
    "\n",
    "# Displaying the 10 least correlated features\n",
    "print(\"10 features least correlated with the target variable:\")\n",
    "print(bottom_10_correlated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a threshold for identifying high correlations (e.g., 0.8 or 0.9)\n",
    "high_corr_threshold = 0.8\n",
    "\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = data_copy.corr()\n",
    "\n",
    "# Find pairs of highly correlated features\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > high_corr_threshold:\n",
    "            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "# Print out the highly correlated pairs\n",
    "for pair in high_corr_pairs:\n",
    "    print(f\"Feature pair: {pair[0]}, {pair[1]} - Correlation: {pair[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0125b559",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to many missing values in these columns, thus removing them\n",
    "columns_to_drop = ['review_scores_accuracy',\"review_scores_cleanliness\",\"review_scores_checkin\",\"review_scores_communication\"\n",
    "                  ,\"review_scores_location\",\"review_scores_value\",\"Dishes and silverware\",\"host_listings_count\",\"calculated_host_listings_count_entire_homes\"\n",
    "                  ,\"availability_30\",\"availability_60\",\"number_of_reviews_ltm\"]\n",
    "\n",
    "data_copy = data_copy.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e8308",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy.to_excel('FINAL_MERGED_DATASET_WITH_PREPRF.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c2a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf62954",
   "metadata": {},
   "source": [
    "# Data merging of the 2 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9881e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have loaded your calendar dataset into a DataFrame named 'calendar_data'\n",
    "# Let's assume your 'calendar_data' has columns: 'listing_id', 'date', 'price', 'availability'\n",
    "\n",
    "\n",
    "# Extract season from date\n",
    "def get_season(month):\n",
    "    if month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Autumn'\n",
    "    else:\n",
    "        return 'Winter'\n",
    "\n",
    "data['season'] = data['date'].dt.month.map(get_season)\n",
    "\n",
    "# Group by listing_id and season, and randomly select one row from each group\n",
    "simplified_calendar_data = data.groupby(['listing_id', 'season']).apply(lambda x: x.sample(1)).reset_index(drop=True)\n",
    "\n",
    "# Now 'simplified_calendar_data' contains only one random day per season for each listing_id\n",
    "# Now you can use 'simplified_calendar_data' for further analysis\n",
    "\n",
    "#lastly we also drop the minimum_nights and maximum nights\n",
    "columns_to_drop = [\"minimum_nights\" , \"maximum_nights\", \"available\",\"adjusted_price\"]\n",
    "simplified_calendar_data = simplified_calendar_data.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c99632",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_calendar_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89fce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming so that both datasets have the same column name\n",
    "simplified_calendar_data.rename(columns={'listing_id': 'id'}, inplace=True)\n",
    "\n",
    "#dropping 'adjusted_price' and \"available\" because these are not relevant anymore\n",
    "#columns_to_drop = [\"minimum_nights\" , \"maximum_nights\", \"available\",\"adjusted_price\"]\n",
    "#data = data.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c265902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import great_circle\n",
    "\n",
    "# Define the monuments and their coordinates\n",
    "monuments = {\n",
    "    \"Eiffel Tower\": (48.8584, 2.2945),\n",
    "    \"Louvre Museum\": (48.8606, 2.3376),\n",
    "    \"Notre-Dame Cathedral\": (48.8529, 2.3500),\n",
    "    \"Sacr-Cur Basilica\": (48.8867, 2.3431),\n",
    "    \"Arc de Triomphe\": (48.8738, 2.2950)\n",
    "}\n",
    "\n",
    "# Calculate distances and add as new columns\n",
    "for monument, coords in monuments.items():\n",
    "    dataframe1[monument + ' Distance'] = dataframe1.apply(lambda row: great_circle((row['latitude'], row['longitude']), coords).kilometers,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1615291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/france_holidays.csv'\n",
    "holidays_data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert the 'date' column in both datasets to datetime objects\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "holidays_data['date'] = pd.to_datetime(holidays_data['date'])\n",
    "\n",
    "# Merge the Airbnb data with the public holidays data based on the 'date' column\n",
    "data['is_holiday'] = data['date'].isin(holidays_data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467faa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/paris_school_holidays.csv'\n",
    "school_holidays_data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert the 'date' column in both datasets to datetime objects\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "school_holidays_data['date'] = pd.to_datetime(school_holidays_data['date'])\n",
    "\n",
    "# Merge the Airbnb data with the public holidays data based on the 'date' column\n",
    "data['is_school_holiday'] = data['date'].isin(school_holidays_data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopandas\n",
    "!pip install rtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20410cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Assuming 'data' is your original DataFrame with latitude and longitude\n",
    "# First, ensure 'data' is loaded correctly, e.g., data = pd.read_excel('path/to/your/data.xlsx')\n",
    "\n",
    "# Convert DataFrame to GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(dataframe1, geometry=gpd.points_from_xy(dataframe1.longitude, dataframe1.latitude))\n",
    "\n",
    "# Create a spatial index for the GeoDataFrame\n",
    "sindex = gdf.sindex\n",
    "\n",
    "def count_nearby_listings(row, gdf, radius_meters=500):\n",
    "    # Convert the radius in meters to degrees (approximation)\n",
    "    # Note: This approximation works well for small distances (e.g., within a few kilometers)\n",
    "    radius_degrees = radius_meters / 111320  # Rough conversion factor for degrees to meters\n",
    "    \n",
    "    # Find possible matches with rtree index, which returns indices of possible matches\n",
    "    possible_matches_index = list(sindex.intersection(row.geometry.buffer(radius_degrees).bounds))\n",
    "    \n",
    "    # Filter actual matches from possible matches\n",
    "    possible_matches = gdf.iloc[possible_matches_index]\n",
    "    actual_matches = possible_matches[possible_matches.geometry.distance(row.geometry) <= radius_degrees]\n",
    "    \n",
    "    # Exclude the row itself from the count\n",
    "    nearby_count = actual_matches.shape[0] - 1\n",
    "    return nearby_count\n",
    "\n",
    "# Apply the function to count nearby listings for each listing\n",
    "gdf['nearby_airbnbs_count'] = gdf.apply(count_nearby_listings, axis=1, gdf=gdf)\n",
    "\n",
    "# If you want to work with a regular DataFrame (excluding the geometry column)\n",
    "dataframe_clean = pd.DataFrame(gdf.drop(columns='geometry'))\n",
    "\n",
    "# Display the first few rows of the updated DataFrame to verify the new column\n",
    "#print(dataframe_clean.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277e5f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading the cleaned_dataset\n",
    "updated_data.to_excel('ALMOST MERGED DATA.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_calendar_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85074945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have two DataFrames: simplified_calendar_data and listings_data\n",
    "# Let's assume 'dataframe1' has columns: 'listing_id', and other relevant features\n",
    "\n",
    "# Merge the two datasets on 'id'\n",
    "merged_data = pd.merge(dataframe_clean, simplified_calendar_data, on='id', how='inner')\n",
    "\n",
    "# Now 'merged_data' contains the combined information from both datasets\n",
    "# You can proceed with your analysis using this merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b701c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for identifiers in dataframe1 that do not appear in simplified_calendar_data\n",
    "identifiers_only_in_dataframe1 = set(dataframe1['id']) - set(simplified_calendar_data['id'])\n",
    "\n",
    "# Check for identifiers in simplified_calendar_data that do not appear in dataframe1\n",
    "identifiers_only_in_simplified_calendar_data = set(simplified_calendar_data['id']) - set(dataframe1['id'])\n",
    "\n",
    "# Print the results\n",
    "print(\"Identifiers only in dataframe1:\", identifiers_only_in_dataframe1)\n",
    "print(\"Identifiers only in simplified_calendar_data:\", identifiers_only_in_simplified_calendar_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf4db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dfdf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data.drop('price_x', axis=1)\n",
    "merged_data.rename(columns={'price_y': 'price'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1be975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading the cleaned_dataset\n",
    "merged_data.to_excel('FINAL_MERGED_DATASET.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e983dd2",
   "metadata": {},
   "source": [
    "# RF MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b76a0",
   "metadata": {},
   "source": [
    "## Feature Selection/Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47661981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the dataset to work with\n",
    "data_copy = merged_data.copy()\n",
    "#looking into which columns currently still have empty values\n",
    "columns_with_nan = data_copy.columns[data_copy.isna().any()].tolist()\n",
    "# Print the columns\n",
    "print(\"Columns with NaN values:\", columns_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0996c017",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"host_id\", \"days_since_host_host\",\"host_since\"]\n",
    "data_copy = data_copy.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361907d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display columns with datetime type\n",
    "datetime_columns = data_copy.select_dtypes(include=['datetime64[ns]']).columns\n",
    "print(\"Datetime Columns:\", datetime_columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc0f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to many missing values in these columns, thus removing them\n",
    "columns_to_drop = ['first_review', 'last_review','days_since_first_review', 'days_since_first_first_review', 'days_since_last_review', \n",
    "                   'days_since_last_last_review']\n",
    "\n",
    "#filling in 0 in the fields where nothing is filled in\n",
    "columns_to_fill = ['review_scores_rating', 'review_scores_accuracy', \n",
    "                   'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', \n",
    "                   'review_scores_location', 'review_scores_value', 'reviews_per_month']\n",
    "\n",
    "# Fill NaN values with 0 in the specified columns\n",
    "for column in columns_to_fill:\n",
    "    data_copy[column] = data_copy[column].fillna(0)\n",
    "\n",
    "data_copy = data_copy.drop(columns_to_drop, axis=1)\n",
    "\n",
    "#looking into which columns currently still have empty values\n",
    "columns_with_nan = data_copy.columns[data_copy.isna().any()].tolist()\n",
    "# Print the columns\n",
    "#print(\"Columns with NaN values:\", columns_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed12f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking which columns are strings\n",
    "string_columns = [col for col in data_copy_clean.columns if data_copy_clean[col].dtype == 'object' or isinstance(data_copy_clean[col].dtype, pd.StringDtype)]\n",
    "print(string_columns)\n",
    "\n",
    "#looking which columns are categories\n",
    "category_columns = [col for col in data_copy_clean.columns if data_copy_clean[col].dtype == 'category' or isinstance(data_copy_clean[col].dtype, pd.CategoricalDtype)]\n",
    "print(category_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0ed5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy_clean[\"days_since_host\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72419fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Splitting the columns up in one-hot and label encoding\n",
    "categorical_columns_one_hot = ['neighbourhood', 'property_type', 'room_type','season']  # For one-hot encoding\n",
    "categorical_columns_label = ['host_response_time', 'host_response_rate']  # For label encoding\n",
    "\n",
    "# One-Hot Encoding\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(data_copy[categorical_columns_one_hot])\n",
    "\n",
    "# Manually create feature names for the one-hot encoded columns\n",
    "one_hot_feature_names = []\n",
    "for i, column in enumerate(categorical_columns_one_hot):\n",
    "    categories = one_hot_encoder.categories_[i]\n",
    "    one_hot_feature_names.extend([f\"{column}_{category}\" for category in categories])\n",
    "\n",
    "one_hot_encoded_df = pd.DataFrame(one_hot_encoded, columns=one_hot_feature_names, index=data_copy.index)\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_columns_label:\n",
    "    data_copy[col] = label_encoder.fit_transform(data_copy[col])\n",
    "\n",
    "# Concatenate the one-hot encoded columns back to the original dataframe\n",
    "data_copy = pd.concat([data_copy, one_hot_encoded_df], axis=1)\n",
    "\n",
    "# Drop the original string columns\n",
    "data_copy.drop(categorical_columns_one_hot, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6273db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8a6849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'date' is the name of your date column\n",
    "data_copy['year'] = pd.to_datetime(data_copy['date']).dt.year\n",
    "data_copy['month'] = pd.to_datetime(data_copy['date']).dt.month\n",
    "data_copy['day'] = pd.to_datetime(data_copy['date']).dt.day\n",
    "data_copy['day_of_year'] = pd.to_datetime(data_copy['date']).dt.dayofyear\n",
    "\n",
    "#drop the date column then\n",
    "data_copy = data_copy.drop('date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns with exclusively 0s and 1s to boolean\n",
    "for col in data_copy.columns:\n",
    "    unique_values = data_copy[col].unique()\n",
    "    if set(unique_values).issubset({0, 1}):\n",
    "        data_copy[col] = data_copy[col].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe99e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy.to_excel('merged_data_RF.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a07869",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36690b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d423926f",
   "metadata": {},
   "source": [
    "## 1. RF with k-fold cross validation and out-of-time validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe602da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Get unique ids\n",
    "all_ids = data_copy['id'].unique()\n",
    "\n",
    "# Split ids into training and testing sets\n",
    "train_ids, test_ids = train_test_split(all_ids, test_size=0.3, random_state=42)\n",
    "\n",
    "# Filter data based on selected ids\n",
    "train_selected = data_copy[data_copy['id'].isin(train_ids)]\n",
    "test_selected = data_copy[data_copy['id'].isin(test_ids)]\n",
    "\n",
    "# Define the time cutoff date\n",
    "cutoff_date = pd.to_datetime('2023-11-24')  # Example: YYYY-MM-DD\n",
    "\n",
    "# Convert 'day', 'month', 'year' to a unified date column in 'train_selected' and 'test_selected'\n",
    "train_selected['date'] = pd.to_datetime(train_selected[['year', 'month', 'day']])\n",
    "test_selected['date'] = pd.to_datetime(test_selected[['year', 'month', 'day']])\n",
    "\n",
    "# Now split the dataset based on the cutoff date\n",
    "train_data = train_selected[train_selected['date'] < cutoff_date]\n",
    "test_data = test_selected[test_selected['date'] >= cutoff_date]\n",
    "\n",
    "# Separate features and target variable for training and testing sets\n",
    "X_train = train_data.drop([\"price\", \"id\",\"date\"], axis=1)\n",
    "y_train = train_data['price']\n",
    "X_test = test_data.drop([\"price\", \"id\",\"date\"], axis=1)\n",
    "y_test = test_data['price']\n",
    "\n",
    "# Initialize and train the model\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the out-of-time testing data\n",
    "mse_score = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse_score)\n",
    "\n",
    "# R-squared (R2) value calculated\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared (R2) Value:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214e0037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "X = data_copy.drop([\"price\"], axis=1)\n",
    "y = data_copy['price']\n",
    "\n",
    "original_features = data_copy.drop([\"price\"], axis=1).columns\n",
    "\n",
    "# Calculate feature importances\n",
    "importances = mutual_info_classif(X, y)\n",
    "feat_importances = pd.Series(importances, X.columns)\n",
    "\n",
    "# Set a threshold for feature importance (this is arbitrary and can be adjusted)\n",
    "threshold = 0.01  # Example threshold\n",
    "\n",
    "# Select features above the importance threshold\n",
    "selected_features = feat_importances[feat_importances > threshold].index\n",
    "\n",
    "# Rebuild the feature set with only selected features\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "# Number of features dropped\n",
    "features_dropped = set(original_features) - set(selected_features)\n",
    "num_features_dropped = len(features_dropped)\n",
    "\n",
    "# Number of features remaining\n",
    "num_features_remaining = len(selected_features)\n",
    "\n",
    "# Print information about dropped and remaining features\n",
    "print(\"Features Dropped:\", features_dropped)\n",
    "print(\"Number of Features Dropped:\", num_features_dropped)\n",
    "print(\"Number of Features Remaining:\", num_features_remaining)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "#looking at MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculate R-squared\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Calculate Adjusted R-squared\n",
    "n = X_test.shape[0]  # Number of observations\n",
    "p = X_test.shape[1]  # Number of predictor variables\n",
    "adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R-squared:\", adjusted_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fb6ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "X = data_copy.drop([\"price\"], axis=1)\n",
    "y = data_copy['price']\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "#looking at MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error:, {mse:.4f}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "\n",
    "# Calculate Adjusted R-squared\n",
    "n = X_test.shape[0]  # Number of observations\n",
    "p = X_test.shape[1]  # Number of predictor variables\n",
    "adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "print(f\"Adjusted R-squared: {adjusted_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6facc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef240d9f",
   "metadata": {},
   "source": [
    "## 2. XGB k-fold cross validation and out-of-time validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn xgboost matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data_copy.drop(['price'], axis=1)\n",
    "y = data_copy['price']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Instantiate XGBoost Regressor\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#MSE value calculated \n",
    "mse_score = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse_score)\n",
    "\n",
    "#r2_score calculated\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared (R2) Value:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af1c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Get unique ids\n",
    "all_ids = data_copy['id'].unique()\n",
    "\n",
    "# Split ids into training and testing sets\n",
    "train_ids, test_ids = train_test_split(all_ids, test_size=0.3, random_state=42)\n",
    "\n",
    "# Filter data based on selected ids\n",
    "train_selected = data_copy[data_copy['id'].isin(train_ids)]\n",
    "test_selected = data_copy[data_copy['id'].isin(test_ids)]\n",
    "\n",
    "# Define the time cutoff date\n",
    "cutoff_date = pd.to_datetime('2023-11-24')  # Example: YYYY-MM-DD\n",
    "\n",
    "# Convert 'day', 'month', 'year' to a unified date column in 'train_selected' and 'test_selected'\n",
    "train_selected['date'] = pd.to_datetime(train_selected[['year', 'month', 'day']])\n",
    "test_selected['date'] = pd.to_datetime(test_selected[['year', 'month', 'day']])\n",
    "\n",
    "# Now split the dataset based on the cutoff date\n",
    "train_data = train_selected[train_selected['date'] < cutoff_date]\n",
    "test_data = test_selected[test_selected['date'] >= cutoff_date]\n",
    "\n",
    "# Separate features and target variable for training and testing sets\n",
    "X_train = train_data.drop([\"price\", \"id\",\"date\"], axis=1)\n",
    "y_train = train_data['price']\n",
    "X_test = test_data.drop([\"price\", \"id\",\"date\"], axis=1)\n",
    "y_test = test_data['price']\n",
    "\n",
    "# Initialize and train the model\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the out-of-time testing data\n",
    "mse_score = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse_score)\n",
    "\n",
    "# R-squared (R2) value calculated\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared (R2) Value:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e28986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "X = data_copy.drop([\"price\"], axis=1)\n",
    "y = data_copy['price']\n",
    "\n",
    "original_features = data_copy.drop([\"price\"], axis=1).columns\n",
    "\n",
    "# Calculate feature importances\n",
    "importances = mutual_info_classif(X, y)\n",
    "feat_importances = pd.Series(importances, X.columns)\n",
    "\n",
    "# Set a threshold for feature importance (this is arbitrary and can be adjusted)\n",
    "threshold = 0.01  # Example threshold\n",
    "\n",
    "# Select features above the importance threshold\n",
    "selected_features = feat_importances[feat_importances > threshold].index\n",
    "\n",
    "# Rebuild the feature set with only selected features\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "# Number of features dropped\n",
    "features_dropped = set(original_features) - set(selected_features)\n",
    "num_features_dropped = len(features_dropped)\n",
    "\n",
    "# Number of features remaining\n",
    "num_features_remaining = len(selected_features)\n",
    "\n",
    "# Print information about dropped and remaining features\n",
    "print(\"Features Dropped:\", features_dropped)\n",
    "print(\"Number of Features Dropped:\", num_features_dropped)\n",
    "print(\"Number of Features Remaining:\", num_features_remaining)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = xgb.XGBRegressor()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "#looking at MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculate R-squared\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Calculate Adjusted R-squared\n",
    "n = X_test.shape[0]  # Number of observations\n",
    "p = X_test.shape[1]  # Number of predictor variables\n",
    "adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R-squared:\", adjusted_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc5a526",
   "metadata": {},
   "source": [
    "## 3. Linear model with k fold cross-validation and out-of-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b02f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Get unique ids\n",
    "all_ids = data_copy['id'].unique()\n",
    "\n",
    "# Split ids into training and testing sets\n",
    "train_ids, test_ids = train_test_split(all_ids, test_size=0.3, random_state=42)\n",
    "\n",
    "# Filter data based on selected ids\n",
    "train_selected = data_copy[data_copy['id'].isin(train_ids)]\n",
    "test_selected = data_copy[data_copy['id'].isin(test_ids)]\n",
    "\n",
    "# Define the time cutoff date\n",
    "cutoff_date = pd.to_datetime('2023-11-24')  # Example: YYYY-MM-DD\n",
    "\n",
    "# Convert 'day', 'month', 'year' to a unified date column in 'train_selected' and 'test_selected'\n",
    "train_selected['date'] = pd.to_datetime(train_selected[['year', 'month', 'day']])\n",
    "test_selected['date'] = pd.to_datetime(test_selected[['year', 'month', 'day']])\n",
    "\n",
    "# Now split the dataset based on the cutoff date\n",
    "train_data = train_selected[train_selected['date'] < cutoff_date]\n",
    "test_data = test_selected[test_selected['date'] >= cutoff_date]\n",
    "\n",
    "# Separate features and target variable for training and testing sets\n",
    "X_train = train_data.drop([\"price\", \"id\",\"date\"], axis=1)\n",
    "y_train = train_data['price']\n",
    "X_test = test_data.drop([\"price\", \"id\",\"date\"], axis=1)\n",
    "y_test = test_data['price']\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the out-of-time testing data\n",
    "mse_score = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse_score)\n",
    "\n",
    "# R-squared (R2) value calculated\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared (R2) Value:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a99a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb913be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas scikit-learn xgboost matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data_copy.drop(['price'], axis=1)\n",
    "y = data_copy['price']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Instantiate XGBoost Regressor\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#MSE value calculated \n",
    "mse_score = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse_score)\n",
    "\n",
    "#r2_score calculated\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared (R2) Value:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae886fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "X = data_copy.drop([\"price\"], axis=1)\n",
    "y = data_copy['price']\n",
    "\n",
    "original_features = data_copy.drop([\"price\"], axis=1).columns\n",
    "\n",
    "# Calculate feature importances\n",
    "importances = mutual_info_classif(X, y)\n",
    "feat_importances = pd.Series(importances, X.columns)\n",
    "\n",
    "# Set a threshold for feature importance (this is arbitrary and can be adjusted)\n",
    "threshold = 0.01  # Example threshold\n",
    "\n",
    "# Select features above the importance threshold\n",
    "selected_features = feat_importances[feat_importances > threshold].index\n",
    "\n",
    "# Rebuild the feature set with only selected features\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "# Number of features dropped\n",
    "features_dropped = set(original_features) - set(selected_features)\n",
    "num_features_dropped = len(features_dropped)\n",
    "\n",
    "# Number of features remaining\n",
    "num_features_remaining = len(selected_features)\n",
    "\n",
    "# Print information about dropped and remaining features\n",
    "print(\"Features Dropped:\", features_dropped)\n",
    "print(\"Number of Features Dropped:\", num_features_dropped)\n",
    "print(\"Number of Features Remaining:\", num_features_remaining)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = LinearRegression()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "#looking at MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculate R-squared\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Calculate Adjusted R-squared\n",
    "n = X_test.shape[0]  # Number of observations\n",
    "p = X_test.shape[1]  # Number of predictor variables\n",
    "adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R-squared:\", adjusted_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63faadec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
