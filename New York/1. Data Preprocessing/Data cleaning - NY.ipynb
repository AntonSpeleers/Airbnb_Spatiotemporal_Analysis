{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de5aab57",
   "metadata": {},
   "source": [
    "# Cleaning Listing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f48e774",
   "metadata": {},
   "source": [
    "#### Reading the listing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20407bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all packages needed \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "from datetime import datetime \n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Data/2. airbnb_data/New York/NY all detailed listings/NY M3 detailed listings.xlsx'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "dataframe1= pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e628679",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c79ca",
   "metadata": {},
   "source": [
    "#### Cleaning process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42fa1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decided which columns will not be relevant for the first initial analysis\n",
    "columns_to_drop =[\"name\",\"description\",\"host_location\",\"host_thumbnail_url\",\"host_name\",\"bathrooms\",\"first_review\",\"last_review\"\n",
    "                  ,\"listing_url\",\"scrape_id\",\"last_scraped\", \"host_picture_url\",\"host_url\", \"host_has_profile_pic\", \n",
    "                  \"host_verifications\",\"source\",\"calendar_last_scraped\",\"license\",\"picture_url\",\"host_about\",\n",
    "                             \"neighbourhood\",\"neighbourhood_group_cleansed\",\"minimum_minimum_nights\",\n",
    "                             \"maximum_minimum_nights\",\"minimum_maximum_nights\",\"maximum_maximum_nights\",\n",
    "                             \"minimum_nights_avg_ntm\",\"maximum_nights_avg_ntm\",\"calendar_updated\",\n",
    "                             \"neighborhood_overview\",\"host_neighbourhood\", \"host_acceptance_rate\", \"host_since\",\n",
    "                  \"minimum_nights\",\"maximum_nights\",\"has_availability\"]\n",
    "dataframe1 = dataframe1.drop(columns_to_drop, axis=1)\n",
    "\n",
    "#solving trailing white space problem\n",
    "string_columns = dataframe1.select_dtypes(include='object').columns.tolist()\n",
    "for i in string_columns:\n",
    "    dataframe1[i] = dataframe1[i].str.strip()\n",
    "\n",
    "#missing values have always been checked during this cleaning with the function: print(dataframe1.isnull().mean()) \n",
    "\n",
    "#host_response_time, filling in the empty ones with \"unknown\"\n",
    "dataframe1.host_response_time.fillna(\"unknown\", inplace=True)\n",
    "dataframe1.host_response_time.value_counts(normalize=True)\n",
    "\n",
    "#adapting the host_response_rate column to better fit\n",
    "# Removing the % sign from the host_response_rate string and converting to an integer\n",
    "dataframe1.host_response_rate = dataframe1.host_response_rate.str[:-1].astype('float64')\n",
    "# Bin into four categories\n",
    "dataframe1.host_response_rate = pd.cut(dataframe1.host_response_rate, bins=[0, 50, 90, 99, 100], labels=['0-49%', '50-89%', '90-99%', '100%'], include_lowest=True)\n",
    "# Converting to string\n",
    "dataframe1.host_response_rate = dataframe1.host_response_rate.astype('str')\n",
    "# Replace nulls with 'unknown'\n",
    "dataframe1.host_response_rate.replace('nan', 'unknown', inplace=True)\n",
    "\n",
    "#these rows do not have that big of a missing value amount so we just delete the missing value rows here\n",
    "col = [\"host_identity_verified\",\"host_listings_count\",\"host_total_listings_count\",\"host_is_superhost\"]\n",
    "for column in col:\n",
    "    dataframe1.dropna(subset=[column], inplace=True)\n",
    "\n",
    "# Category counts\n",
    "dataframe1.host_response_rate.value_counts()\n",
    "#fill out all NaN within string columns with ''\n",
    "for column in string_columns:\n",
    "    dataframe1[column] = dataframe1[column].fillna(\"\")   \n",
    "\n",
    "#fixing the \"bathroom_text\" column to only numbers and renaming it\n",
    "def extract_numeric(value):\n",
    "    numeric_part = re.search(r'\\d+\\.\\d+|\\d+', str(value))\n",
    "    return float(numeric_part.group()) if numeric_part else None\n",
    "dataframe1['bathrooms_text'] = dataframe1['bathrooms_text'].apply(extract_numeric)\n",
    "dataframe1.rename(columns={'bathrooms_text': 'bathrooms'}, inplace=True)\n",
    "\n",
    "#dropping those that still have no bathroom amount \n",
    "dataframe1.dropna(subset=[\"bathrooms\"], inplace=True)\n",
    "    \n",
    "#delete those that have no information about both beds and bedrooms\n",
    "dataframe1.dropna(subset=['beds', 'bedrooms'], how='all', inplace=True)\n",
    "\n",
    "#dropping those with +4 bedrooms (outliers) and no information about bedroom and between 1-4 beds is a studio so 0 bedrooms\n",
    "dataframe1.loc[(dataframe1['bedrooms'].isnull()) & (dataframe1['beds'].between(1, 4)), 'bedrooms'] = 0\n",
    "dataframe1.drop(dataframe1[(dataframe1['bedrooms'].isnull()) & (dataframe1['beds'] > 4)].index, inplace=True)\n",
    "\n",
    "#fill in all the other empty values with the amount of bedrooms \n",
    "dataframe1['beds'].fillna(dataframe1['bedrooms'], inplace=True)\n",
    "\n",
    "#dropping those that still have no bedroom amount \n",
    "dataframe1.dropna(subset=[\"bedrooms\"], inplace=True)\n",
    "\n",
    "#Simplifying the property_types in to 4 categories\n",
    "dataframe1.property_type.replace({\n",
    "    'Barn': 'House',\n",
    "    'Boat': 'Other',\n",
    "    'Bus': 'Other',\n",
    "    'Camper/RV': 'Other',\n",
    "    'Casa particular': 'House',\n",
    "    'Cave': 'Other',\n",
    "    'Dome': 'Other',\n",
    "    'Earthen home': 'House',\n",
    "    'Entire bed and breakfast': 'Hotel',    \n",
    "    'Entire bungalow': 'House',\n",
    "    'Entire condo': 'Apartmen',\n",
    "    'Entire guesthouse': 'House',\n",
    "    'Entire home': 'House',\n",
    "    'Entire guest suite': 'Apartment',\n",
    "    'Entire rental unit': 'Apartment',\n",
    "    'Entire loft': 'Apartment',\n",
    "    'Entire home/apt': 'House',\n",
    "    'Entire place': 'House',\n",
    "    'Entire serviced apartment': 'Apartment',\n",
    "    'Entire townhouse': 'House',\n",
    "    'Entire villa': 'House',\n",
    "    'Entire vacation home': 'House',\n",
    "    'Floor': 'Other',\n",
    "    'Houseboat': 'Other',\n",
    "    'Private room': 'Apartment',\n",
    "    'Island': 'Other',\n",
    "    'Private room in bed and breakfast': 'Hotel',    \n",
    "    'Private room in boat': 'Other',\n",
    "    'Private room in casa particular': 'House',\n",
    "    'Private room in condo': 'Apartment',\n",
    "    'Private room in guest suite': 'Apartment',\n",
    "    'Private room in earthen home': 'House',\n",
    "    'Private room in home': 'House',\n",
    "    'Private room in guesthouse': 'House',\n",
    "    'Private room in loft': 'Apartment',\n",
    "    'Private room in hostel': 'Hotel',\n",
    "    'Private room in rental unit': 'Apartment',\n",
    "    'Private room in townhouse': 'House',\n",
    "    'Private room in tiny home': 'House',\n",
    "    'Private room in serviced apartment': 'Apartment',\n",
    "    'Room in bed and breakfast': 'Hotel',\n",
    "    'Private room in villa': 'House',\n",
    "    'Room in serviced apartment': 'Apartment',\n",
    "    'Room in boutique hotel': 'Hotel',\n",
    "    'Room in hotel': 'Hotel',\n",
    "    'Room in hostel': 'Hotel',\n",
    "    'Shared room in bed and breakfast': 'Hotel',\n",
    "    'Shared room in boutique hotel': 'Hotel',\n",
    "    'Shared room in cabin': 'Other',\n",
    "    'Shared room in boat': 'Other',\n",
    "    'Shared room in condo': 'Apartment',\n",
    "    'Shared room in farm stay': 'Other',\n",
    "    'Shared room in guesthouse': 'House',\n",
    "    'Shared room in casa particular': 'House',\n",
    "    'Shared room in ice dome': 'Other',\n",
    "    'Shared room in home': 'House',\n",
    "    'Shared room in hostel': 'Hotel',\n",
    "    'Shared room in hotel': 'Hotel',\n",
    "    'Shared room in rental unit': 'Apartment',\n",
    "    'Tiny home': 'House',\n",
    "    'Shared room in loft': 'Apartment',\n",
    "    'Shared room in townhouse': 'House',\n",
    "    'Shared room in tiny home': 'House',\n",
    "    }, inplace=True)\n",
    "\n",
    "# Replacing other categories with 'other'\n",
    "dataframe1.loc[~dataframe1.property_type.isin(['House', 'Apartment','Hotel']), 'property_type'] = 'Other'\n",
    "#dataframe1['property_type'].value_counts()\n",
    "\n",
    "#changing the name from neighbourhoud_cleansed to neighbourhood\n",
    "dataframe1.rename(columns={'neighbourhood_cleansed': 'neighbourhood'}, inplace=True)\n",
    "\n",
    "#convert the necessary columns to a boolean type, which is easier to use\n",
    "columns_to_convert = ['host_is_superhost', 'instant_bookable', 'host_identity_verified'] \n",
    "for column in columns_to_convert:\n",
    "    dataframe1[column] = dataframe1[column].replace({'f': False, 't': True}).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986c8034",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe1.info()\n",
    "print(dataframe1.isnull().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3994231",
   "metadata": {},
   "source": [
    "#### Seperating amenities column into seperate boolean columns + total_amenities column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64001d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the string representation of lists to actual lists\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a unique list out of all the different kind of amenities that there are\n",
    "unique_items_set = set.union(*dataframe1[\"amenities\"].apply(set))\n",
    "print(unique_items_set)\n",
    "print(len(unique_items_set)) \n",
    "#the one that is shown here is taken after the adjustments you can see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cb3808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with the code down below we will simplify the amenities to reduce the amount of amenities in dataframe1\n",
    "\n",
    "oven_items_set = {item for item in unique_items_set if 'oven' in item.lower()}\n",
    "def replace_oven_items(item_list):\n",
    "    return ['oven' if item in oven_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_oven_items)\n",
    "\n",
    "soap_items_set = {item for item in unique_items_set if 'soap' in item.lower()}\n",
    "def replace_soap_items(item_list):\n",
    "    return ['soap' if item in soap_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_soap_items)\n",
    "\n",
    "shampoo_items_set = {item for item in unique_items_set if 'shampoo' in item.lower()}\n",
    "def replace_shampoo_items(item_list):\n",
    "    return ['shampoo' if item in shampoo_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_shampoo_items)\n",
    "\n",
    "wifi_items_set = {item for item in unique_items_set if 'wifi' in item.lower()}\n",
    "def replace_wifi_items(item_list):\n",
    "    return ['wifi' if item in wifi_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_wifi_items)\n",
    "\n",
    "special_items_set = {item for item in unique_items_set if any(keyword in item for keyword in ['Netflix', 'Disney+', 'Amazon Prime'])}\n",
    "def replace_and_add_broadcast(item_list):\n",
    "    # Replace items from special_items_set with 'Broadcast'\n",
    "    item_list = ['broadcast' if item in special_items_set else item for item in item_list]\n",
    "    # Add 'TV' to the list if modified\n",
    "    modified = any(item == 'broadcast' for item in item_list)\n",
    "    if modified:\n",
    "        item_list.append('TV')\n",
    "    return item_list\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_and_add_broadcast)\n",
    "\n",
    "tv_items_set = {item for item in unique_items_set if 'tv' in item.lower()}\n",
    "def replace_tv_items(item_list):\n",
    "    return ['tv' if item in tv_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_tv_items)\n",
    "\n",
    "ref_items_set = {item for item in unique_items_set if 'refrigerator' in item.lower()}\n",
    "def replace_ref_items(item_list):\n",
    "    return ['refrigerator' if item in ref_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_ref_items)\n",
    "\n",
    "coffee_items_set = {item for item in unique_items_set if 'coffee' in item.lower()}\n",
    "def replace_coffee_items(item_list):\n",
    "    return ['coffee' if item in coffee_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_coffee_items)\n",
    "\n",
    "sound_items_set = {item for item in unique_items_set if any(keyword in item for keyword in ['sound system', 'Bluetooth'])}\n",
    "def replace_sound_items(item_list):\n",
    "    item_list = ['sound system' if item in sound_items_set else item for item in item_list]\n",
    "    return item_list\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_sound_items)\n",
    "\n",
    "stove_items_set = {item for item in unique_items_set if 'stove' in item.lower()}\n",
    "def replace_stove_items(item_list):\n",
    "    return ['stove' if item in stove_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_stove_items)\n",
    "\n",
    "cond_items_set = {item for item in unique_items_set if 'conditioner' in item.lower()}\n",
    "def replace_cond_items(item_list):\n",
    "    return ['conditioner' if item in cond_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_cond_items)\n",
    "\n",
    "park_items_set = {item for item in unique_items_set if 'parking' in item.lower()}\n",
    "def replace_park_items(item_list):\n",
    "    return ['parking' if item in park_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_park_items)\n",
    "\n",
    "clothing_items_set = {item for item in unique_items_set if 'clothing storage' in item.lower()}\n",
    "def replace_clothing_items(item_list):\n",
    "    return ['clothing storage' if item in clothing_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_clothing_items)\n",
    "\n",
    "exercise_items_set = {item for item in unique_items_set if 'exercise equipment' in item.lower()}\n",
    "def replace_exercise_items(item_list):\n",
    "    return ['exercise equipment' if item in exercise_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_exercise_items)\n",
    "\n",
    "children_items_set = {item for item in unique_items_set if 'children' in item.lower()}\n",
    "def replace_children_items(item_list):\n",
    "    return ['toys children' if item in children_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_children_items)\n",
    "\n",
    "bbq_items_set = {item for item in unique_items_set if 'bbq' in item.lower()}\n",
    "def replace_bbq_items(item_list):\n",
    "    return ['bbq' if item in bbq_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_bbq_items)\n",
    "\n",
    "pool_items_set = {item for item in unique_items_set if 'pool' in item.lower()}\n",
    "def replace_pool_items(item_list):\n",
    "    return ['pool' if item in pool_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_pool_items)\n",
    "\n",
    "hot_items_set = {item for item in unique_items_set if 'hot tub' in item.lower()}\n",
    "def replace_hot_items(item_list):\n",
    "    return ['hot tub' if item in hot_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_hot_items)\n",
    "\n",
    "backyard_items_set = {item for item in unique_items_set if 'backyard' in item.lower()}\n",
    "def replace_backyard_items(item_list):\n",
    "    return ['backyard' if item in backyard_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_backyard_items)\n",
    "\n",
    "gym_items_set = {item for item in unique_items_set if 'gym' in item.lower()}\n",
    "def replace_gym_items(item_list):\n",
    "    return ['gym' if item in gym_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_gym_items)\n",
    "\n",
    "view_items_set = {item for item in unique_items_set if 'view' in item.lower()}\n",
    "def replace_view_items(item_list):\n",
    "    return ['view' if item in view_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_view_items)\n",
    "\n",
    "crib_items_set = {item for item in unique_items_set if 'crib' in item.lower()}\n",
    "def replace_crib_items(item_list):\n",
    "    return ['crib' if item in crib_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_crib_items)\n",
    "\n",
    "gc_items_set = {item for item in unique_items_set if 'game console' in item.lower()}\n",
    "def replace_gc_items(item_list):\n",
    "    return ['game console' if item in gc_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_gc_items)\n",
    "\n",
    "sauna_items_set = {item for item in unique_items_set if 'sauna' in item.lower()}\n",
    "def replace_sauna_items(item_list):\n",
    "    return ['sauna' if item in sauna_items_set else item for item in item_list]\n",
    "dataframe1[\"amenities\"] = dataframe1[\"amenities\"].apply(replace_sauna_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18876906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#last step to clean the amenities is removing the ones that do not appear 5000 times or more in the column\n",
    "flat_list = [word for sublist in dataframe1[\"amenities\"] for word in sublist]\n",
    "word_counts = Counter(flat_list)\n",
    "filtered_word_set = {word for word, count in word_counts.items() if count < 5000}\n",
    "common_elements = list(filtered_word_set & unique_items_set)\n",
    "\n",
    "def remove_common_elements(item_list):\n",
    "    return [item for item in item_list if item not in common_elements]\n",
    "dataframe1['amenities'] = dataframe1['amenities'].apply(remove_common_elements)\n",
    "\n",
    "unique_items_set = set.union(*dataframe1[\"amenities\"].apply(set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5918ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_items_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a687ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making new columns for the amenities indivually with a 0 or 1 inside of them \n",
    "for item in unique_items_set:\n",
    "    dataframe1[item] = dataframe1[\"amenities\"].apply(lambda x: int(item in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5834894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will be able to remove the 'amenities' column\n",
    "dataframe1 = dataframe1.drop('amenities', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c973e209",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe1.columns[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b09f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of amenities for each listing and make this a new column\n",
    "dataframe1['total_amenities'] = dataframe1.iloc[:, 38:-1].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e228a",
   "metadata": {},
   "source": [
    "#### Handling review columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426a2061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to many missing values in these columns, thus removing them\n",
    "review_columns = [\"review_scores_rating\",'review_scores_accuracy',\"review_scores_cleanliness\",\n",
    "                  \"review_scores_checkin\",\"review_scores_communication\",\"review_scores_location\",\n",
    "                  \"review_scores_value\",\"reviews_per_month\"]                                    \n",
    "\n",
    "# Impute missing values with 0\n",
    "dataframe1[review_columns] = dataframe1[review_columns].fillna(0)\n",
    "\n",
    "# Create the 'Listing_reviewed' column\n",
    "dataframe1['listing_reviewed'] = dataframe1[review_columns].sum(axis=1).apply(lambda x: 0 if x == 0 else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195fef27",
   "metadata": {},
   "source": [
    "#### Applying encoding techniques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df9184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Splitting the columns up in one-hot and label encoding\n",
    "categorical_columns_one_hot = ['neighbourhood', 'property_type', 'room_type']  # For one-hot encoding\n",
    "categorical_columns_label = ['host_response_time', 'host_response_rate']  # For label encoding\n",
    "\n",
    "# One-Hot Encoding\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(dataframe1[categorical_columns_one_hot])\n",
    "\n",
    "# Manually create feature names for the one-hot encoded columns\n",
    "one_hot_feature_names = []\n",
    "for i, column in enumerate(categorical_columns_one_hot):\n",
    "    categories = one_hot_encoder.categories_[i]\n",
    "    one_hot_feature_names.extend([f\"{column}_{category}\" for category in categories])\n",
    "\n",
    "one_hot_encoded_df = pd.DataFrame(one_hot_encoded, columns=one_hot_feature_names, index=dataframe1.index)\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_columns_label:\n",
    "    dataframe1[col] = label_encoder.fit_transform(dataframe1[col])\n",
    "\n",
    "# Concatenate the one-hot encoded columns back to the original dataframe\n",
    "dataframe1 = pd.concat([dataframe1, one_hot_encoded_df], axis=1)\n",
    "\n",
    "# Drop the original string columns\n",
    "dataframe1.drop(categorical_columns_one_hot, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63bd537",
   "metadata": {},
   "source": [
    "#### Making columns booleans if needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f3fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns with exclusively 0s and 1s to boolean\n",
    "for col in dataframe1.columns:\n",
    "    unique_values = dataframe1[col].unique()\n",
    "    if set(unique_values).issubset({0, 1}):\n",
    "        dataframe1[col] = dataframe1[col].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ca232f",
   "metadata": {},
   "source": [
    "#### Simplifying the neighborhood columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1cac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns that start with \"neighbourhood\"\n",
    "neighbourhood_columns = [col for col in dataframe1.columns if col.startswith('neighbourhood')]\n",
    "\n",
    "# Count the number of these columns\n",
    "num_neighbourhood_columns = len(neighbourhood_columns)\n",
    "print(f\"Number of columns that start with 'neighbourhood': {num_neighbourhood_columns}\")\n",
    "\n",
    "# Calculate the sum of True values for each neighbourhood column\n",
    "true_value_counts = dataframe1[neighbourhood_columns].sum().sort_values(ascending=False)\n",
    "\n",
    "# Display the columns with the most True values (top 20)\n",
    "print(\"Top 20 columns with the most True values:\")\n",
    "print(true_value_counts.head(20))\n",
    "\n",
    "# Display the columns with the least True values (bottom 20)\n",
    "print(\"\\nBottom 20 columns with the least True values:\")\n",
    "print(true_value_counts.tail(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe5f1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns that start with \"neighbourhood\"\n",
    "neighbourhood_columns = [col for col in dataframe1.columns if col.startswith('neighbourhood')]\n",
    "\n",
    "# Count the number of these columns\n",
    "num_neighbourhood_columns = len(neighbourhood_columns)\n",
    "print(f\"Number of columns that start with 'neighbourhood': {num_neighbourhood_columns}\")\n",
    "\n",
    "# Calculate the sum of True values for each neighbourhood column\n",
    "true_value_counts = dataframe1[neighbourhood_columns].sum().sort_values(ascending=False)\n",
    "\n",
    "# Display the neighbourhood columns ranked between 40 and 150\n",
    "print(\"Neighbourhood columns ranked top 30:\")\n",
    "print(true_value_counts.iloc[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b84c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns that start with \"neighbourhood\"\n",
    "neighbourhood_columns = [col for col in dataframe1.columns if col.startswith('neighbourhood')]\n",
    "\n",
    "# Count the number of these columns\n",
    "num_neighbourhood_columns = len(neighbourhood_columns)\n",
    "print(f\"Number of columns that start with 'neighbourhood': {num_neighbourhood_columns}\")\n",
    "\n",
    "# Calculate the sum of True values for each neighbourhood column\n",
    "true_value_counts = dataframe1[neighbourhood_columns].sum().sort_values(ascending=False)\n",
    "\n",
    "# Calculate the total sum of True values for all neighbourhood columns\n",
    "total_true_values = true_value_counts.sum()\n",
    "\n",
    "# Calculate the sum of True values for the top 30 neighbourhood columns\n",
    "top_30_true_values = true_value_counts.head(30).sum()\n",
    "\n",
    "# Calculate the percentage of the top 30 relative to the total\n",
    "percentage_top_30 = (top_30_true_values / total_true_values) * 100\n",
    "\n",
    "# Display the result\n",
    "print(f\"The top 30 neighbourhood columns represent {percentage_top_30:.2f}% of the total True values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns that start with \"neighbourhood\"\n",
    "neighbourhood_columns = [col for col in dataframe1.columns if col.startswith('neighbourhood')]\n",
    "\n",
    "# Count the number of these columns\n",
    "num_neighbourhood_columns = len(neighbourhood_columns)\n",
    "print(f\"Number of columns that start with 'neighbourhood': {num_neighbourhood_columns}\")\n",
    "\n",
    "# Calculate the sum of True values for each neighbourhood column\n",
    "true_value_counts = dataframe1[neighbourhood_columns].sum().sort_values(ascending=False)\n",
    "\n",
    "# Get the top 30 neighbourhood columns\n",
    "top_30_neighbourhoods = true_value_counts.head(30).index.tolist()\n",
    "\n",
    "# Combine the remaining neighbourhood columns into one column\n",
    "neighbourhood_others = dataframe1[neighbourhood_columns].drop(columns=top_30_neighbourhoods).sum(axis=1)\n",
    "\n",
    "# Create a new column for neighbourhood_others\n",
    "dataframe1['neighbourhood_others'] = neighbourhood_others\n",
    "\n",
    "# Drop the original neighbourhood columns that are not in the top 30\n",
    "dataframe1.drop(columns=[col for col in neighbourhood_columns if col not in top_30_neighbourhoods], inplace=True)\n",
    "\n",
    "# Display the result\n",
    "print(dataframe1.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd7b5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns that start with \"neighbourhood\"\n",
    "neighbourhood_columns = [col for col in dataframe1.columns if col.startswith('neighbourhood')]\n",
    "\n",
    "# Count the number of these columns\n",
    "num_neighbourhood_columns = len(neighbourhood_columns)\n",
    "print(f\"Number of columns that start with 'neighbourhood': {num_neighbourhood_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1244ee72",
   "metadata": {},
   "source": [
    "#### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb4067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading the cleaned_dataset\n",
    "dataframe1.to_csv('cleaned_dataset_ny.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de56dc3",
   "metadata": {},
   "source": [
    "# Cleaning Calendar Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb7b7aa",
   "metadata": {},
   "source": [
    "#### Reading the calendar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bad765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#reading in the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Data/2. airbnb_data/New York/3. March airbnb_data NY/calendar/calendar.csv'\n",
    "data3 = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d982c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3751ddfb",
   "metadata": {},
   "source": [
    "#### Cleaning process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b455609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing the price column with removing the dollar sign and making it numerical\n",
    "data3['price'] = data3['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "#dropping adjusted_price, maximum_nights, minimum_nights since it brings no additional value\n",
    "data3 = data3.drop('adjusted_price', axis=1)\n",
    "data3 = data3.drop('maximum_nights', axis=1)\n",
    "data3 = data3.drop('minimum_nights', axis=1)\n",
    "#making sure it is datetime\n",
    "data3['date']=pd.to_datetime(data3['date'])\n",
    "#renaming listing_id to id\n",
    "data3.rename(columns={'listing_id': 'id'}, inplace=True)\n",
    "#including the season\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Extract season from date\n",
    "def get_season(month):\n",
    "    if month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Autumn'\n",
    "    else:\n",
    "        return 'Winter'\n",
    "data3['season'] = data3['date'].dt.month.map(get_season)\n",
    "# Convert 'available' column to boolean\n",
    "data3['available'] = data3['available'].map({'f': False, 't': True})\n",
    "data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9f05eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print start and end date scraped Q3\n",
    "start_date3 = data3['date'].min()\n",
    "end_date3 = data3['date'].max()\n",
    "\n",
    "print(\"Start Date:\", start_date3)\n",
    "print(\"End Date:\", end_date3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fabbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking missing values\n",
    "print(data3.isnull().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d647a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the missing values from the dataset after seeing that there are not that many missing values\n",
    "data3.dropna(inplace=True)\n",
    "print(data3.isnull().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf8be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by both month and year\n",
    "mean_price = data3.groupby([data3['date'].dt.year, data3['date'].dt.month])['price'].mean()\n",
    "\n",
    "# Print the average prices\n",
    "print(mean_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3ba305",
   "metadata": {},
   "source": [
    "#### Applying encoding techniques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14414de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Splitting the columns up in one-hot and label encoding\n",
    "categorical_columns_one_hot = ['season']  # For one-hot encoding\n",
    "\n",
    "# One-Hot Encoding\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(data3[categorical_columns_one_hot])\n",
    "\n",
    "# Manually create feature names for the one-hot encoded columns\n",
    "one_hot_feature_names = []\n",
    "for i, column in enumerate(categorical_columns_one_hot):\n",
    "    categories = one_hot_encoder.categories_[i]\n",
    "    one_hot_feature_names.extend([f\"{column}_{category}\" for category in categories])\n",
    "\n",
    "one_hot_encoded_df = pd.DataFrame(one_hot_encoded, columns=one_hot_feature_names, index=data3.index)\n",
    "\n",
    "# Concatenate the one-hot encoded columns back to the original dataframe\n",
    "data3 = pd.concat([data3, one_hot_encoded_df], axis=1)\n",
    "\n",
    "# Drop the original string columns\n",
    "data3.drop(categorical_columns_one_hot, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7cea5a",
   "metadata": {},
   "source": [
    "#### Making columns booleans if needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114c5e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns with exclusively 0s and 1s to boolean\n",
    "for col in data3.columns:\n",
    "    unique_values = data3[col].unique()\n",
    "    if set(unique_values).issubset({0, 1}):\n",
    "        data3[col] = data3[col].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f60276",
   "metadata": {},
   "source": [
    "#### Save the cleaned datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45eb839",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.to_csv('cleaned_calendar_Q3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f055a3",
   "metadata": {},
   "source": [
    "# Merging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c45a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/NY Airbnb_Spatiotemporal_Analysis/Data Cleaning Listings/cleaned_calendar_Q3.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "data3= pd.read_csv(file_path)\n",
    "\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/NY Airbnb_Spatiotemporal_Analysis/Data Cleaning Listings/cleaned_dataset_ny.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "df_listing_cleaned_ny= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff6c345",
   "metadata": {},
   "source": [
    "#### Making sure listings have enough dates with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0590888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure the 'date' column is in datetime format\n",
    "data3['date'] = pd.to_datetime(data3['date'])\n",
    "\n",
    "# Sort the dataset by date\n",
    "data3_sorted = data3.sort_values(by='date')\n",
    "\n",
    "# Calculate the number of rows per day\n",
    "rows_per_day = data3_sorted['date'].value_counts().sort_index(ascending=False)\n",
    "\n",
    "# Select the most recent days with more than 35,000 values\n",
    "selected_dates = rows_per_day[rows_per_day > 35000].index[:150]\n",
    "\n",
    "# Filter the dataset to only include the selected dates\n",
    "df_merged_calendar_ny = data3_sorted[data3_sorted['date'].isin(selected_dates)]\n",
    "\n",
    "# Verify the number of unique dates\n",
    "unique_dates_count = df_merged_calendar_ny['date'].nunique()\n",
    "print(f\"Number of unique dates in the resulting dataframe: {unique_dates_count}\")\n",
    "\n",
    "# Display the resulting dataframe\n",
    "print(df_merged_calendar_ny)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d092331a",
   "metadata": {},
   "source": [
    "#### Merging Calendar data with Listing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae02312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'id' is the common key in both datasets\n",
    "\n",
    "# Perform an inner merge to keep only the IDs present in both datasets\n",
    "final_merged_data = pd.merge(df_merged_calendar_ny, df_listing_cleaned_ny, on='id', how='inner', suffixes=('_merged', '_additional'))\n",
    "\n",
    "# Count number of rows and unique IDs after merging\n",
    "total_rows_after = final_merged_data.shape[0]\n",
    "unique_ids_after = final_merged_data['id'].nunique()\n",
    "\n",
    "print(f\"Total number of rows after merging: {total_rows_after}\")\n",
    "print(f\"Total number of unique IDs after merging: {unique_ids_after}\")\n",
    "\n",
    "# Now final_merged_data contains merged information with IDs present in both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7618c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc67e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if no columns are duplicated\n",
    "merged_columns = final_merged_data.columns\n",
    "\n",
    "print(\"Columns with suffixes:\")\n",
    "for col in merged_columns:\n",
    "    if col.endswith('_merged') or col.endswith('_additional'):\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a44f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in 'price_additional' column\n",
    "unique_prices = final_merged_data['price_merged'].nunique()\n",
    "print(\"Unique values in 'price_merged' column:\", unique_prices)\n",
    "\n",
    "# Check unique values in 'price_additional' column\n",
    "unique_prices = final_merged_data['price_additional'].nunique()\n",
    "print(\"Unique values in 'price_additional' column:\", unique_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fdec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the price column from the listing data\n",
    "final_merged_data = final_merged_data.drop('price_additional', axis=1)\n",
    "# Rename 'price_merged' to 'price'\n",
    "final_merged_data.rename(columns={'price_merged': 'price'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa36ba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with object type\n",
    "object_columns = final_merged_data.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(\"Columns with object type:\")\n",
    "print(object_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2128c9e9",
   "metadata": {},
   "source": [
    "# Adding additional features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd7d258",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/NY Airbnb_Spatiotemporal_Analysis/ny_holidays.xlsx'\n",
    "holidays_data = pd.read_excel(file_path)\n",
    "\n",
    "# Convert the 'date' column in both datasets to datetime objects\n",
    "df_merged_calendar_ny['date'] = pd.to_datetime(df_merged_calendar_ny['date'])\n",
    "holidays_data['Date'] = pd.to_datetime(holidays_data['Date'])\n",
    "\n",
    "# Merge the Airbnb data with the public holidays data based on the 'date' column\n",
    "df_merged_calendar_ny['is_holiday'] = df_merged_calendar_ny['date'].isin(holidays_data['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c518dfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/NY Airbnb_Spatiotemporal_Analysis/ny_school_holidays.xlsx'\n",
    "school_holidays_data = pd.read_excel(file_path)\n",
    "\n",
    "# Convert the 'date' column in both datasets to datetime objects\n",
    "df_merged_calendar_ny['date'] = pd.to_datetime(df_merged_calendar_ny['date'])\n",
    "school_holidays_data['Date'] = pd.to_datetime(school_holidays_data['Date'])\n",
    "\n",
    "# Merge the Airbnb data with the public holidays data based on the 'date' column\n",
    "df_merged_calendar_ny['is_school_holiday'] = df_merged_calendar_ny['date'].isin(school_holidays_data['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08ff1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopandas\n",
    "!pip install rtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6198e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "# Convert DataFrame to GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df_listing_cleaned_ny, \n",
    "    geometry=gpd.points_from_xy(df_listing_cleaned_ny.longitude, df_listing_cleaned_ny.latitude)\n",
    ")\n",
    "\n",
    "# Create a spatial index for the GeoDataFrame\n",
    "sindex = gdf.sindex\n",
    "\n",
    "def count_nearby_listings(row, gdf, sindex, radius_meters=500):\n",
    "    # Convert the radius in meters to degrees (approximation)\n",
    "    radius_degrees = radius_meters / 111320\n",
    "    \n",
    "    # Find possible matches with rtree index, which returns indices of possible matches\n",
    "    possible_matches_index = list(sindex.intersection(row.geometry.buffer(radius_degrees).bounds))\n",
    "    \n",
    "    # Filter actual matches from possible matches\n",
    "    possible_matches = gdf.iloc[possible_matches_index]\n",
    "    actual_matches = possible_matches[possible_matches.geometry.distance(row.geometry) <= radius_degrees]\n",
    "    \n",
    "    # Exclude the row itself from the count\n",
    "    nearby_count = actual_matches.shape[0] - 1\n",
    "    return nearby_count\n",
    "\n",
    "# Apply the function to count nearby listings for each listing\n",
    "gdf['nearby_airbnbs_count'] = gdf.apply(count_nearby_listings, axis=1, gdf=gdf, sindex=sindex)\n",
    "\n",
    "# If you want to work with a regular DataFrame (excluding the geometry column)\n",
    "df_listing_cleaned_ny = pd.DataFrame(gdf.drop(columns='geometry'))\n",
    "\n",
    "# Display the first few rows of the updated DataFrame to verify the new column\n",
    "print(df_listing_cleaned_ny.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc390f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "luxury_amenities = ['sauna', 'hot tub', 'gym', 'pool', 'Bathtub','exercise equipment', 'game console']\n",
    "df_listing_cleaned_ny['luxury_amenities_score'] = df_listing_cleaned_ny[luxury_amenities].sum(axis=1)\n",
    "\n",
    "df_listing_cleaned_ny['luxury_amenities_score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770ad00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Function to count nearby POIs using spatial indexing\n",
    "def count_nearby_pois_spatial_index(row, pois_gdf, max_distance=500):\n",
    "    try:\n",
    "        # Build spatial index for POIs GeoDataFrame\n",
    "        pois_sindex = pois_gdf.sindex\n",
    "        \n",
    "        # Get nearby POIs using spatial index\n",
    "        nearby_pois_idx = list(pois_sindex.intersection(row['geometry'].buffer(max_distance).bounds))\n",
    "        nearby_pois = pois_gdf.iloc[nearby_pois_idx]\n",
    "        \n",
    "        # Filter nearby POIs within max_distance\n",
    "        nearby_pois = nearby_pois[nearby_pois.geometry.distance(row['geometry']) <= max_distance]\n",
    "        \n",
    "        return len(nearby_pois)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {row.name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Convert your full dataset to a GeoDataFrame and set CRS\n",
    "gdf_airbnb = gpd.GeoDataFrame(df_listing_cleaned_ny, geometry=gpd.points_from_xy(df_listing_cleaned_ny.longitude, df_listing_cleaned_ny.latitude))\n",
    "gdf_airbnb.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "# Fetch POIs data from OpenStreetMap for your area\n",
    "area = \"New York, USA\"  # Replace with your area of interest\n",
    "tags = {\"amenity\": [\"restaurant\", \"bar\"]}\n",
    "pois_gdf = ox.geometries_from_place(area, tags=tags)\n",
    "\n",
    "# Find UTM zone for the area and project both GeoDataFrames to this CRS\n",
    "utm_crs = \"EPSG:32618\"  # UTM zone for NY\n",
    "gdf_airbnb = gdf_airbnb.to_crs(utm_crs)\n",
    "pois_gdf = pois_gdf.to_crs(utm_crs)\n",
    "\n",
    "# Measure the processing time for the full dataset\n",
    "start_time = time.time()\n",
    "\n",
    "# Apply the function to each Airbnb listing in the full dataset and track progress\n",
    "tqdm.pandas(desc=\"Processing rows\")\n",
    "gdf_airbnb['nearby_restaurants_bars'] = gdf_airbnb.progress_apply(\n",
    "    lambda row: count_nearby_pois_spatial_index(row, pois_gdf, max_distance=1000), axis=1)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for processing the full dataset: {elapsed_time / 60} minutes\")\n",
    "\n",
    "# Convert back to a regular DataFrame if necessary\n",
    "df_listing_cleaned_ny = pd.DataFrame(gdf_airbnb.drop(columns='geometry'))\n",
    "\n",
    "# Look at the statistics of the new column\n",
    "print(df_listing_cleaned_ny['nearby_restaurants_bars'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d9c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Function to count nearby transport POIs using spatial indexing\n",
    "def count_nearby_transport_spatial_index(row, transport_gdf, transport_sindex, max_distance=500):\n",
    "    try:\n",
    "        # Get nearby transport POIs using spatial index\n",
    "        nearby_transport_idx = list(transport_sindex.intersection(row['geometry'].buffer(max_distance).bounds))\n",
    "        nearby_transport = transport_gdf.iloc[nearby_transport_idx]\n",
    "        \n",
    "        # Filter nearby transport POIs within max_distance\n",
    "        nearby_transport = nearby_transport[nearby_transport.geometry.distance(row['geometry']) <= max_distance]\n",
    "        \n",
    "        return len(nearby_transport)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {row.name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Convert your full dataset to a GeoDataFrame and set CRS\n",
    "gdf_airbnb = gpd.GeoDataFrame(df_listing_cleaned_ny, geometry=gpd.points_from_xy(df_listing_cleaned_ny.longitude, df_listing_cleaned_ny.latitude))\n",
    "gdf_airbnb.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "# Fetch transport POIs data from OpenStreetMap for your area\n",
    "area = \"New York, USA\"  # Replace with your area of interest\n",
    "tags = {'amenity': ['bus_station', 'bus_stop'], \n",
    "        'public_transport': ['station', 'stop_position']}\n",
    "transport_gdf = ox.geometries_from_place(area, tags=tags)\n",
    "\n",
    "# Find UTM zone for the area and project both GeoDataFrames to this CRS\n",
    "utm_crs = \"EPSG:32618\"  # UTM zone for NY\n",
    "gdf_airbnb = gdf_airbnb.to_crs(utm_crs)\n",
    "transport_gdf = transport_gdf.to_crs(utm_crs)\n",
    "\n",
    "# Build spatial index for transport GeoDataFrame once\n",
    "transport_sindex = transport_gdf.sindex\n",
    "\n",
    "# Measure the processing time for the full dataset\n",
    "start_time = time.time()\n",
    "\n",
    "# Apply the function to each Airbnb listing in the full dataset and track progress\n",
    "tqdm.pandas(desc=\"Processing rows\")\n",
    "gdf_airbnb['nearby_transport'] = gdf_airbnb.progress_apply(\n",
    "    lambda row: count_nearby_transport_spatial_index(row, transport_gdf, transport_sindex, max_distance=500), axis=1)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for processing the full dataset: {elapsed_time / 60} minutes\")\n",
    "\n",
    "# Convert back to a regular DataFrame if necessary\n",
    "df_listing_cleaned_ny = pd.DataFrame(gdf_airbnb.drop(columns='geometry'))\n",
    "\n",
    "# Look at the statistics of the new column\n",
    "print(df_listing_cleaned_ny['nearby_transport'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacbaa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if the required columns are present in the DataFrame\n",
    "required_columns = ['latitude', 'longitude', 'price', 'date']\n",
    "missing_columns = [col for col in required_columns if col not in final_merged_data.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    raise KeyError(f\"The following required columns are missing from the DataFrame: {missing_columns}\")\n",
    "\n",
    "# Initialize a list to store mean neighbor prices for each row\n",
    "mean_neighbor_prices = []\n",
    "\n",
    "# Group by date\n",
    "grouped = final_merged_data.groupby('date')\n",
    "\n",
    "# Iterate over each group (i.e., each date)\n",
    "for date, group in tqdm(grouped, desc=\"Processing dates\"):\n",
    "    if len(group) < 2:\n",
    "        # If there's less than 2 listings for a date, use the original price for that row\n",
    "        mean_neighbor_prices.extend(group['price'])\n",
    "        continue\n",
    "    \n",
    "    # Prepare data for KDTree\n",
    "    coordinates = group[['latitude', 'longitude']].values\n",
    "    prices = group['price'].values\n",
    "    \n",
    "    # Build the KDTree for fast lookup of nearest neighbors\n",
    "    tree = cKDTree(coordinates)\n",
    "    \n",
    "    # Define the number of neighbors to consider (5 closest neighbors)\n",
    "    k = 5\n",
    "    \n",
    "    # Query the tree for the k nearest neighbors for each listing\n",
    "    distances, indices = tree.query(coordinates, k=k+1)  # k+1 because the nearest neighbor includes the point itself\n",
    "    \n",
    "    # Calculate mean price of the nearest neighbors, excluding the point itself\n",
    "    for idx in indices:\n",
    "        mean_price = prices[idx[1:]].mean()  # Exclude the point itself\n",
    "        mean_neighbor_prices.append(mean_price)\n",
    "\n",
    "# Add the mean_neighbor_prices as a new column in the DataFrame\n",
    "final_merged_data['mean_price_neighbors'] = np.round(mean_neighbor_prices).astype(int)\n",
    "\n",
    "# Display the result\n",
    "print(final_merged_data.head())\n",
    "print(final_merged_data[\"mean_price_neighbors\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075518c",
   "metadata": {},
   "source": [
    "#### Saving dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f0048",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_data.to_csv('MERGED_CLEANED_DATASET.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1ba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7864959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the name and type of every column\n",
    "for col in final_merged_data.columns:\n",
    "    print(f\"Column: {col}, Type: {final_merged_data[col].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0afaaa",
   "metadata": {},
   "source": [
    "# Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd4f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/NY Airbnb_Spatiotemporal_Analysis/Data Cleaning Listings/MERGED_CLEANED_DATASET.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "final_merged_data= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a01716c",
   "metadata": {},
   "source": [
    "#### Splitting the data in a 60/20/20 split across IDs and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e267762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming final_merged_data is already loaded\n",
    "\n",
    "# Step 1: Convert 'date' column to datetime if it's not already\n",
    "final_merged_data['date'] = pd.to_datetime(final_merged_data['date'])\n",
    "\n",
    "# Step 2: Split the time frame of the dataset into 60/20/20 without overlap\n",
    "sorted_data = final_merged_data.sort_values(by='date')\n",
    "total_size = len(sorted_data)\n",
    "train_end_idx = int(0.6 * total_size)\n",
    "val_end_idx = int(0.8 * total_size)\n",
    "\n",
    "train_timeframe = sorted_data.iloc[:train_end_idx]\n",
    "val_timeframe = sorted_data.iloc[train_end_idx:val_end_idx]\n",
    "test_timeframe = sorted_data.iloc[val_end_idx:]\n",
    "\n",
    "# To ensure no overlap, remove the first day of val_timeframe and test_timeframe\n",
    "val_timeframe = val_timeframe[val_timeframe['date'] > train_timeframe['date'].max()]\n",
    "test_timeframe = test_timeframe[test_timeframe['date'] > val_timeframe['date'].max()]\n",
    "\n",
    "# Step 3: Extract unique IDs and split them into 60/20/20 randomly\n",
    "unique_ids = final_merged_data['id'].unique()\n",
    "np.random.shuffle(unique_ids)\n",
    "\n",
    "train_size = int(0.6 * len(unique_ids))\n",
    "val_size = int(0.2 * len(unique_ids))\n",
    "test_size = len(unique_ids) - train_size - val_size\n",
    "\n",
    "train_ids = set(unique_ids[:train_size])\n",
    "val_ids = set(unique_ids[train_size:train_size + val_size])\n",
    "test_ids = set(unique_ids[train_size + val_size:])\n",
    "\n",
    "# Step 4: Create the final training, validation, and test sets\n",
    "final_train_data = train_timeframe[train_timeframe['id'].isin(train_ids)]\n",
    "final_val_data = val_timeframe[val_timeframe['id'].isin(val_ids)]\n",
    "final_test_data = test_timeframe[test_timeframe['id'].isin(test_ids)]\n",
    "\n",
    "# Verification\n",
    "def verify_splits(train, val, test):\n",
    "    # Check unique IDs in each split\n",
    "    train_ids = set(train['id'])\n",
    "    val_ids = set(val['id'])\n",
    "    test_ids = set(test['id'])\n",
    "    \n",
    "    print(f\"Number of unique IDs in train: {len(train_ids)}\")\n",
    "    print(f\"Number of unique IDs in val: {len(val_ids)}\")\n",
    "    print(f\"Number of unique IDs in test: {len(test_ids)}\")\n",
    "    \n",
    "    assert train_ids.isdisjoint(val_ids), \"Train and validation sets are not disjoint.\"\n",
    "    assert train_ids.isdisjoint(test_ids), \"Train and test sets are not disjoint.\"\n",
    "    assert val_ids.isdisjoint(test_ids), \"Validation and test sets are not disjoint.\"\n",
    "    \n",
    "    # Check date ranges\n",
    "    print(f\"Train date range: {train['date'].min()} to {train['date'].max()}\")\n",
    "    print(f\"Validation date range: {val['date'].min()} to {val['date'].max()}\")\n",
    "    print(f\"Test date range: {test['date'].min()} to {test['date'].max()}\")\n",
    "\n",
    "verify_splits(final_train_data, final_val_data, final_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e9c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_data.to_csv('EDA_TRAIN_NY.csv', index=False)\n",
    "final_val_data.to_csv('EDA_VALID_NY.csv', index=False)\n",
    "final_test_data.to_csv('EDA_TEST_NY.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de97ec4d",
   "metadata": {},
   "source": [
    "#### Loading in the sets seperatly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eadd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/NY Airbnb_Spatiotemporal_Analysis/Data Cleaning Listings/EDA_TRAIN_NY.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "train_data= pd.read_csv(file_path)\n",
    "\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/NY Airbnb_Spatiotemporal_Analysis/Data Cleaning Listings/EDA_VALID_NY.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "valid_data= pd.read_csv(file_path)\n",
    "\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/NY Airbnb_Spatiotemporal_Analysis/Data Cleaning Listings/EDA_TEST_NY.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "test_data= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d893c648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data, valid_data, and test_data are the original datasets\n",
    "\n",
    "# Function to count unique IDs and check overlaps\n",
    "def analyze_ids(train_data, valid_data, test_data):\n",
    "    # Count unique IDs in each dataset\n",
    "    unique_train_ids = set(train_data['id'])\n",
    "    unique_valid_ids = set(valid_data['id'])\n",
    "    unique_test_ids = set(test_data['id'])\n",
    "    \n",
    "    print(\"Unique ID Counts:\")\n",
    "    print(f\"Train IDs: {len(unique_train_ids)}\")\n",
    "    print(f\"Validation IDs: {len(unique_valid_ids)}\")\n",
    "    print(f\"Test IDs: {len(unique_test_ids)}\")\n",
    "    print()\n",
    "    \n",
    "    # Check for overlaps\n",
    "    overlap_train_valid = unique_train_ids.intersection(unique_valid_ids)\n",
    "    overlap_train_test = unique_train_ids.intersection(unique_test_ids)\n",
    "    overlap_valid_test = unique_valid_ids.intersection(unique_test_ids)\n",
    "    \n",
    "    print(\"Overlaps:\")\n",
    "    print(f\"Train and Validation overlap: {len(overlap_train_valid)} IDs\")\n",
    "    print(f\"Train and Test overlap: {len(overlap_train_test)} IDs\")\n",
    "    print(f\"Validation and Test overlap: {len(overlap_valid_test)} IDs\")\n",
    "    \n",
    "    # Optionally, print some of the overlapping IDs\n",
    "    if len(overlap_train_valid) > 0:\n",
    "        print(\"\\nSample overlapping IDs between Train and Validation:\")\n",
    "        print(list(overlap_train_valid)[:10])  # Print a sample of 10 overlapping IDs\n",
    "        \n",
    "    if len(overlap_train_test) > 0:\n",
    "        print(\"\\nSample overlapping IDs between Train and Test:\")\n",
    "        print(list(overlap_train_test)[:10])  # Print a sample of 10 overlapping IDs\n",
    "        \n",
    "    if len(overlap_valid_test) > 0:\n",
    "        print(\"\\nSample overlapping IDs between Validation and Test:\")\n",
    "        print(list(overlap_valid_test)[:10])  # Print a sample of 10 overlapping IDs\n",
    "\n",
    "# Analyze the IDs in the original datasets\n",
    "analyze_ids(train_data, valid_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97a40d",
   "metadata": {},
   "source": [
    "# EDA on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01237dc8",
   "metadata": {},
   "source": [
    "### Summary Descriptive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e43a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'final_merged_data' is your dataset\n",
    "# Ensure 'date' column is in datetime format\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "\n",
    "# Descriptive statistics for numerical columns\n",
    "numerical_cols = train_data.select_dtypes(include=[np.number]).columns\n",
    "print(\"Descriptive Statistics for Numerical Columns:\")\n",
    "print(train_data[numerical_cols].describe().transpose())\n",
    "\n",
    "# Descriptive statistics for boolean columns\n",
    "boolean_cols = train_data.select_dtypes(include=[bool]).columns\n",
    "print(\"\\nDescriptive Statistics for Boolean Columns:\")\n",
    "boolean_stats = train_data[boolean_cols].astype(int).describe().transpose()\n",
    "boolean_stats.index = boolean_stats.index.map(lambda x: f'{x} (True/False)')\n",
    "print(boolean_stats)\n",
    "\n",
    "# Descriptive statistics for categorical columns\n",
    "categorical_cols = train_data.select_dtypes(include=[object]).columns\n",
    "print(\"\\nDescriptive Statistics for Categorical Columns:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(f\"Number of unique values: {train_data[col].nunique()}\")\n",
    "    print(f\"Top value: {train_data[col].mode()[0]}\")\n",
    "    print(f\"Frequency of top value: {train_data[col].value_counts().iloc[0]}\")\n",
    "    print(f\"Value counts:\\n{train_data[col].value_counts()}\\n\")\n",
    "\n",
    "# For datetime columns, show min and max\n",
    "date_columns = train_data.select_dtypes(include=[np.datetime64]).columns\n",
    "print(\"\\nDate Range for Date Columns:\")\n",
    "for column in date_columns:\n",
    "    print(f\"\\nColumn: {column}\")\n",
    "    print(f\"Start Date: {train_data[column].min()}\")\n",
    "    print(f\"End Date: {train_data[column].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57985a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data is your DataFrame\n",
    "\n",
    "# Find all boolean columns\n",
    "bool_columns = train_data.select_dtypes(include='bool').columns\n",
    "\n",
    "# Function to calculate percentages\n",
    "def calculate_percentages(column):\n",
    "    value_counts = train_data[column].value_counts(normalize=True) * 100\n",
    "    return value_counts\n",
    "\n",
    "# Print percentages for each boolean column\n",
    "for column in bool_columns:\n",
    "    percentages = calculate_percentages(column)\n",
    "    print(f\"Percentages for {column}:\")\n",
    "    print(f\"0: {percentages.get(False, 0):.2f}%\")\n",
    "    print(f\"1: {percentages.get(True, 0):.2f}%\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6ce802",
   "metadata": {},
   "source": [
    "### Outliers detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9006a8f",
   "metadata": {},
   "source": [
    "#### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e72924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select only numerical columns\n",
    "numerical_columns = train_data.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Plot boxplots for each numerical column\n",
    "for column in numerical_columns:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.boxplot(train_data[column].dropna(), vert=False)\n",
    "    plt.title(f'Boxplot for {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e88dd9c",
   "metadata": {},
   "source": [
    "#### Z-scores with threshold 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da2812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ensure the required columns are present in the DataFrame\n",
    "excluded_columns = ['longitude', 'latitude', 'id', 'host_id']\n",
    "\n",
    "# Select numerical columns except the excluded ones\n",
    "numerical_columns = train_data.select_dtypes(include=['number']).columns\n",
    "numerical_columns = [col for col in numerical_columns if col not in excluded_columns]\n",
    "\n",
    "# Function to identify outliers using z-score\n",
    "def identify_outliers_zscore(df, column, threshold=3):\n",
    "    z_scores = np.abs((df[column] - df[column].mean()) / df[column].std())\n",
    "    outliers = df[z_scores > threshold]\n",
    "    return outliers\n",
    "\n",
    "# Analyze outliers for each numerical column\n",
    "for column in numerical_columns:\n",
    "    outliers = identify_outliers_zscore(train_data, column)\n",
    "    num_outliers = len(outliers)\n",
    "    \n",
    "    # Print outlier statistics\n",
    "    print(f\"Number of outliers in '{column}': {num_outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a5970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Columns that need to be corrected for outliers\n",
    "columns_to_correct = [\n",
    "    'price', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'total_amenities',\n",
    "    'host_listings_count', 'host_total_listings_count', 'calculated_host_listings_count',\n",
    "    'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms',\n",
    "    'calculated_host_listings_count_shared_rooms', 'number_of_reviews_ltm', 'number_of_reviews_l30d',\n",
    "    'reviews_per_month', 'nearby_transport'\n",
    "]\n",
    "\n",
    "# Make a copy of the train_data\n",
    "train_data_corrected = train_data.copy()\n",
    "\n",
    "# Function to identify and remove outliers using z-score\n",
    "def remove_outliers_zscore(df, columns, threshold=3):\n",
    "    for column in columns:\n",
    "        z_scores = np.abs((df[column] - df[column].mean()) / df[column].std())\n",
    "        df = df[z_scores <= threshold]\n",
    "    return df\n",
    "\n",
    "# Apply the function to remove outliers\n",
    "train_data_corrected = remove_outliers_zscore(train_data_corrected, columns_to_correct)\n",
    "\n",
    "# Print the sizes of the original and corrected datasets to verify\n",
    "print(f\"Original train_data size: {train_data.shape[0]}\")\n",
    "print(f\"Corrected train_data size: {train_data_corrected.shape[0]}\")\n",
    "\n",
    "# Optionally, save the corrected dataset to a CSV file\n",
    "# train_data_corrected.to_csv('train_data_corrected.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f29aa91",
   "metadata": {},
   "source": [
    "#### Recalculate mean_price_neighbors for train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e08379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if the required columns are present in the DataFrame\n",
    "required_columns = ['latitude', 'longitude', 'price', 'date']\n",
    "missing_columns = [col for col in required_columns if col not in train_data_corrected.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    raise KeyError(f\"The following required columns are missing from the DataFrame: {missing_columns}\")\n",
    "\n",
    "# Initialize a list to store mean neighbor prices for each row\n",
    "mean_neighbor_prices = []\n",
    "\n",
    "# Group by date\n",
    "grouped = train_data_corrected.groupby('date')\n",
    "\n",
    "# Iterate over each group (i.e., each date)\n",
    "for date, group in tqdm(grouped, desc=\"Processing dates\"):\n",
    "    if len(group) < 2:\n",
    "        # If there's less than 2 listings for a date, use the original price for that row\n",
    "        mean_neighbor_prices.extend(group['price'])\n",
    "        continue\n",
    "    \n",
    "    # Prepare data for KDTree\n",
    "    coordinates = group[['latitude', 'longitude']].values\n",
    "    prices = group['price'].values\n",
    "    \n",
    "    # Build the KDTree for fast lookup of nearest neighbors\n",
    "    tree = cKDTree(coordinates)\n",
    "    \n",
    "    # Define the number of neighbors to consider (5 closest neighbors)\n",
    "    k = 5\n",
    "    \n",
    "    # Query the tree for the k nearest neighbors for each listing\n",
    "    distances, indices = tree.query(coordinates, k=k+1)  # k+1 because the nearest neighbor includes the point itself\n",
    "    \n",
    "    # Calculate mean price of the nearest neighbors, excluding the point itself\n",
    "    for idx in indices:\n",
    "        mean_price = prices[idx[1:]].mean()  # Exclude the point itself\n",
    "        mean_neighbor_prices.append(mean_price)\n",
    "\n",
    "# Add the mean_neighbor_prices as a new column in the DataFrame\n",
    "train_data_corrected['mean_price_neighbors'] = np.round(mean_neighbor_prices).astype(int)\n",
    "\n",
    "# Display the result\n",
    "print(train_data_corrected[\"mean_price_neighbors\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75997c",
   "metadata": {},
   "source": [
    "#### Cap the validation and test set based on train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9381d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Columns that need to be corrected for outliers\n",
    "columns_to_correct = [\n",
    "    'price', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'total_amenities',\n",
    "    'host_listings_count', 'host_total_listings_count', 'calculated_host_listings_count',\n",
    "    'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms',\n",
    "    'calculated_host_listings_count_shared_rooms', 'number_of_reviews_ltm', 'number_of_reviews_l30d',\n",
    "    'reviews_per_month', 'nearby_transport'\n",
    "]\n",
    "\n",
    "# Make copies of the valid_data and test_data\n",
    "valid_data_corrected = valid_data.copy()\n",
    "test_data_corrected = test_data.copy()\n",
    "\n",
    "# Calculate the maximum values for the specified columns in the train_data_corrected\n",
    "max_values = train_data_corrected[columns_to_correct].max()\n",
    "\n",
    "# Function to remove rows with values higher than the max values in the specified columns\n",
    "def remove_rows_above_max(df, max_values):\n",
    "    for column in max_values.index:\n",
    "        df = df[df[column] <= max_values[column]]\n",
    "    return df\n",
    "\n",
    "# Apply the function to valid_data and test_data\n",
    "valid_data_corrected = remove_rows_above_max(valid_data_corrected, max_values)\n",
    "test_data_corrected = remove_rows_above_max(test_data_corrected, max_values)\n",
    "\n",
    "# Print the sizes of the original and corrected datasets to verify\n",
    "print(f\"Original valid_data size: {valid_data.shape[0]}\")\n",
    "print(f\"Corrected valid_data size: {valid_data_corrected.shape[0]}\")\n",
    "print(f\"Original test_data size: {test_data.shape[0]}\")\n",
    "print(f\"Corrected test_data size: {test_data_corrected.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1dd9ae",
   "metadata": {},
   "source": [
    "#### Recalculate mean_price_neighbors for valid and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75246ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if the required columns are present in the DataFrame\n",
    "required_columns = ['latitude', 'longitude', 'price', 'date']\n",
    "missing_columns = [col for col in required_columns if col not in valid_data_corrected.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    raise KeyError(f\"The following required columns are missing from the DataFrame: {missing_columns}\")\n",
    "\n",
    "# Initialize a list to store mean neighbor prices for each row\n",
    "mean_neighbor_prices = []\n",
    "\n",
    "# Group by date\n",
    "grouped = valid_data_corrected.groupby('date')\n",
    "\n",
    "# Iterate over each group (i.e., each date)\n",
    "for date, group in tqdm(grouped, desc=\"Processing dates\"):\n",
    "    if len(group) < 2:\n",
    "        # If there's less than 2 listings for a date, use the original price for that row\n",
    "        mean_neighbor_prices.extend(group['price'])\n",
    "        continue\n",
    "    \n",
    "    # Prepare data for KDTree\n",
    "    coordinates = group[['latitude', 'longitude']].values\n",
    "    prices = group['price'].values\n",
    "    \n",
    "    # Build the KDTree for fast lookup of nearest neighbors\n",
    "    tree = cKDTree(coordinates)\n",
    "    \n",
    "    # Define the number of neighbors to consider (5 closest neighbors)\n",
    "    k = 5\n",
    "    \n",
    "    # Query the tree for the k nearest neighbors for each listing\n",
    "    distances, indices = tree.query(coordinates, k=k+1)  # k+1 because the nearest neighbor includes the point itself\n",
    "    \n",
    "    # Calculate mean price of the nearest neighbors, excluding the point itself\n",
    "    for idx in indices:\n",
    "        mean_price = prices[idx[1:]].mean()  # Exclude the point itself\n",
    "        mean_neighbor_prices.append(mean_price)\n",
    "\n",
    "# Add the mean_neighbor_prices as a new column in the DataFrame\n",
    "valid_data_corrected['mean_price_neighbors'] = np.round(mean_neighbor_prices).astype(int)\n",
    "\n",
    "# Display the result\n",
    "print(valid_data_corrected[\"mean_price_neighbors\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c882a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if the required columns are present in the DataFrame\n",
    "required_columns = ['latitude', 'longitude', 'price', 'date']\n",
    "missing_columns = [col for col in required_columns if col not in test_data_corrected.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    raise KeyError(f\"The following required columns are missing from the DataFrame: {missing_columns}\")\n",
    "\n",
    "# Initialize a list to store mean neighbor prices for each row\n",
    "mean_neighbor_prices = []\n",
    "\n",
    "# Group by date\n",
    "grouped = test_data_corrected.groupby('date')\n",
    "\n",
    "# Iterate over each group (i.e., each date)\n",
    "for date, group in tqdm(grouped, desc=\"Processing dates\"):\n",
    "    if len(group) < 2:\n",
    "        # If there's less than 2 listings for a date, use the original price for that row\n",
    "        mean_neighbor_prices.extend(group['price'])\n",
    "        continue\n",
    "    \n",
    "    # Prepare data for KDTree\n",
    "    coordinates = group[['latitude', 'longitude']].values\n",
    "    prices = group['price'].values\n",
    "    \n",
    "    # Build the KDTree for fast lookup of nearest neighbors\n",
    "    tree = cKDTree(coordinates)\n",
    "    \n",
    "    # Define the number of neighbors to consider (5 closest neighbors)\n",
    "    k = 5\n",
    "    \n",
    "    # Query the tree for the k nearest neighbors for each listing\n",
    "    distances, indices = tree.query(coordinates, k=k+1)  # k+1 because the nearest neighbor includes the point itself\n",
    "    \n",
    "    # Calculate mean price of the nearest neighbors, excluding the point itself\n",
    "    for idx in indices:\n",
    "        mean_price = prices[idx[1:]].mean()  # Exclude the point itself\n",
    "        mean_neighbor_prices.append(mean_price)\n",
    "\n",
    "# Add the mean_neighbor_prices as a new column in the DataFrame\n",
    "test_data_corrected['mean_price_neighbors'] = np.round(mean_neighbor_prices).astype(int)\n",
    "\n",
    "# Display the result\n",
    "print(test_data_corrected[\"mean_price_neighbors\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7ed5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_corrected.to_csv('train_data_corrected_outliers.csv', index=False)\n",
    "valid_data_corrected.to_csv('valid_data_corrected_outliers.csv', index=False)\n",
    "test_data_corrected.to_csv('test_data_corrected_outliers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a83898",
   "metadata": {},
   "source": [
    "### Correlation Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afeb52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd17b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Convert the DataFrame to a Dask DataFrame\n",
    "dask_df = dd.from_pandas(train_data_corrected, npartitions=10)\n",
    "\n",
    "# Define the target column\n",
    "target = 'price'\n",
    "\n",
    "# Track the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Sample 50% of the data using Dask\n",
    "print(\"Sampling data...\")\n",
    "sampled_dask_df = dask_df.sample(frac=0.50, random_state=1).compute()\n",
    "\n",
    "# Convert the sampled data back to a Pandas DataFrame for correlation computation\n",
    "sampled_data = pd.DataFrame(sampled_dask_df)\n",
    "\n",
    "# Filter out non-numeric and boolean columns\n",
    "numeric_data = sampled_data.select_dtypes(include=[float, int, bool]).copy()\n",
    "\n",
    "# Temporarily convert boolean columns to integers for correlation computation\n",
    "boolean_columns = numeric_data.select_dtypes(include=[bool]).columns\n",
    "numeric_data[boolean_columns] = numeric_data[boolean_columns].astype(int)\n",
    "\n",
    "# Check for constant columns and remove them\n",
    "constant_columns = [col for col in numeric_data.columns if numeric_data[col].nunique() <= 1]\n",
    "if constant_columns:\n",
    "    print(f\"Removing constant columns: {constant_columns}\")\n",
    "    numeric_data.drop(columns=constant_columns, inplace=True)\n",
    "\n",
    "# Compute the correlation matrix for the numeric data\n",
    "print(\"Computing correlation matrix...\")\n",
    "correlation_matrix = numeric_data.corr()\n",
    "\n",
    "# Calculate correlations with the target column\n",
    "correlations_with_target = correlation_matrix[target].dropna().sort_values(ascending=False)\n",
    "\n",
    "# Display the correlations with the target column, sorted\n",
    "print(f\"\\nCorrelations with {target}:\")\n",
    "for col, value in correlations_with_target.items():\n",
    "    if col != target:\n",
    "        print(f\"{col}: {value:.2f}\")\n",
    "\n",
    "# Time taken for sampling and correlation computation\n",
    "sampling_time = time.time() - start_time\n",
    "print(f\"Time taken for sampling and correlation computation: {sampling_time:.2f} seconds\")\n",
    "\n",
    "# Track time for heatmap visualization with tqdm progress\n",
    "print(\"Visualizing heatmap...\")\n",
    "with tqdm(total=1, desc=\"Creating heatmap\", bar_format=\"{l_bar}{bar} [Time: {elapsed} < {remaining}, {rate_fmt}{postfix}]\") as pbar:\n",
    "    visualization_start_time = time.time()\n",
    "    \n",
    "    # Visualize the correlations with a heatmap\n",
    "    plt.figure(figsize=(20, 16))  # Adjust the size if needed\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "    plt.title('Correlation Matrix (50% Sample)')\n",
    "    plt.show()\n",
    "    \n",
    "    # Update the progress bar\n",
    "    pbar.update(1)\n",
    "\n",
    "# Time taken for heatmap visualization\n",
    "visualization_time = time.time() - visualization_start_time\n",
    "print(f\"Time taken for heatmap visualization: {visualization_time:.2f} seconds\")\n",
    "\n",
    "# Total time\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total time taken: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d15233",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_corrected[\"room_type_Shared room\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39a8426",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop =[\"room_type_Shared room\",\"calculated_host_listings_count_shared_rooms\"]\n",
    "train_data_corrected = train_data_corrected.drop(columns_to_drop, axis=1)\n",
    "valid_data_corrected = valid_data_corrected.drop(columns_to_drop, axis=1)\n",
    "test_data_corrected = test_data_corrected.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c413108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'correlation_matrix' is already computed\n",
    "# and 'target' is defined as 'price'\n",
    "\n",
    "# Extract the correlation values for the target variable\n",
    "correlation_with_target = correlation_matrix[target]\n",
    "\n",
    "# Create a DataFrame to hold both absolute and actual values\n",
    "correlation_df = pd.DataFrame({\n",
    "    'Correlation': correlation_with_target,\n",
    "    'Absolute Correlation': correlation_with_target.abs()\n",
    "})\n",
    "\n",
    "# Sort by absolute correlation values\n",
    "sorted_correlation_df = correlation_df.sort_values(by='Absolute Correlation', ascending=False)\n",
    "\n",
    "# Top 10 features most correlated with the target\n",
    "top_10_features = sorted_correlation_df.iloc[1:11]  # Exclude the target itself\n",
    "\n",
    "# Bottom 10 features least correlated with the target\n",
    "bottom_10_features = sorted_correlation_df.iloc[-10:]\n",
    "\n",
    "# Display the top 10 and bottom 10 features\n",
    "print(\"Top 10 features most correlated with the target variable:\")\n",
    "print(top_10_features[['Correlation', 'Absolute Correlation']])\n",
    "\n",
    "print(\"\\nBottom 10 features least correlated with the target variable:\")\n",
    "print(bottom_10_features[['Correlation', 'Absolute Correlation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb38c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import time\n",
    "\n",
    "# Convert the DataFrame to a Dask DataFrame\n",
    "dask_df = dd.from_pandas(train_data_corrected, npartitions=10)\n",
    "\n",
    "# Track the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Sample 50% of the data using Dask\n",
    "print(\"Sampling data...\")\n",
    "sampled_dask_df = dask_df.sample(frac=0.50, random_state=1).compute()\n",
    "\n",
    "# Convert the sampled data back to a Pandas DataFrame\n",
    "sampled_data = pd.DataFrame(sampled_dask_df)\n",
    "\n",
    "# Filter out non-numeric and boolean columns\n",
    "numeric_data = sampled_data.select_dtypes(include=[float, int, bool]).copy()\n",
    "\n",
    "# Temporarily convert boolean columns to integers for VIF calculation\n",
    "boolean_columns = numeric_data.select_dtypes(include=[bool]).columns\n",
    "numeric_data[boolean_columns] = numeric_data[boolean_columns].astype(int)\n",
    "\n",
    "# Check for constant columns and remove them\n",
    "constant_columns = [col for col in numeric_data.columns if numeric_data[col].nunique() <= 1]\n",
    "if constant_columns:\n",
    "    print(f\"Removing constant columns: {constant_columns}\")\n",
    "    numeric_data.drop(columns=constant_columns, inplace=True)\n",
    "\n",
    "# Function to calculate VIF for each feature\n",
    "def calculate_vif(df):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = df.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# Calculate VIF values\n",
    "print(\"Calculating VIF values...\")\n",
    "vif_df = calculate_vif(numeric_data)\n",
    "\n",
    "# Display the VIF values\n",
    "print(\"\\nVIF values:\")\n",
    "print(vif_df.sort_values(by=\"VIF\", ascending=False))\n",
    "\n",
    "# Time taken for sampling and VIF calculation\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total time taken: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01169b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming vif_df is your DataFrame containing the VIF values\n",
    "\n",
    "# Save the VIF values to a CSV file\n",
    "vif_df.to_csv('vif_values.csv', index=False)\n",
    "\n",
    "print(\"VIF values saved to 'vif_values.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the VIF values from the CSV file\n",
    "vif_df = pd.read_csv('vif_values.csv')\n",
    "\n",
    "print(\"VIF values read from 'vif_values.csv':\")\n",
    "print(vif_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4adcd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming correlation_matrix and high_vif_df have been already calculated\n",
    "\n",
    "# Filter VIF values above 5\n",
    "high_vif_df = vif_df[vif_df[\"VIF\"] > 5]\n",
    "\n",
    "# Display the high VIF values\n",
    "print(\"\\nVIF values above 5:\")\n",
    "print(high_vif_df.sort_values(by=\"VIF\", ascending=False))\n",
    "\n",
    "# Find all pairs of features with a correlation higher than 0.8\n",
    "high_corr_pairs = []\n",
    "for i in range(correlation_matrix.shape[0]):\n",
    "    for j in range(i + 1, correlation_matrix.shape[1]):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            high_corr_pairs.append((correlation_matrix.index[i], correlation_matrix.columns[j], correlation_matrix.iloc[i, j]))\n",
    "\n",
    "# Display the pairs of features with high correlations\n",
    "print(\"\\nPairs of features with absolute correlation higher than 0.8:\")\n",
    "for pair in high_corr_pairs:\n",
    "    print(f\"{pair[0]} and {pair[1]}: correlation = {pair[2]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca2b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decided columns to drop based on the correlation and multivariate analysis\n",
    "columns_to_drop =[\"review_scores_accuracy\",\"review_scores_cleanliness\",\"review_scores_checkin\",\"review_scores_communication\"\n",
    "                 ,\"review_scores_location\",\"review_scores_value\",\"availability_60\",\"availability_30\"\n",
    "                 ,'season_Spring', 'season_Summer', 'season_Winter',\"room_type_Hotel room\",'host_response_rate']\n",
    "train_data_corrected = train_data_corrected.drop(columns_to_drop, axis=1)\n",
    "valid_data_corrected = valid_data_corrected.drop(columns_to_drop, axis=1)\n",
    "test_data_corrected = test_data_corrected.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422242ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data_corrected, valid_data_corrected, and test_data_corrected are your DataFrames\n",
    "\n",
    "# Define the kitchen-related features to combine\n",
    "kitchen_features = ['Dishes and silverware', 'refrigerator', 'Cooking basics']\n",
    "\n",
    "# Create a new feature 'kitchen_amenities' as the sum of the kitchen-related features for train_data_corrected\n",
    "train_data_corrected['kitchen_amenities'] = train_data_corrected[kitchen_features].sum(axis=1)\n",
    "# Drop the original kitchen-related features for train_data_corrected\n",
    "train_data_corrected = train_data_corrected.drop(columns=kitchen_features)\n",
    "\n",
    "# Repeat the same for valid_data_corrected\n",
    "valid_data_corrected['kitchen_amenities'] = valid_data_corrected[kitchen_features].sum(axis=1)\n",
    "valid_data_corrected = valid_data_corrected.drop(columns=kitchen_features)\n",
    "\n",
    "# Repeat the same for test_data_corrected\n",
    "test_data_corrected['kitchen_amenities'] = test_data_corrected[kitchen_features].sum(axis=1)\n",
    "test_data_corrected = test_data_corrected.drop(columns=kitchen_features)\n",
    "\n",
    "# Display the first few rows and describe of the modified DataFrames to verify the changes\n",
    "print(\"Train Data:\")\n",
    "print(train_data_corrected[\"kitchen_amenities\"].describe())\n",
    "\n",
    "print(\"\\nValid Data:\")\n",
    "print(valid_data_corrected[\"kitchen_amenities\"].describe())\n",
    "\n",
    "print(\"\\nTest Data:\")\n",
    "print(test_data_corrected[\"kitchen_amenities\"].describe())\n",
    "\n",
    "# Optionally, save the modified DataFrames to CSV files\n",
    "# train_data_corrected.to_csv('train_data_corrected_with_kitchen_amenities.csv', index=False)\n",
    "# valid_data_corrected.to_csv('valid_data_corrected_with_kitchen_amenities.csv', index=False)\n",
    "# test_data_corrected.to_csv('test_data_corrected_with_kitchen_amenities.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d221b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_corrected.to_csv('train_data_corrected_with_kitchen_amenities.csv', index=False)\n",
    "valid_data_corrected.to_csv('valid_data_corrected_with_kitchen_amenities.csv', index=False)\n",
    "test_data_corrected.to_csv('test_data_corrected_with_kitchen_amenities.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32b73e9",
   "metadata": {},
   "source": [
    "### Missing value check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5d6a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data_corrected, valid_data_corrected, and test_data_corrected are your DataFrames\n",
    "\n",
    "# Function to check and print rows with missing values\n",
    "def check_missing_values(df, df_name):\n",
    "    rows_with_missing_values = df[df.isnull().any(axis=1)]\n",
    "    \n",
    "    print(f\"Rows with missing values in {df_name}:\")\n",
    "    print(rows_with_missing_values)\n",
    "    \n",
    "    # Display the total number of rows with missing values\n",
    "    print(f\"Total number of rows with missing values in {df_name}: {rows_with_missing_values.shape[0]}\")\n",
    "    \n",
    "    # Optionally, save these rows to a CSV file\n",
    "    # rows_with_missing_values.to_csv(f'rows_with_missing_values_in_{df_name}.csv', index=False)\n",
    "\n",
    "# Check for missing values in train_data_corrected\n",
    "check_missing_values(train_data_corrected, \"train_data_corrected\")\n",
    "\n",
    "# Check for missing values in valid_data_corrected\n",
    "check_missing_values(valid_data_corrected, \"valid_data_corrected\")\n",
    "\n",
    "# Check for missing values in test_data_corrected\n",
    "check_missing_values(test_data_corrected, \"test_data_corrected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40524529",
   "metadata": {},
   "source": [
    "# Saving the final datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf692b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/NY Airbnb_Spatiotemporal_Analysis/Data Cleaning Listings/train_data_corrected_with_kitchen_amenities.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "final_train_data= pd.read_csv(file_path)\n",
    "\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/NY Airbnb_Spatiotemporal_Analysis/Data Cleaning Listings/valid_data_corrected_with_kitchen_amenities.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "final_val_data= pd.read_csv(file_path)\n",
    "\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/NY Airbnb_Spatiotemporal_Analysis/Data Cleaning Listings/test_data_corrected_with_kitchen_amenities.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "final_test_data= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fa1280",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_data.info()\n",
    "final_val_data.info()\n",
    "final_test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2243b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_data.to_csv('FINAL_TRAIN_DATA_NY.csv', index=False)\n",
    "final_val_data.to_csv('FINAL_VALID_DATA_NY.csv', index=False)\n",
    "final_test_data.to_csv('FINAL_TEST_DATA_NY.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
