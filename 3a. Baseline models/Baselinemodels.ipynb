{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c7fbd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Data Cleaning Listings/FINAL_TRAIN_DATA_PARIS.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "train_data_paris= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Data Cleaning Listings/FINAL_VALID_DATA_PARIS.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "valid_data_paris= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Data Cleaning Listings/FINAL_TEST_DATA_PARIS.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "test_data_paris= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7aea2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2776915 entries, 0 to 2776914\n",
      "Columns: 145 entries, id to kitchen_amenities\n",
      "dtypes: bool(105), float64(15), int64(24), object(1)\n",
      "memory usage: 1.1+ GB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 321564 entries, 0 to 321563\n",
      "Columns: 145 entries, id to kitchen_amenities\n",
      "dtypes: bool(105), float64(15), int64(24), object(1)\n",
      "memory usage: 130.3+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 323624 entries, 0 to 323623\n",
      "Columns: 145 entries, id to kitchen_amenities\n",
      "dtypes: bool(105), float64(15), int64(24), object(1)\n",
      "memory usage: 131.2+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data_paris.info()\n",
    "valid_data_paris.info()\n",
    "test_data_paris.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f010848",
   "metadata": {},
   "source": [
    "# Reduction dataset for baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51afadd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 377965 entries, 0 to 377964\n",
      "Columns: 145 entries, id to kitchen_amenities\n",
      "dtypes: bool(105), float64(15), int64(24), object(1)\n",
      "memory usage: 153.2+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 95272 entries, 0 to 95271\n",
      "Columns: 145 entries, id to kitchen_amenities\n",
      "dtypes: bool(105), float64(15), int64(24), object(1)\n",
      "memory usage: 38.6+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105978 entries, 0 to 105977\n",
      "Columns: 145 entries, id to kitchen_amenities\n",
      "dtypes: bool(105), float64(15), int64(24), object(1)\n",
      "memory usage: 43.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to reduce data to a specified number of rows per ID per month\n",
    "def reduce_to_n_rows_per_month(df, n, random_state=42):\n",
    "    # Group by id, month, and year, and randomly select n rows from each group\n",
    "    reduced_df = df.groupby(['id', 'year', 'month']).apply(lambda x: x.sample(min(n, len(x)), random_state=random_state)).reset_index(drop=True)\n",
    "    return reduced_df\n",
    "\n",
    "# Apply the reduction with different row limits for each set\n",
    "train_data_paris_reduced = reduce_to_n_rows_per_month(train_data_paris, 4)\n",
    "valid_data_paris_reduced = reduce_to_n_rows_per_month(valid_data_paris, 8)\n",
    "test_data_paris_reduced = reduce_to_n_rows_per_month(test_data_paris, 8)\n",
    "\n",
    "# Check the size of each DataFrame\n",
    "print(train_data_paris_reduced.info())\n",
    "print(valid_data_paris_reduced.info())\n",
    "print(test_data_paris_reduced.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ffd883a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Percentage: 60.00%\n",
      "Validation Data Percentage: 20.00%\n",
      "Test Data Percentage: 20.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of rows\n",
    "total_rows = len(train_data_paris_reduced) + len(valid_data_paris_reduced) + len(test_data_paris_reduced)\n",
    "\n",
    "# Calculate the percentage of each set\n",
    "train_percentage = (len(train_data_paris_reduced) / total_rows) * 100\n",
    "valid_percentage = (len(valid_data_paris_reduced) / total_rows) * 100\n",
    "test_percentage = (len(test_data_paris_reduced) / total_rows) * 100\n",
    "\n",
    "print(f\"Train Data Percentage: {train_percentage:.2f}%\")\n",
    "\n",
    "print(f\"Validation Data Percentage: {valid_percentage:.2f}%\")\n",
    "\n",
    "print(f\"Test Data Percentage: {test_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a6c80ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in reduced Train Data: 285816\n",
      "Number of rows in reduced Valid Data: 95272\n",
      "Number of rows in reduced Test Data: 95272\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to randomly reduce the number of rows in a DataFrame to a specified target\n",
    "def reduce_dataframe(df, target_rows, random_state=42):\n",
    "    if len(df) > target_rows:\n",
    "        return df.sample(target_rows, random_state=random_state)\n",
    "    return df\n",
    "\n",
    "# Reduce the training and testing sets\n",
    "train_data_paris_reduced = reduce_dataframe(train_data_paris, 285816)\n",
    "test_data_paris_reduced = reduce_dataframe(test_data_paris, 95272)\n",
    "\n",
    "# Print the size of each DataFrame to verify\n",
    "print(f\"Number of rows in reduced Train Data: {len(train_data_paris_reduced)}\")\n",
    "print(f\"Number of rows in reduced Valid Data: {len(valid_data_paris_reduced)}\")\n",
    "print(f\"Number of rows in reduced Test Data: {len(test_data_paris_reduced)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d97cb6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Percentage: 60.00%\n",
      "Validation Data Percentage: 20.00%\n",
      "Test Data Percentage: 20.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of rows\n",
    "total_rows = len(train_data_paris_reduced) + len(valid_data_paris_reduced) + len(test_data_paris_reduced)\n",
    "\n",
    "# Calculate the percentage of each set\n",
    "train_percentage = (len(train_data_paris_reduced) / total_rows) * 100\n",
    "valid_percentage = (len(valid_data_paris_reduced) / total_rows) * 100\n",
    "test_percentage = (len(test_data_paris_reduced) / total_rows) * 100\n",
    "\n",
    "print(f\"Train Data Percentage: {train_percentage:.2f}%\")\n",
    "\n",
    "print(f\"Validation Data Percentage: {valid_percentage:.2f}%\")\n",
    "\n",
    "print(f\"Test Data Percentage: {test_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b747444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set start time: 2023-03-13, end time: 2024-01-31\n",
      "Validation set start time: 2024-02-01, end time: 2024-05-17\n",
      "Test set start time: 2024-05-19, end time: 2024-09-04\n",
      "No unique IDs appearing in both Training and Validation sets.\n",
      "No unique IDs appearing in both Training and Test sets.\n",
      "No unique IDs appearing in both Validation and Test sets.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get start and end times for each split\n",
    "train_start_time = train_data_paris_reduced['date'].min()\n",
    "train_end_time = train_data_paris_reduced['date'].max()\n",
    "valid_start_time = valid_data_paris_reduced['date'].min()\n",
    "valid_end_time = valid_data_paris_reduced['date'].max()\n",
    "test_start_time = test_data_paris_reduced['date'].min()\n",
    "test_end_time = test_data_paris_reduced['date'].max()\n",
    "\n",
    "# Print start and end times\n",
    "print(f\"Training set start time: {train_start_time}, end time: {train_end_time}\")\n",
    "print(f\"Validation set start time: {valid_start_time}, end time: {valid_end_time}\")\n",
    "print(f\"Test set start time: {test_start_time}, end time: {test_end_time}\")\n",
    "\n",
    "# Function to check if a unique ID appears in more than one set\n",
    "def check_unique_ids(train_df, valid_df, test_df):\n",
    "    train_ids = set(train_df['id'])\n",
    "    valid_ids = set(valid_df['id'])\n",
    "    test_ids = set(test_df['id'])\n",
    "\n",
    "    common_train_valid = train_ids.intersection(valid_ids)\n",
    "    common_train_test = train_ids.intersection(test_ids)\n",
    "    common_valid_test = valid_ids.intersection(test_ids)\n",
    "\n",
    "    if common_train_valid:\n",
    "        print(f\"Unique IDs appearing in both Training and Validation sets: {len(common_train_valid)}\")\n",
    "        print(common_train_valid)\n",
    "    else:\n",
    "        print(\"No unique IDs appearing in both Training and Validation sets.\")\n",
    "\n",
    "    if common_train_test:\n",
    "        print(f\"Unique IDs appearing in both Training and Test sets: {len(common_train_test)}\")\n",
    "        print(common_train_test)\n",
    "    else:\n",
    "        print(\"No unique IDs appearing in both Training and Test sets.\")\n",
    "\n",
    "    if common_valid_test:\n",
    "        print(f\"Unique IDs appearing in both Validation and Test sets: {len(common_valid_test)}\")\n",
    "        print(common_valid_test)\n",
    "    else:\n",
    "        print(\"No unique IDs appearing in both Validation and Test sets.\")\n",
    "\n",
    "# Check for unique IDs appearing in more than one set\n",
    "check_unique_ids(train_data_paris_reduced, valid_data_paris_reduced, test_data_paris_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "221dc26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_paris_reduced.to_csv('Baslinemodels_train_paris.csv', index=False)\n",
    "valid_data_paris_reduced.to_csv('Baslinemodels_valid_paris.csv', index=False)\n",
    "test_data_paris_reduced.to_csv('Baslinemodels_test_paris.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd7fed9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 285816 entries, 2069150 to 1887596\n",
      "Columns: 145 entries, id to kitchen_amenities\n",
      "dtypes: bool(105), float64(15), int64(24), object(1)\n",
      "memory usage: 118.0+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 95272 entries, 0 to 95271\n",
      "Columns: 145 entries, id to kitchen_amenities\n",
      "dtypes: bool(105), float64(15), int64(24), object(1)\n",
      "memory usage: 38.6+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 95272 entries, 272598 to 237313\n",
      "Columns: 145 entries, id to kitchen_amenities\n",
      "dtypes: bool(105), float64(15), int64(24), object(1)\n",
      "memory usage: 39.3+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data_paris_reduced.info()\n",
    "valid_data_paris_reduced.info()\n",
    "test_data_paris_reduced.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a01f46",
   "metadata": {},
   "source": [
    "# Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78d7be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/Baslinemodels_train_paris.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "train_data_paris= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/Baslinemodels_valid_paris.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "valid_data_paris= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/Baslinemodels_test_paris.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "test_data_paris= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f6404",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95effab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Model: 100%|██████████ [time left: 00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  21.131227779473765\n",
      "Mean Squared Error (MSE):  1107.022706885795\n",
      "R-squared (R2):  0.9179340297082517\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  50.67080064217383\n",
      "Mean Squared Error (MSE):  11182.36228352938\n",
      "R-squared (R2):  0.4039117818610509\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  64.29577800362003\n",
      "Mean Squared Error (MSE):  20979.798725497763\n",
      "R-squared (R2):  0.32326455460456327\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor # This import will now work\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train, y_train = prepare_data(train_data_paris, 'price')\n",
    "X_valid, y_valid = prepare_data(valid_data_paris, 'price')\n",
    "X_test, y_test = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the model with default parameters\n",
    "xgb = XGBRegressor(objective='reg:squarederror', eval_metric='rmse')\n",
    "\n",
    "# Wrap the fitting process with tqdm for visualization\n",
    "with tqdm(total=1, desc=\"Fitting Model\", bar_format=\"{l_bar}{bar} [time left: {remaining}]\") as pbar:\n",
    "    xgb.fit(X_train_scaled, y_train)\n",
    "    pbar.update()  # Simulate completion\n",
    "\n",
    "# Predict on the training, validation, and test sets\n",
    "y_train_pred = xgb.predict(X_train_scaled)\n",
    "y_valid_pred = xgb.predict(X_valid_scaled)\n",
    "y_test_pred = xgb.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print_metrics(y_train, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea390cd",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "867f4afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 18:31:28,052] A new study created in memory with name: no-name-fe94ea03-7102-4396-807c-d2b673c2c5bf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72542f55e29941e2b06208aa608c2e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 18:31:45,712] Trial 0 finished with value: 48.658787187555795 and parameters: {'n_estimators': 273, 'max_depth': 5, 'learning_rate': 0.14364220513745304, 'subsample': 0.7975244972426706, 'colsample_bytree': 0.6658434626177697, 'gamma': 3.477531017390301, 'reg_alpha': 7.606335318815843, 'reg_lambda': 1.1792543209331618}. Best is trial 0 with value: 48.658787187555795.\n",
      "[I 2024-07-23 18:32:30,193] Trial 1 finished with value: 49.52748401755699 and parameters: {'n_estimators': 626, 'max_depth': 7, 'learning_rate': 0.118358441131174, 'subsample': 0.6857309447916276, 'colsample_bytree': 0.5516696611602749, 'gamma': 0.665599921130367, 'reg_alpha': 5.980431545766281, 'reg_lambda': 2.88303844170911}. Best is trial 0 with value: 48.658787187555795.\n",
      "[I 2024-07-23 18:33:29,040] Trial 2 finished with value: 48.562497663937364 and parameters: {'n_estimators': 931, 'max_depth': 6, 'learning_rate': 0.15372254710641553, 'subsample': 0.9658004770596977, 'colsample_bytree': 0.9772244505392327, 'gamma': 1.5350073114237768, 'reg_alpha': 7.113668123682095, 'reg_lambda': 9.673536941764725}. Best is trial 2 with value: 48.562497663937364.\n",
      "[I 2024-07-23 18:34:19,400] Trial 3 finished with value: 51.1137446994758 and parameters: {'n_estimators': 878, 'max_depth': 4, 'learning_rate': 0.23000833626760256, 'subsample': 0.5033735201074552, 'colsample_bytree': 0.6865654774503183, 'gamma': 3.0456288060813175, 'reg_alpha': 9.992680942883933, 'reg_lambda': 4.019175617496813}. Best is trial 2 with value: 48.562497663937364.\n",
      "[I 2024-07-23 18:34:44,579] Trial 4 finished with value: 49.808920595025086 and parameters: {'n_estimators': 354, 'max_depth': 8, 'learning_rate': 0.18328928377234488, 'subsample': 0.9672095247206096, 'colsample_bytree': 0.6336739101080888, 'gamma': 3.7839869408847897, 'reg_alpha': 6.038706986876933, 'reg_lambda': 4.722567722657542}. Best is trial 2 with value: 48.562497663937364.\n",
      "[I 2024-07-23 18:35:32,840] Trial 5 finished with value: 52.64215784095611 and parameters: {'n_estimators': 780, 'max_depth': 5, 'learning_rate': 0.2946264000446167, 'subsample': 0.8038084410013682, 'colsample_bytree': 0.8690418622211971, 'gamma': 4.7762455942745525, 'reg_alpha': 0.5172867812904458, 'reg_lambda': 7.047764206321256}. Best is trial 2 with value: 48.562497663937364.\n",
      "[I 2024-07-23 18:36:11,452] Trial 6 finished with value: 46.46657380614095 and parameters: {'n_estimators': 504, 'max_depth': 8, 'learning_rate': 0.011062255920556072, 'subsample': 0.5505569609960044, 'colsample_bytree': 0.6364398070639476, 'gamma': 2.9835346558652724, 'reg_alpha': 0.6724275273195857, 'reg_lambda': 6.187471167777151}. Best is trial 6 with value: 46.46657380614095.\n",
      "[I 2024-07-23 18:36:57,628] Trial 7 finished with value: 49.85563612939584 and parameters: {'n_estimators': 789, 'max_depth': 4, 'learning_rate': 0.18667132151043703, 'subsample': 0.5831175233170285, 'colsample_bytree': 0.808289454310865, 'gamma': 4.09487947633762, 'reg_alpha': 2.519171171738667, 'reg_lambda': 4.064921118726632}. Best is trial 6 with value: 46.46657380614095.\n",
      "[I 2024-07-23 18:37:30,541] Trial 8 finished with value: 47.92704417281311 and parameters: {'n_estimators': 352, 'max_depth': 10, 'learning_rate': 0.07990644666641222, 'subsample': 0.6903885990864715, 'colsample_bytree': 0.7364581019668803, 'gamma': 3.842473823158356, 'reg_alpha': 7.444684659871036, 'reg_lambda': 3.6465477592477438}. Best is trial 6 with value: 46.46657380614095.\n",
      "[I 2024-07-23 18:38:23,408] Trial 9 finished with value: 51.179682612871765 and parameters: {'n_estimators': 935, 'max_depth': 3, 'learning_rate': 0.2458620698117055, 'subsample': 0.5554091443038409, 'colsample_bytree': 0.8411068831593677, 'gamma': 4.473185542995154, 'reg_alpha': 9.517308747345716, 'reg_lambda': 3.078865633175123}. Best is trial 6 with value: 46.46657380614095.\n",
      "[I 2024-07-23 18:38:35,443] Trial 10 finished with value: 50.497843484672735 and parameters: {'n_estimators': 114, 'max_depth': 10, 'learning_rate': 0.013083757489556034, 'subsample': 0.6393907109111368, 'colsample_bytree': 0.5402585411618177, 'gamma': 2.1754447474308507, 'reg_alpha': 3.1446193432490444, 'reg_lambda': 6.955862070719487}. Best is trial 6 with value: 46.46657380614095.\n",
      "[I 2024-07-23 18:39:19,569] Trial 11 finished with value: 46.7422937183899 and parameters: {'n_estimators': 488, 'max_depth': 10, 'learning_rate': 0.019490953847458678, 'subsample': 0.7198158970139454, 'colsample_bytree': 0.744317699174061, 'gamma': 2.5683040592642765, 'reg_alpha': 0.12880526984092744, 'reg_lambda': 6.535829473880231}. Best is trial 6 with value: 46.46657380614095.\n",
      "[I 2024-07-23 18:40:05,491] Trial 12 finished with value: 46.54237667162299 and parameters: {'n_estimators': 554, 'max_depth': 9, 'learning_rate': 0.017382989955669503, 'subsample': 0.8627764748605822, 'colsample_bytree': 0.6038275158106017, 'gamma': 2.4724373248092966, 'reg_alpha': 0.12301775024360846, 'reg_lambda': 6.632597589523086}. Best is trial 6 with value: 46.46657380614095.\n",
      "[I 2024-07-23 18:40:50,522] Trial 13 finished with value: 47.278807015430615 and parameters: {'n_estimators': 623, 'max_depth': 8, 'learning_rate': 0.06409162311745896, 'subsample': 0.8737732186398918, 'colsample_bytree': 0.6006417090833162, 'gamma': 1.5167388875862051, 'reg_alpha': 1.9067178974160663, 'reg_lambda': 8.843704732686493}. Best is trial 6 with value: 46.46657380614095.\n",
      "[I 2024-07-23 18:41:28,744] Trial 14 finished with value: 47.21491583685539 and parameters: {'n_estimators': 504, 'max_depth': 8, 'learning_rate': 0.063953311129562, 'subsample': 0.8802420526313939, 'colsample_bytree': 0.5125576739891854, 'gamma': 2.3891875621903567, 'reg_alpha': 3.9190527959168913, 'reg_lambda': 5.882241601867477}. Best is trial 6 with value: 46.46657380614095.\n",
      "[I 2024-07-23 18:42:18,898] Trial 15 finished with value: 46.67924319896983 and parameters: {'n_estimators': 640, 'max_depth': 9, 'learning_rate': 0.039634981700523056, 'subsample': 0.8904473898818874, 'colsample_bytree': 0.5956139317217096, 'gamma': 0.20705236590723475, 'reg_alpha': 1.222916497018666, 'reg_lambda': 8.236437215293215}. Best is trial 6 with value: 46.46657380614095.\n",
      "[I 2024-07-23 18:42:49,651] Trial 16 finished with value: 48.57067998361103 and parameters: {'n_estimators': 431, 'max_depth': 7, 'learning_rate': 0.10984785002956471, 'subsample': 0.7793778264003706, 'colsample_bytree': 0.6792471820044059, 'gamma': 2.9397051688803377, 'reg_alpha': 3.91820425559087, 'reg_lambda': 5.684976839818079}. Best is trial 6 with value: 46.46657380614095.\n",
      "[I 2024-07-23 18:43:45,788] Trial 17 finished with value: 47.733910026693636 and parameters: {'n_estimators': 737, 'max_depth': 9, 'learning_rate': 0.08896161469670666, 'subsample': 0.6304279723542059, 'colsample_bytree': 0.5997202075540309, 'gamma': 1.72842220808736, 'reg_alpha': 1.5536512171792682, 'reg_lambda': 7.652128976193671}. Best is trial 6 with value: 46.46657380614095.\n",
      "[I 2024-07-23 18:44:05,421] Trial 18 finished with value: 46.91386190440935 and parameters: {'n_estimators': 229, 'max_depth': 9, 'learning_rate': 0.03777802163569984, 'subsample': 0.8383167274689636, 'colsample_bytree': 0.7092682889738531, 'gamma': 0.9964963003845149, 'reg_alpha': 0.010726143778364783, 'reg_lambda': 1.514385133028454}. Best is trial 6 with value: 46.46657380614095.\n",
      "[I 2024-07-23 18:44:44,488] Trial 19 finished with value: 47.101443514814704 and parameters: {'n_estimators': 574, 'max_depth': 6, 'learning_rate': 0.010073011075012586, 'subsample': 0.9183549108907435, 'colsample_bytree': 0.8059640086174569, 'gamma': 3.214848875703061, 'reg_alpha': 3.312585560786708, 'reg_lambda': 5.297166367039006}. Best is trial 6 with value: 46.46657380614095.\n",
      "[I 2024-07-23 18:45:16,615] Trial 20 finished with value: 46.789616763496674 and parameters: {'n_estimators': 420, 'max_depth': 8, 'learning_rate': 0.04741986762987982, 'subsample': 0.511551034036789, 'colsample_bytree': 0.6335191572543755, 'gamma': 2.701803868142006, 'reg_alpha': 1.068308136157654, 'reg_lambda': 9.666318820026795}. Best is trial 6 with value: 46.46657380614095.\n",
      "[I 2024-07-23 18:46:12,857] Trial 21 finished with value: 46.66565933111623 and parameters: {'n_estimators': 705, 'max_depth': 9, 'learning_rate': 0.04265556250772318, 'subsample': 0.9063867078984282, 'colsample_bytree': 0.5783180592583717, 'gamma': 0.10723662157229041, 'reg_alpha': 1.1736532373757527, 'reg_lambda': 8.193022294035721}. Best is trial 6 with value: 46.46657380614095.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 18:47:09,903] Trial 22 finished with value: 46.89052675857243 and parameters: {'n_estimators': 703, 'max_depth': 9, 'learning_rate': 0.038347950905006825, 'subsample': 0.9968784909519597, 'colsample_bytree': 0.5580671748566728, 'gamma': 0.043306944382758644, 'reg_alpha': 2.2914183193171542, 'reg_lambda': 8.388767024456564}. Best is trial 6 with value: 46.46657380614095.\n",
      "[I 2024-07-23 18:47:46,587] Trial 23 finished with value: 47.74759391075078 and parameters: {'n_estimators': 542, 'max_depth': 7, 'learning_rate': 0.09068226109040536, 'subsample': 0.931235321912006, 'colsample_bytree': 0.6324399459548242, 'gamma': 2.0431806011579523, 'reg_alpha': 0.8105319291757076, 'reg_lambda': 6.200604342213375}. Best is trial 6 with value: 46.46657380614095.\n",
      "[I 2024-07-23 18:48:39,354] Trial 24 finished with value: 47.44632247498498 and parameters: {'n_estimators': 706, 'max_depth': 9, 'learning_rate': 0.061723734456737574, 'subsample': 0.8336095473145134, 'colsample_bytree': 0.5185665302429544, 'gamma': 0.8638978419731982, 'reg_alpha': 2.409574968000334, 'reg_lambda': 7.505118605942988}. Best is trial 6 with value: 46.46657380614095.\n",
      "[I 2024-07-23 18:49:43,594] Trial 25 finished with value: 46.20102191457861 and parameters: {'n_estimators': 866, 'max_depth': 8, 'learning_rate': 0.027655927275208225, 'subsample': 0.7439122417230587, 'colsample_bytree': 0.5741833610858744, 'gamma': 0.44515502393717776, 'reg_alpha': 4.822242176855599, 'reg_lambda': 8.910499569117624}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 18:50:46,594] Trial 26 finished with value: 48.29605635553936 and parameters: {'n_estimators': 870, 'max_depth': 8, 'learning_rate': 0.11730616702406255, 'subsample': 0.754624964359438, 'colsample_bytree': 0.503449428360832, 'gamma': 1.1215364798463185, 'reg_alpha': 4.745200512928186, 'reg_lambda': 9.267088691426686}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 18:51:57,826] Trial 27 finished with value: 46.2169047161938 and parameters: {'n_estimators': 995, 'max_depth': 7, 'learning_rate': 0.023309584181104993, 'subsample': 0.5753531943018569, 'colsample_bytree': 0.6517300427295493, 'gamma': 3.4536990296675585, 'reg_alpha': 8.701971149370724, 'reg_lambda': 4.99294045397686}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 18:53:00,813] Trial 28 finished with value: 47.30192320837723 and parameters: {'n_estimators': 986, 'max_depth': 6, 'learning_rate': 0.07712162951572846, 'subsample': 0.5828511636793239, 'colsample_bytree': 0.6543187924508211, 'gamma': 3.5545582535210842, 'reg_alpha': 8.773785538293495, 'reg_lambda': 4.809367996168101}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 18:53:56,205] Trial 29 finished with value: 49.096372523798244 and parameters: {'n_estimators': 838, 'max_depth': 7, 'learning_rate': 0.13628336174455133, 'subsample': 0.6339948298609386, 'colsample_bytree': 0.7078372801339624, 'gamma': 4.223192920806209, 'reg_alpha': 8.726541640812693, 'reg_lambda': 1.8166991285089438}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 18:54:55,923] Trial 30 finished with value: 50.47463397196777 and parameters: {'n_estimators': 1000, 'max_depth': 5, 'learning_rate': 0.1735747749967762, 'subsample': 0.5608869777600403, 'colsample_bytree': 0.7809628367590229, 'gamma': 3.3683908829359366, 'reg_alpha': 6.446165007494521, 'reg_lambda': 2.7068254044230704}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 18:56:04,749] Trial 31 finished with value: 47.033131926233295 and parameters: {'n_estimators': 931, 'max_depth': 8, 'learning_rate': 0.026083029706803002, 'subsample': 0.6635134140536305, 'colsample_bytree': 0.6296158102193248, 'gamma': 2.8143338967702975, 'reg_alpha': 4.645397238540314, 'reg_lambda': 0.0958865872741761}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 18:56:24,503] Trial 32 finished with value: 46.525121576370225 and parameters: {'n_estimators': 265, 'max_depth': 7, 'learning_rate': 0.025455939841574182, 'subsample': 0.5338445865763002, 'colsample_bytree': 0.569679571046519, 'gamma': 4.980485368060971, 'reg_alpha': 8.263472722696292, 'reg_lambda': 5.338278452704529}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 18:56:42,024] Trial 33 finished with value: 46.5603899537531 and parameters: {'n_estimators': 214, 'max_depth': 7, 'learning_rate': 0.05551731895521545, 'subsample': 0.5532877585642262, 'colsample_bytree': 0.5621116529115863, 'gamma': 4.956613236126706, 'reg_alpha': 8.42550102277256, 'reg_lambda': 5.289373504593585}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 18:56:51,478] Trial 34 finished with value: 47.8705621679 and parameters: {'n_estimators': 124, 'max_depth': 6, 'learning_rate': 0.026739238236904477, 'subsample': 0.5243704046129286, 'colsample_bytree': 0.6649179551579867, 'gamma': 3.5508065357179857, 'reg_alpha': 8.048749213954226, 'reg_lambda': 4.443348119332089}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 18:57:12,245] Trial 35 finished with value: 47.79401458800214 and parameters: {'n_estimators': 289, 'max_depth': 7, 'learning_rate': 0.09965847425258045, 'subsample': 0.6068079714797383, 'colsample_bytree': 0.9320454108612726, 'gamma': 4.507480304736792, 'reg_alpha': 9.361039724428704, 'reg_lambda': 5.798835251118029}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 18:57:34,795] Trial 36 finished with value: 48.18298499468482 and parameters: {'n_estimators': 329, 'max_depth': 6, 'learning_rate': 0.13466847920117547, 'subsample': 0.5347228877680064, 'colsample_bytree': 0.5416776313049388, 'gamma': 0.5276785733716944, 'reg_alpha': 6.749741676885821, 'reg_lambda': 3.3152300448541516}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 18:58:05,333] Trial 37 finished with value: 46.2407183094803 and parameters: {'n_estimators': 418, 'max_depth': 7, 'learning_rate': 0.031476536227706464, 'subsample': 0.5955654359465729, 'colsample_bytree': 0.7094843216261887, 'gamma': 3.8515110987279098, 'reg_alpha': 5.702755494423116, 'reg_lambda': 2.4639760296247606}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 18:58:37,934] Trial 38 finished with value: 47.63472203782348 and parameters: {'n_estimators': 419, 'max_depth': 8, 'learning_rate': 0.07022475816024878, 'subsample': 0.5929159901191933, 'colsample_bytree': 0.7136776653896229, 'gamma': 3.869520491748735, 'reg_alpha': 5.488847760338608, 'reg_lambda': 2.3486636209314655}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 18:59:32,725] Trial 39 finished with value: 53.20428541370819 and parameters: {'n_estimators': 934, 'max_depth': 5, 'learning_rate': 0.28899240715952257, 'subsample': 0.701150675068219, 'colsample_bytree': 0.7764421403047168, 'gamma': 3.275410555532895, 'reg_alpha': 5.553443816439101, 'reg_lambda': 0.4019029438213879}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 19:00:26,548] Trial 40 finished with value: 46.974982423339505 and parameters: {'n_estimators': 814, 'max_depth': 7, 'learning_rate': 0.0518166448599376, 'subsample': 0.7368457952531415, 'colsample_bytree': 0.6840057686349368, 'gamma': 3.0871636797650885, 'reg_alpha': 7.429045546009037, 'reg_lambda': 3.9931989128990812}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 19:01:00,589] Trial 41 finished with value: 46.240316822382894 and parameters: {'n_estimators': 471, 'max_depth': 7, 'learning_rate': 0.03286211844631693, 'subsample': 0.6115594007075927, 'colsample_bytree': 0.6565270620707767, 'gamma': 4.168251674646125, 'reg_alpha': 9.95723320715801, 'reg_lambda': 0.7938320752108821}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 19:01:37,155] Trial 42 finished with value: 46.78008789439197 and parameters: {'n_estimators': 469, 'max_depth': 8, 'learning_rate': 0.010019473088140236, 'subsample': 0.6641761974959044, 'colsample_bytree': 0.6475299484471688, 'gamma': 4.115015679388222, 'reg_alpha': 9.87247318222265, 'reg_lambda': 0.8198071136381933}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 19:02:03,609] Trial 43 finished with value: 50.74086559185027 and parameters: {'n_estimators': 377, 'max_depth': 7, 'learning_rate': 0.21646770089088838, 'subsample': 0.6057515697852555, 'colsample_bytree': 0.7207842457632501, 'gamma': 3.8266622286690604, 'reg_alpha': 9.229864151046382, 'reg_lambda': 2.1984324737719954}. Best is trial 25 with value: 46.20102191457861.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 19:02:48,653] Trial 44 finished with value: 46.90262279317105 and parameters: {'n_estimators': 620, 'max_depth': 8, 'learning_rate': 0.03270038541568181, 'subsample': 0.5676038489800301, 'colsample_bytree': 0.6191337740477546, 'gamma': 4.420593991042029, 'reg_alpha': 7.917916076559697, 'reg_lambda': 1.0228029781918573}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 19:03:18,735] Trial 45 finished with value: 46.58182419979039 and parameters: {'n_estimators': 463, 'max_depth': 6, 'learning_rate': 0.05128964767463657, 'subsample': 0.6674447421370531, 'colsample_bytree': 0.6879235896085519, 'gamma': 3.6469347779728714, 'reg_alpha': 9.981375241422889, 'reg_lambda': 1.611863327755087}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 19:03:58,486] Trial 46 finished with value: 46.65085348927591 and parameters: {'n_estimators': 534, 'max_depth': 8, 'learning_rate': 0.02402893441048325, 'subsample': 0.6208715136420702, 'colsample_bytree': 0.7522232328364097, 'gamma': 3.94045550898156, 'reg_alpha': 6.889410879770164, 'reg_lambda': 3.5998634386773083}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 19:04:56,933] Trial 47 finished with value: 47.580871794834195 and parameters: {'n_estimators': 895, 'max_depth': 7, 'learning_rate': 0.0763976108836446, 'subsample': 0.5784366433041426, 'colsample_bytree': 0.6635302476406959, 'gamma': 4.316895663561649, 'reg_alpha': 3.022419753660413, 'reg_lambda': 4.39150379722214}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 19:05:32,110] Trial 48 finished with value: 46.5521128169835 and parameters: {'n_estimators': 584, 'max_depth': 4, 'learning_rate': 0.035706827715491246, 'subsample': 0.5456313843538255, 'colsample_bytree': 0.5831835620018647, 'gamma': 3.0367356457730583, 'reg_alpha': 3.8176981819135136, 'reg_lambda': 7.456921524010803}. Best is trial 25 with value: 46.20102191457861.\n",
      "[I 2024-07-23 19:05:58,691] Trial 49 finished with value: 46.49159580923086 and parameters: {'n_estimators': 390, 'max_depth': 6, 'learning_rate': 0.019769090854651783, 'subsample': 0.5037611410844269, 'colsample_bytree': 0.6158256469076764, 'gamma': 4.675271476656308, 'reg_alpha': 6.200750341457021, 'reg_lambda': 2.8009142536650176}. Best is trial 25 with value: 46.20102191457861.\n",
      "Best hyperparameters:  {'n_estimators': 866, 'max_depth': 8, 'learning_rate': 0.027655927275208225, 'subsample': 0.7439122417230587, 'colsample_bytree': 0.5741833610858744, 'gamma': 0.44515502393717776, 'reg_alpha': 4.822242176855599, 'reg_lambda': 8.910499569117624}\n",
      "Performance with tuned hyperparameters:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  14.469068995830604\n",
      "Mean Squared Error (MSE):  588.0934492755139\n",
      "R-squared (R2):  0.9564033698344048\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  46.20102191457861\n",
      "Mean Squared Error (MSE):  9769.60116570237\n",
      "R-squared (R2):  0.4792205794146658\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  60.430028444474026\n",
      "Mean Squared Error (MSE):  20464.237608659023\n",
      "R-squared (R2):  0.33989476572324295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train, y_train = prepare_data(train_data_paris, 'price')\n",
    "X_valid, y_valid = prepare_data(valid_data_paris, 'price')\n",
    "X_test, y_test = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Scale features \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  \n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10)\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        eval_metric='mae',  # Use mae for evaluation\n",
    "        **param\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train, eval_set=[(X_valid_scaled, y_valid)], verbose=False)\n",
    "    preds = model.predict(X_valid_scaled)\n",
    "    mae = mean_absolute_error(y_valid, preds)\n",
    "    return mae\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    eval_metric='mae',  # Use mae for evaluation\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_best = best_model.predict(X_train_scaled)\n",
    "y_valid_pred_best = best_model.predict(X_valid_scaled)\n",
    "y_test_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with tuned hyperparameters:\")\n",
    "print_metrics(y_train, y_train_pred_best, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred_best, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred_best, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad0b6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'xgb_model.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('xgb_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2129ab7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance with loaded model:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  14.469068995830604\n",
      "Mean Squared Error (MSE):  588.0934492755139\n",
      "R-squared (R2):  0.9564033698344048\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  46.20102191457861\n",
      "Mean Squared Error (MSE):  9769.60116570237\n",
      "R-squared (R2):  0.4792205794146658\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  60.430028444474026\n",
      "Mean Squared Error (MSE):  20464.237608659023\n",
      "R-squared (R2):  0.33989476572324295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('xgb_model.joblib')\n",
    "\n",
    "# Assuming you have your data prepared and scaled (X_train_scaled, X_valid_scaled, X_test_scaled)\n",
    "# You can reuse the scaled data from your previous steps or prepare and scale new data similarly\n",
    "\n",
    "# Predict on the training, validation, and test sets with the loaded model\n",
    "y_train_pred_best = loaded_model.predict(X_train_scaled)\n",
    "y_valid_pred_best = loaded_model.predict(X_valid_scaled)\n",
    "y_test_pred_best = loaded_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with loaded model:\")\n",
    "print_metrics(y_train, y_train_pred_best, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred_best, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred_best, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1948251",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf7fb4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Model: 100%|██████████ [time left: 00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  1.3448344266940968\n",
      "Mean Squared Error (MSE):  40.56486113722242\n",
      "R-squared (R2):  0.9969928397418865\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  54.194796687379295\n",
      "Mean Squared Error (MSE):  13402.559869786506\n",
      "R-squared (R2):  0.2855616882446329\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  67.61733680059899\n",
      "Mean Squared Error (MSE):  22070.758846680106\n",
      "R-squared (R2):  0.2880739699294247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train, y_train = prepare_data(train_data_paris, 'price')\n",
    "X_valid, y_valid = prepare_data(valid_data_paris, 'price')\n",
    "X_test, y_test = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the Random Forest Regression model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Wrap the fitting process with tqdm for visualization\n",
    "with tqdm(total=1, desc=\"Fitting Model\", bar_format=\"{l_bar}{bar} [time left: {remaining}]\") as pbar:\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    pbar.update()  # Simulate completion\n",
    "\n",
    "# Predict on the training, validation, and test sets\n",
    "y_train_pred = rf.predict(X_train_scaled)\n",
    "y_valid_pred = rf.predict(X_valid_scaled)\n",
    "y_test_pred = rf.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print_metrics(y_train, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce82fca7",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3fc0c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 19:19:25,897] A new study created in memory with name: no-name-135c154f-36a8-4f43-82e7-f054f32b14e2\n",
      "[I 2024-07-23 19:20:02,366] Trial 7 finished with value: 58.762743116510826 and parameters: {'n_estimators': 133, 'max_depth': 4, 'min_samples_split': 12, 'min_samples_leaf': 15, 'max_features': 'log2'}. Best is trial 7 with value: 58.762743116510826.\n",
      "[I 2024-07-23 19:20:36,224] Trial 2 finished with value: 58.59508507136756 and parameters: {'n_estimators': 274, 'max_depth': 4, 'min_samples_split': 5, 'min_samples_leaf': 12, 'max_features': 'log2'}. Best is trial 2 with value: 58.59508507136756.\n",
      "[I 2024-07-23 19:21:33,688] Trial 0 finished with value: 49.73924138223839 and parameters: {'n_estimators': 190, 'max_depth': 13, 'min_samples_split': 6, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 0 with value: 49.73924138223839.\n",
      "[I 2024-07-23 19:21:37,323] Trial 9 finished with value: 53.44715956921291 and parameters: {'n_estimators': 292, 'max_depth': 6, 'min_samples_split': 5, 'min_samples_leaf': 14, 'max_features': 'sqrt'}. Best is trial 0 with value: 49.73924138223839.\n",
      "[I 2024-07-23 19:23:19,149] Trial 8 finished with value: 48.83362527221748 and parameters: {'n_estimators': 326, 'max_depth': 12, 'min_samples_split': 8, 'min_samples_leaf': 11, 'max_features': 'sqrt'}. Best is trial 8 with value: 48.83362527221748.\n",
      "[I 2024-07-23 19:29:49,960] Trial 4 finished with value: 53.8203903575173 and parameters: {'n_estimators': 166, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 12, 'max_features': None}. Best is trial 8 with value: 48.83362527221748.\n",
      "[I 2024-07-23 19:31:09,752] Trial 1 finished with value: 53.79468144625232 and parameters: {'n_estimators': 171, 'max_depth': 8, 'min_samples_split': 15, 'min_samples_leaf': 8, 'max_features': None}. Best is trial 8 with value: 48.83362527221748.\n",
      "[I 2024-07-23 19:42:17,484] Trial 6 finished with value: 53.54366055451247 and parameters: {'n_estimators': 318, 'max_depth': 10, 'min_samples_split': 11, 'min_samples_leaf': 9, 'max_features': None}. Best is trial 8 with value: 48.83362527221748.\n",
      "[I 2024-07-23 19:43:53,011] Trial 3 finished with value: 53.596743888125175 and parameters: {'n_estimators': 386, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 8, 'max_features': None}. Best is trial 8 with value: 48.83362527221748.\n",
      "[I 2024-07-23 19:47:27,216] Trial 5 finished with value: 53.60245987385621 and parameters: {'n_estimators': 468, 'max_depth': 9, 'min_samples_split': 13, 'min_samples_leaf': 11, 'max_features': None}. Best is trial 8 with value: 48.83362527221748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'n_estimators': 326, 'max_depth': 12, 'min_samples_split': 8, 'min_samples_leaf': 11, 'max_features': 'sqrt'}\n",
      "Performance with tuned hyperparameters:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  29.34384301248031\n",
      "Mean Squared Error (MSE):  2721.830656507401\n",
      "R-squared (R2):  0.7978014817001471\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  48.8855135821614\n",
      "Mean Squared Error (MSE):  10807.264734416567\n",
      "R-squared (R2):  0.42508563854353076\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  62.72797489252845\n",
      "Mean Squared Error (MSE):  21686.80310798193\n",
      "R-squared (R2):  0.30419610493149274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Subsample the data (20%)\n",
    "def subsample_data(data, sample_size=0.2):\n",
    "    return train_test_split(data, test_size=sample_size, random_state=42)\n",
    "\n",
    "# Example subsampling, assuming train_data_reduced, valid_data_reduced, and test_data_reduced are loaded\n",
    "train_data_sub, _ = subsample_data(train_data_paris)\n",
    "valid_data_sub, _ = subsample_data(valid_data_paris)\n",
    "test_data_sub, _ = subsample_data(test_data_paris)\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train, y_train = prepare_data(train_data_sub, 'price')\n",
    "X_valid, y_valid = prepare_data(valid_data_sub, 'price')\n",
    "X_test, y_test = prepare_data(test_data_sub, 'price')\n",
    "\n",
    "#Scale features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 5, 15),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5, 15),\n",
    "        'max_features': trial.suggest_categorical('max_features', [None, 'sqrt', 'log2'])\n",
    "    }\n",
    "\n",
    "    model = RandomForestRegressor(**param)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    preds = model.predict(X_valid_scaled)\n",
    "    mae = mean_absolute_error(y_valid, preds)\n",
    "    return mae\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "\n",
    "# Optimize using parallelization with Joblib\n",
    "study.optimize(objective, n_trials=10, n_jobs=-1)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = RandomForestRegressor(**best_params)\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_best = best_model.predict(X_train_scaled)\n",
    "y_valid_pred_best = best_model.predict(X_valid_scaled)\n",
    "y_test_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with tuned hyperparameters:\")\n",
    "print_metrics(y_train, y_train_pred_best, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred_best, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred_best, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b68d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'rf_model.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('rf_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c13d8557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance with loaded model:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  29.34384301248031\n",
      "Mean Squared Error (MSE):  2721.830656507401\n",
      "R-squared (R2):  0.7978014817001471\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  48.8855135821614\n",
      "Mean Squared Error (MSE):  10807.264734416567\n",
      "R-squared (R2):  0.42508563854353076\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  62.72797489252845\n",
      "Mean Squared Error (MSE):  21686.80310798193\n",
      "R-squared (R2):  0.30419610493149274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('rf_model.joblib')\n",
    "\n",
    "# Assuming you have your data prepared and scaled (X_train_scaled, X_valid_scaled, X_test_scaled)\n",
    "# You can reuse the scaled data from your previous steps or prepare and scale new data similarly\n",
    "\n",
    "# Predict on the training, validation, and test sets with the loaded model\n",
    "y_train_pred_best = loaded_model.predict(X_train_scaled)\n",
    "y_valid_pred_best = loaded_model.predict(X_valid_scaled)\n",
    "y_test_pred_best = loaded_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with loaded model:\")\n",
    "print_metrics(y_train, y_train_pred_best, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred_best, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred_best, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b66a802",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0dfd00ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Model: 100%|██████████ [time left: 00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  45.969151474127045\n",
      "Mean Squared Error (MSE):  7354.173574057827\n",
      "R-squared (R2):  0.4548193218666722\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  52.86564501905403\n",
      "Mean Squared Error (MSE):  11499.017249851935\n",
      "R-squared (R2):  0.3870321378418191\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  66.9949030635178\n",
      "Mean Squared Error (MSE):  22269.753730269975\n",
      "R-squared (R2):  0.2816550860812239\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train, y_train = prepare_data(train_data_paris, 'price')\n",
    "X_valid, y_valid = prepare_data(valid_data_paris, 'price')\n",
    "X_test, y_test = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the Linear Regression model\n",
    "linear_reg = LinearRegression()\n",
    "\n",
    "# Wrap the fitting process with tqdm for visualization\n",
    "with tqdm(total=1, desc=\"Fitting Model\", bar_format=\"{l_bar}{bar} [time left: {remaining}]\") as pbar:\n",
    "    linear_reg.fit(X_train_scaled, y_train)\n",
    "    pbar.update()  # Simulate completion\n",
    "\n",
    "# Predict on the training, validation, and test sets\n",
    "y_train_pred = linear_reg.predict(X_train_scaled)\n",
    "y_valid_pred = linear_reg.predict(X_valid_scaled)\n",
    "y_test_pred = linear_reg.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print_metrics(y_train, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecde530f",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a581262e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 19:49:57,210] A new study created in memory with name: no-name-347b5052-5d0e-4325-9207-aa282dbaed4b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf8f31ac2984b5d9392b7f1743ffe3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 19:49:59,300] Trial 0 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:01,547] Trial 1 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:03,518] Trial 2 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:05,785] Trial 3 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:07,886] Trial 4 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:10,090] Trial 5 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:12,282] Trial 6 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:14,568] Trial 7 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:16,715] Trial 8 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:18,872] Trial 9 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:20,953] Trial 10 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:23,209] Trial 11 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:25,802] Trial 12 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:28,275] Trial 13 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:30,643] Trial 14 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:32,877] Trial 15 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:34,948] Trial 16 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:36,934] Trial 17 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:38,996] Trial 18 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:41,072] Trial 19 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:43,158] Trial 20 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:45,243] Trial 21 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:47,332] Trial 22 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:49,328] Trial 23 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:51,370] Trial 24 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:53,435] Trial 25 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:55,643] Trial 26 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:57,907] Trial 27 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:50:59,995] Trial 28 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:51:02,178] Trial 29 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:51:04,289] Trial 30 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:51:06,301] Trial 31 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:51:08,337] Trial 32 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:51:10,333] Trial 33 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:51:12,387] Trial 34 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:51:14,465] Trial 35 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:51:16,532] Trial 36 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:51:18,636] Trial 37 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:51:20,719] Trial 38 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:51:22,718] Trial 39 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:51:24,623] Trial 40 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:51:26,694] Trial 41 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:51:28,856] Trial 42 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:51:31,014] Trial 43 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:51:33,119] Trial 44 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:51:35,232] Trial 45 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:51:37,293] Trial 46 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:51:39,277] Trial 47 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:51:41,411] Trial 48 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "[I 2024-07-23 19:51:43,634] Trial 49 finished with value: 52.86564501905403 and parameters: {}. Best is trial 0 with value: 52.86564501905403.\n",
      "Performance with tuned hyperparameters:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  45.969151474127045\n",
      "Mean Squared Error (MSE):  7354.173574057827\n",
      "R-squared (R2):  0.4548193218666722\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  52.86564501905403\n",
      "Mean Squared Error (MSE):  11499.017249851935\n",
      "R-squared (R2):  0.3870321378418191\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  66.9949030635178\n",
      "Mean Squared Error (MSE):  22269.753730269975\n",
      "R-squared (R2):  0.2816550860812239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train, y_train = prepare_data(train_data_paris, 'price')\n",
    "X_valid, y_valid = prepare_data(valid_data_paris, 'price')\n",
    "X_test, y_test = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Scale features (handling NaN values should be done before scaling if necessary)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Example: Fill NaNs with 0, can use other strategies\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    param = {}  # No hyperparameters to tune for Linear Regression\n",
    "\n",
    "    model = LinearRegression(**param)\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    preds = model.predict(X_valid_scaled)\n",
    "    mae = mean_absolute_error(y_valid, preds)\n",
    "    return mae\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Since there are no hyperparameters to tune, we use a default Linear Regression model\n",
    "best_model = LinearRegression()\n",
    "\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_best = best_model.predict(X_train_scaled)\n",
    "y_valid_pred_best = best_model.predict(X_valid_scaled)\n",
    "y_test_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with tuned hyperparameters:\")\n",
    "print_metrics(y_train, y_train_pred_best, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred_best, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred_best, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2dfcdc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'linear_model.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('linear_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed03bee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance with loaded model:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  45.969151474127045\n",
      "Mean Squared Error (MSE):  7354.173574057827\n",
      "R-squared (R2):  0.4548193218666722\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  52.86564501905403\n",
      "Mean Squared Error (MSE):  11499.017249851935\n",
      "R-squared (R2):  0.3870321378418191\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  66.9949030635178\n",
      "Mean Squared Error (MSE):  22269.753730269975\n",
      "R-squared (R2):  0.2816550860812239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('linear_model.joblib')\n",
    "\n",
    "# Assuming you have your data prepared and scaled (X_train_scaled, X_valid_scaled, X_test_scaled)\n",
    "# You can reuse the scaled data from your previous steps or prepare and scale new data similarly\n",
    "\n",
    "# Predict on the training, validation, and test sets with the loaded model\n",
    "y_train_pred_best = loaded_model.predict(X_train_scaled)\n",
    "y_valid_pred_best = loaded_model.predict(X_valid_scaled)\n",
    "y_test_pred_best = loaded_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with loaded model:\")\n",
    "print_metrics(y_train, y_train_pred_best, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred_best, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred_best, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034e777f",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3e7036c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Model: 100%|██████████ [time left: 00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  45.97135293720669\n",
      "Mean Squared Error (MSE):  7354.081628911417\n",
      "R-squared (R2):  0.4548261379578027\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  52.907759980516666\n",
      "Mean Squared Error (MSE):  11505.87962258329\n",
      "R-squared (R2):  0.38666633145584073\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  66.96124552864107\n",
      "Mean Squared Error (MSE):  22260.873595957495\n",
      "R-squared (R2):  0.2819415283740102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train, y_train = prepare_data(train_data_paris, 'price')\n",
    "X_valid, y_valid = prepare_data(valid_data_paris, 'price')\n",
    "X_test, y_test = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the Ridge Regression model\n",
    "ridge = Ridge()\n",
    "\n",
    "# Wrap the fitting process with tqdm for visualization\n",
    "with tqdm(total=1, desc=\"Fitting Model\", bar_format=\"{l_bar}{bar} [time left: {remaining}]\") as pbar:\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "    pbar.update()  # Simulate completion\n",
    "\n",
    "# Predict on the training, validation, and test sets\n",
    "y_train_pred = ridge.predict(X_train_scaled)\n",
    "y_valid_pred = ridge.predict(X_valid_scaled)\n",
    "y_test_pred = ridge.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print_metrics(y_train, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377df321",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "732031c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 19:51:51,200] A new study created in memory with name: no-name-fe1eaf02-76bb-476d-a4d5-477ab8498bd7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3ed9d6806542d19b933854f44635ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 19:51:51,662] Trial 0 finished with value: 52.907438698540986 and parameters: {'alpha': 1.6744154908236122}. Best is trial 0 with value: 52.907438698540986.\n",
      "[I 2024-07-23 19:51:52,123] Trial 1 finished with value: 52.908129447754916 and parameters: {'alpha': 0.2249803670838502}. Best is trial 0 with value: 52.907438698540986.\n",
      "[I 2024-07-23 19:51:52,609] Trial 2 finished with value: 52.90733770075788 and parameters: {'alpha': 1.8866307418489154}. Best is trial 2 with value: 52.90733770075788.\n",
      "[I 2024-07-23 19:51:53,086] Trial 3 finished with value: 52.908115302043235 and parameters: {'alpha': 0.2546376878714847}. Best is trial 2 with value: 52.90733770075788.\n",
      "[I 2024-07-23 19:51:53,581] Trial 4 finished with value: 52.907902398329554 and parameters: {'alpha': 0.701174476669867}. Best is trial 2 with value: 52.90733770075788.\n",
      "[I 2024-07-23 19:51:54,072] Trial 5 finished with value: 52.907946502168336 and parameters: {'alpha': 0.6086558775796835}. Best is trial 2 with value: 52.90733770075788.\n",
      "[I 2024-07-23 19:51:54,546] Trial 6 finished with value: 52.905924155692496 and parameters: {'alpha': 4.861784985166936}. Best is trial 6 with value: 52.905924155692496.\n",
      "[I 2024-07-23 19:51:55,035] Trial 7 finished with value: 52.9081608174708 and parameters: {'alpha': 0.15921431783537338}. Best is trial 6 with value: 52.905924155692496.\n",
      "[I 2024-07-23 19:51:55,514] Trial 8 finished with value: 52.90792072252367 and parameters: {'alpha': 0.6627341942882958}. Best is trial 6 with value: 52.905924155692496.\n",
      "[I 2024-07-23 19:51:55,984] Trial 9 finished with value: 52.90792637337463 and parameters: {'alpha': 0.6508801337990533}. Best is trial 6 with value: 52.905924155692496.\n",
      "[I 2024-07-23 19:51:56,466] Trial 10 finished with value: 52.904738529437516 and parameters: {'alpha': 7.364567278874314}. Best is trial 10 with value: 52.904738529437516.\n",
      "[I 2024-07-23 19:51:56,960] Trial 11 finished with value: 52.904403930103996 and parameters: {'alpha': 8.072332768692442}. Best is trial 11 with value: 52.904403930103996.\n",
      "[I 2024-07-23 19:51:57,481] Trial 12 finished with value: 52.90391035303073 and parameters: {'alpha': 9.117406139418309}. Best is trial 12 with value: 52.90391035303073.\n",
      "[I 2024-07-23 19:51:58,020] Trial 13 finished with value: 52.90627998649195 and parameters: {'alpha': 4.112058637951946}. Best is trial 12 with value: 52.90391035303073.\n",
      "[I 2024-07-23 19:51:58,571] Trial 14 finished with value: 52.90357669160628 and parameters: {'alpha': 9.825090505115442}. Best is trial 14 with value: 52.90357669160628.\n",
      "[I 2024-07-23 19:51:59,117] Trial 15 finished with value: 52.906812333131384 and parameters: {'alpha': 2.9912461283884704}. Best is trial 14 with value: 52.90357669160628.\n",
      "[I 2024-07-23 19:51:59,657] Trial 16 finished with value: 52.90378162051671 and parameters: {'alpha': 9.390398463075801}. Best is trial 14 with value: 52.90357669160628.\n",
      "[I 2024-07-23 19:52:00,220] Trial 17 finished with value: 52.9070797416833 and parameters: {'alpha': 2.428807800712915}. Best is trial 14 with value: 52.90357669160628.\n",
      "[I 2024-07-23 19:52:00,747] Trial 18 finished with value: 52.90581231241735 and parameters: {'alpha': 5.097526311416253}. Best is trial 14 with value: 52.90357669160628.\n",
      "[I 2024-07-23 19:52:01,276] Trial 19 finished with value: 52.90761492442002 and parameters: {'alpha': 1.304432179941995}. Best is trial 14 with value: 52.90357669160628.\n",
      "[I 2024-07-23 19:52:01,877] Trial 20 finished with value: 52.90351335699558 and parameters: {'alpha': 9.95947091893419}. Best is trial 20 with value: 52.90351335699558.\n",
      "[I 2024-07-23 19:52:02,457] Trial 21 finished with value: 52.90520301077915 and parameters: {'alpha': 6.3830903507628385}. Best is trial 20 with value: 52.90351335699558.\n",
      "[I 2024-07-23 19:52:03,028] Trial 22 finished with value: 52.903710487367405 and parameters: {'alpha': 9.541268804502113}. Best is trial 20 with value: 52.90351335699558.\n",
      "[I 2024-07-23 19:52:03,608] Trial 23 finished with value: 52.90609918484761 and parameters: {'alpha': 4.492946670255822}. Best is trial 20 with value: 52.90351335699558.\n",
      "[I 2024-07-23 19:52:04,307] Trial 24 finished with value: 52.90672285720733 and parameters: {'alpha': 3.1795541029955987}. Best is trial 20 with value: 52.90351335699558.\n",
      "[I 2024-07-23 19:52:04,856] Trial 25 finished with value: 52.903617797555896 and parameters: {'alpha': 9.737885738721396}. Best is trial 20 with value: 52.90351335699558.\n",
      "[I 2024-07-23 19:52:05,348] Trial 26 finished with value: 52.90522896931887 and parameters: {'alpha': 6.32829027025873}. Best is trial 20 with value: 52.90351335699558.\n",
      "[I 2024-07-23 19:52:05,855] Trial 27 finished with value: 52.906633712701066 and parameters: {'alpha': 3.3672025144394104}. Best is trial 20 with value: 52.90351335699558.\n",
      "[I 2024-07-23 19:52:06,434] Trial 28 finished with value: 52.90549178751601 and parameters: {'alpha': 5.773593663836449}. Best is trial 20 with value: 52.90351335699558.\n",
      "[I 2024-07-23 19:52:07,042] Trial 29 finished with value: 52.90732729576127 and parameters: {'alpha': 1.9084955498997336}. Best is trial 20 with value: 52.90351335699558.\n",
      "[I 2024-07-23 19:52:07,553] Trial 30 finished with value: 52.9077011674211 and parameters: {'alpha': 1.123423658157783}. Best is trial 20 with value: 52.90351335699558.\n",
      "[I 2024-07-23 19:52:08,141] Trial 31 finished with value: 52.90350912632423 and parameters: {'alpha': 9.968448263354981}. Best is trial 31 with value: 52.90350912632423.\n",
      "[I 2024-07-23 19:52:08,707] Trial 32 finished with value: 52.904887022098485 and parameters: {'alpha': 7.050588221577722}. Best is trial 31 with value: 52.90350912632423.\n",
      "[I 2024-07-23 19:52:09,289] Trial 33 finished with value: 52.9047100602734 and parameters: {'alpha': 7.424772226137694}. Best is trial 31 with value: 52.90350912632423.\n",
      "[I 2024-07-23 19:52:09,862] Trial 34 finished with value: 52.90805864056308 and parameters: {'alpha': 0.373447782534759}. Best is trial 31 with value: 52.90350912632423.\n",
      "[I 2024-07-23 19:52:10,390] Trial 35 finished with value: 52.90644344048639 and parameters: {'alpha': 3.7678139860349438}. Best is trial 31 with value: 52.90350912632423.\n",
      "[I 2024-07-23 19:52:10,875] Trial 36 finished with value: 52.90360803492159 and parameters: {'alpha': 9.758596289048747}. Best is trial 31 with value: 52.90350912632423.\n",
      "[I 2024-07-23 19:52:11,368] Trial 37 finished with value: 52.907064235203386 and parameters: {'alpha': 2.4614064191950598}. Best is trial 31 with value: 52.90350912632423.\n",
      "[I 2024-07-23 19:52:11,857] Trial 38 finished with value: 52.90568518498452 and parameters: {'alpha': 5.36556424396825}. Best is trial 31 with value: 52.90350912632423.\n",
      "[I 2024-07-23 19:52:12,334] Trial 39 finished with value: 52.90537112314893 and parameters: {'alpha': 6.028235324688718}. Best is trial 31 with value: 52.90350912632423.\n",
      "[I 2024-07-23 19:52:12,817] Trial 40 finished with value: 52.90806541172385 and parameters: {'alpha': 0.3592467669970353}. Best is trial 31 with value: 52.90350912632423.\n",
      "[I 2024-07-23 19:52:13,296] Trial 41 finished with value: 52.903649254143104 and parameters: {'alpha': 9.67115562581867}. Best is trial 31 with value: 52.90350912632423.\n",
      "[I 2024-07-23 19:52:13,775] Trial 42 finished with value: 52.90440317193621 and parameters: {'alpha': 8.073936924927754}. Best is trial 31 with value: 52.90350912632423.\n",
      "[I 2024-07-23 19:52:14,253] Trial 43 finished with value: 52.90365174404166 and parameters: {'alpha': 9.665873848627507}. Best is trial 31 with value: 52.90350912632423.\n",
      "[I 2024-07-23 19:52:14,729] Trial 44 finished with value: 52.90818373416529 and parameters: {'alpha': 0.11117198101705016}. Best is trial 31 with value: 52.90350912632423.\n",
      "[I 2024-07-23 19:52:15,226] Trial 45 finished with value: 52.90467828391057 and parameters: {'alpha': 7.491974300578015}. Best is trial 31 with value: 52.90350912632423.\n",
      "[I 2024-07-23 19:52:15,749] Trial 46 finished with value: 52.90608087743612 and parameters: {'alpha': 4.531520949423554}. Best is trial 31 with value: 52.90350912632423.\n",
      "[I 2024-07-23 19:52:16,262] Trial 47 finished with value: 52.90449082753741 and parameters: {'alpha': 7.88848528216561}. Best is trial 31 with value: 52.90350912632423.\n",
      "[I 2024-07-23 19:52:16,778] Trial 48 finished with value: 52.90584227326987 and parameters: {'alpha': 5.034371184370843}. Best is trial 31 with value: 52.90350912632423.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 19:52:17,294] Trial 49 finished with value: 52.90522053262318 and parameters: {'alpha': 6.346100404938367}. Best is trial 31 with value: 52.90350912632423.\n",
      "Best hyperparameters:  {'alpha': 9.968448263354981}\n",
      "Performance with tuned hyperparameters:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  45.970474159433955\n",
      "Mean Squared Error (MSE):  7354.082344272964\n",
      "R-squared (R2):  0.4548260849265151\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  52.90350912632423\n",
      "Mean Squared Error (MSE):  11504.923167391527\n",
      "R-squared (R2):  0.38671731636015094\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  66.95601796822811\n",
      "Mean Squared Error (MSE):  22258.724757280892\n",
      "R-squared (R2):  0.2820108424469392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train, y_train = prepare_data(train_data_paris, 'price')\n",
    "X_valid, y_valid = prepare_data(valid_data_paris, 'price')\n",
    "X_test, y_test = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Scale features (handling NaN values should be done before scaling if necessary)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Example: Fill NaNs with 0, can use other strategies\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'alpha': trial.suggest_float('alpha', 0.1, 10.0, log=True)\n",
    "    }\n",
    "\n",
    "    model = Ridge(**param)\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    preds = model.predict(X_valid_scaled)\n",
    "    mae = mean_absolute_error(y_valid, preds)\n",
    "    return mae\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = Ridge(**best_params)\n",
    "\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_best = best_model.predict(X_train_scaled)\n",
    "y_valid_pred_best = best_model.predict(X_valid_scaled)\n",
    "y_test_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with tuned hyperparameters:\")\n",
    "print_metrics(y_train, y_train_pred_best, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred_best, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred_best, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f45b6153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'ridge_model.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('ridge_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d562a61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance with loaded model:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  45.970474159433955\n",
      "Mean Squared Error (MSE):  7354.082344272964\n",
      "R-squared (R2):  0.4548260849265151\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  52.90350912632423\n",
      "Mean Squared Error (MSE):  11504.923167391527\n",
      "R-squared (R2):  0.38671731636015094\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  66.95601796822811\n",
      "Mean Squared Error (MSE):  22258.724757280892\n",
      "R-squared (R2):  0.2820108424469392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('ridge_model.joblib')\n",
    "\n",
    "# Assuming you have your data prepared and scaled (X_train_scaled, X_valid_scaled, X_test_scaled)\n",
    "# You can reuse the scaled data from your previous steps or prepare and scale new data similarly\n",
    "\n",
    "# Predict on the training, validation, and test sets with the loaded model\n",
    "y_train_pred_best = loaded_model.predict(X_train_scaled)\n",
    "y_valid_pred_best = loaded_model.predict(X_valid_scaled)\n",
    "y_test_pred_best = loaded_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with loaded model:\")\n",
    "print_metrics(y_train, y_train_pred_best, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred_best, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred_best, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517f8bdb",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13dcc3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Model: 100%|██████████ [time left: 00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  45.56571495132371\n",
      "Mean Squared Error (MSE):  7492.947317741675\n",
      "R-squared (R2):  0.44453172626850135\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  51.82987307949523\n",
      "Mean Squared Error (MSE):  11368.645783240596\n",
      "R-squared (R2):  0.39398173339757914\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  65.79030543590959\n",
      "Mean Squared Error (MSE):  21724.027870987124\n",
      "R-squared (R2):  0.2992583070309437\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train, y_train = prepare_data(train_data_paris, 'price')\n",
    "X_valid, y_valid = prepare_data(valid_data_paris, 'price')\n",
    "X_test, y_test = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the Lasso Regression model\n",
    "lasso = Lasso()\n",
    "\n",
    "# Wrap the fitting process with tqdm for visualization\n",
    "with tqdm(total=1, desc=\"Fitting Model\", bar_format=\"{l_bar}{bar} [time left: {remaining}]\") as pbar:\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "    pbar.update()  # Simulate completion\n",
    "\n",
    "# Predict on the training, validation, and test sets\n",
    "y_train_pred = lasso.predict(X_train_scaled)\n",
    "y_valid_pred = lasso.predict(X_valid_scaled)\n",
    "y_test_pred = lasso.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print_metrics(y_train, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d9410a",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b8380fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 19:52:23,500] A new study created in memory with name: no-name-2310e19a-2a9c-4aff-905c-e5d0c59f5df1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c270a4199b4c3a9ab500de6100d573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.176e+08, tolerance: 3.855e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 19:52:39,936] Trial 0 finished with value: 52.792940447892256 and parameters: {'alpha': 0.0032733965353772872}. Best is trial 0 with value: 52.792940447892256.\n",
      "[I 2024-07-23 19:52:52,101] Trial 1 finished with value: 52.20144590892572 and parameters: {'alpha': 0.08509400863197177}. Best is trial 1 with value: 52.20144590892572.\n",
      "[I 2024-07-23 19:53:17,087] Trial 2 finished with value: 52.2823901450153 and parameters: {'alpha': 0.0486960087009572}. Best is trial 1 with value: 52.20144590892572.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.455e+05, tolerance: 3.855e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 19:53:35,625] Trial 3 finished with value: 52.516383173289135 and parameters: {'alpha': 0.013308949105812283}. Best is trial 1 with value: 52.20144590892572.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.407e+08, tolerance: 3.855e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 19:53:58,032] Trial 4 finished with value: 52.888316354795336 and parameters: {'alpha': 0.0005466050132018707}. Best is trial 1 with value: 52.20144590892572.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.862e+08, tolerance: 3.855e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 19:54:14,848] Trial 5 finished with value: 52.82462209849576 and parameters: {'alpha': 0.0023517746187406547}. Best is trial 1 with value: 52.20144590892572.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.403e+08, tolerance: 3.855e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 19:54:38,248] Trial 6 finished with value: 52.89396271280466 and parameters: {'alpha': 0.0003910817331771402}. Best is trial 1 with value: 52.20144590892572.\n",
      "[I 2024-07-23 19:54:39,503] Trial 7 finished with value: 51.8220939972153 and parameters: {'alpha': 0.9699660166634534}. Best is trial 7 with value: 51.8220939972153.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.037e+08, tolerance: 3.855e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 19:54:57,893] Trial 8 finished with value: 52.7489173100724 and parameters: {'alpha': 0.0046082711109654015}. Best is trial 7 with value: 51.8220939972153.\n",
      "[I 2024-07-23 19:55:02,210] Trial 9 finished with value: 51.90356311206967 and parameters: {'alpha': 0.2946431310407348}. Best is trial 7 with value: 51.8220939972153.\n",
      "[I 2024-07-23 19:55:03,539] Trial 10 finished with value: 51.828269555917394 and parameters: {'alpha': 0.9935642687649991}. Best is trial 7 with value: 51.8220939972153.\n",
      "[I 2024-07-23 19:55:04,939] Trial 11 finished with value: 51.78907969243228 and parameters: {'alpha': 0.7792813465876249}. Best is trial 11 with value: 51.78907969243228.\n",
      "[I 2024-07-23 19:55:06,151] Trial 12 finished with value: 51.81533575981091 and parameters: {'alpha': 0.9434228891486743}. Best is trial 11 with value: 51.78907969243228.\n",
      "[I 2024-07-23 19:55:10,002] Trial 13 finished with value: 52.01964473559657 and parameters: {'alpha': 0.18916716582295431}. Best is trial 11 with value: 51.78907969243228.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.770e+05, tolerance: 3.855e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 19:55:36,455] Trial 14 finished with value: 52.35216023222825 and parameters: {'alpha': 0.026098661468312967}. Best is trial 11 with value: 51.78907969243228.\n",
      "[I 2024-07-23 19:55:39,628] Trial 15 finished with value: 51.95127833119191 and parameters: {'alpha': 0.2449895201794494}. Best is trial 11 with value: 51.78907969243228.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.373e+08, tolerance: 3.855e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 19:56:02,132] Trial 16 finished with value: 52.904165839658525 and parameters: {'alpha': 0.00011221055960795049}. Best is trial 11 with value: 51.78907969243228.\n",
      "[I 2024-07-23 19:56:05,360] Trial 17 finished with value: 51.81854146749653 and parameters: {'alpha': 0.47447750011046563}. Best is trial 11 with value: 51.78907969243228.\n",
      "[I 2024-07-23 19:56:17,164] Trial 18 finished with value: 52.193296768718 and parameters: {'alpha': 0.08925957977605549}. Best is trial 11 with value: 51.78907969243228.\n",
      "[I 2024-07-23 19:56:22,626] Trial 19 finished with value: 52.110854088579046 and parameters: {'alpha': 0.13373355536903786}. Best is trial 11 with value: 51.78907969243228.\n",
      "[I 2024-07-23 19:56:25,307] Trial 20 finished with value: 51.81816862577114 and parameters: {'alpha': 0.47585184473173475}. Best is trial 11 with value: 51.78907969243228.\n",
      "[I 2024-07-23 19:56:26,775] Trial 21 finished with value: 51.793286182088536 and parameters: {'alpha': 0.6058722936426661}. Best is trial 11 with value: 51.78907969243228.\n",
      "[I 2024-07-23 19:56:28,048] Trial 22 finished with value: 51.81859605089452 and parameters: {'alpha': 0.9566444245485984}. Best is trial 11 with value: 51.78907969243228.\n",
      "[I 2024-07-23 19:56:31,562] Trial 23 finished with value: 51.83700404348913 and parameters: {'alpha': 0.410111641680706}. Best is trial 11 with value: 51.78907969243228.\n",
      "[I 2024-07-23 19:56:57,243] Trial 24 finished with value: 52.28630458865759 and parameters: {'alpha': 0.04732416846350613}. Best is trial 11 with value: 51.78907969243228.\n",
      "[I 2024-07-23 19:56:59,891] Trial 25 finished with value: 51.81528441276817 and parameters: {'alpha': 0.4869729315359958}. Best is trial 11 with value: 51.78907969243228.\n",
      "[I 2024-07-23 19:57:05,304] Trial 26 finished with value: 52.08778626602599 and parameters: {'alpha': 0.1473062546654522}. Best is trial 11 with value: 51.78907969243228.\n",
      "[I 2024-07-23 19:57:08,459] Trial 27 finished with value: 51.82298788355941 and parameters: {'alpha': 0.45686595109909256}. Best is trial 11 with value: 51.78907969243228.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.579e+05, tolerance: 3.855e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 19:57:29,502] Trial 28 finished with value: 52.43089274630049 and parameters: {'alpha': 0.01806916655431118}. Best is trial 11 with value: 51.78907969243228.\n",
      "[I 2024-07-23 19:57:51,838] Trial 29 finished with value: 52.26739108151467 and parameters: {'alpha': 0.053687700963144}. Best is trial 11 with value: 51.78907969243228.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.050e+06, tolerance: 3.855e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 19:58:10,166] Trial 30 finished with value: 52.68940650505906 and parameters: {'alpha': 0.006507288333435396}. Best is trial 11 with value: 51.78907969243228.\n",
      "[I 2024-07-23 19:58:11,759] Trial 31 finished with value: 51.79029009326245 and parameters: {'alpha': 0.6268988020134583}. Best is trial 11 with value: 51.78907969243228.\n",
      "[I 2024-07-23 19:58:15,012] Trial 32 finished with value: 51.929515517195455 and parameters: {'alpha': 0.26624016947629026}. Best is trial 11 with value: 51.78907969243228.\n",
      "[I 2024-07-23 19:58:16,510] Trial 33 finished with value: 51.79417321560573 and parameters: {'alpha': 0.6008115488419686}. Best is trial 11 with value: 51.78907969243228.\n",
      "[I 2024-07-23 19:58:25,293] Trial 34 finished with value: 52.15915116791077 and parameters: {'alpha': 0.10810338718768632}. Best is trial 11 with value: 51.78907969243228.\n",
      "[I 2024-07-23 19:58:26,845] Trial 35 finished with value: 51.78540903706281 and parameters: {'alpha': 0.7093697243198016}. Best is trial 35 with value: 51.78540903706281.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.049e+08, tolerance: 3.855e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 19:58:43,418] Trial 36 finished with value: 52.84858980102216 and parameters: {'alpha': 0.0016599819557454082}. Best is trial 35 with value: 51.78540903706281.\n",
      "[I 2024-07-23 19:59:04,853] Trial 37 finished with value: 52.258377615536695 and parameters: {'alpha': 0.05699952560344621}. Best is trial 35 with value: 51.78540903706281.\n",
      "[I 2024-07-23 19:59:08,000] Trial 38 finished with value: 51.93198742265407 and parameters: {'alpha': 0.26371681265458}. Best is trial 35 with value: 51.78540903706281.\n",
      "[I 2024-07-23 19:59:12,302] Trial 39 finished with value: 52.05811569609128 and parameters: {'alpha': 0.16500724706755335}. Best is trial 35 with value: 51.78540903706281.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.851e+05, tolerance: 3.855e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 19:59:37,459] Trial 40 finished with value: 52.33919688910157 and parameters: {'alpha': 0.029723132255657352}. Best is trial 35 with value: 51.78540903706281.\n",
      "[I 2024-07-23 19:59:39,945] Trial 41 finished with value: 51.80855457851262 and parameters: {'alpha': 0.515364735476558}. Best is trial 35 with value: 51.78540903706281.\n",
      "[I 2024-07-23 19:59:41,406] Trial 42 finished with value: 51.795151197619205 and parameters: {'alpha': 0.595447796048545}. Best is trial 35 with value: 51.78540903706281.\n",
      "[I 2024-07-23 19:59:42,738] Trial 43 finished with value: 51.78540514848887 and parameters: {'alpha': 0.7096124891287513}. Best is trial 43 with value: 51.78540514848887.\n",
      "[I 2024-07-23 19:59:44,152] Trial 44 finished with value: 51.78704367230235 and parameters: {'alpha': 0.7514769141931554}. Best is trial 43 with value: 51.78540514848887.\n",
      "[I 2024-07-23 19:59:47,935] Trial 45 finished with value: 51.87263522584186 and parameters: {'alpha': 0.33897158920952025}. Best is trial 43 with value: 51.78540514848887.\n",
      "[I 2024-07-23 19:59:49,390] Trial 46 finished with value: 51.787953289055274 and parameters: {'alpha': 0.7619344404672358}. Best is trial 43 with value: 51.78540514848887.\n",
      "[I 2024-07-23 19:59:50,656] Trial 47 finished with value: 51.78987766988204 and parameters: {'alpha': 0.7905612565185696}. Best is trial 43 with value: 51.78540514848887.\n",
      "[I 2024-07-23 19:59:53,998] Trial 48 finished with value: 51.993921162004234 and parameters: {'alpha': 0.20821257045476607}. Best is trial 43 with value: 51.78540514848887.\n",
      "[I 2024-07-23 19:59:57,680] Trial 49 finished with value: 51.866435943277814 and parameters: {'alpha': 0.34874120912270706}. Best is trial 43 with value: 51.78540514848887.\n",
      "Best hyperparameters:  {'alpha': 0.7096124891287513}\n",
      "Performance with tuned hyperparameters:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  45.57831118911151\n",
      "Mean Squared Error (MSE):  7452.50383243236\n",
      "R-squared (R2):  0.4475298886758706\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  51.78540514848887\n",
      "Mean Squared Error (MSE):  11307.512271468107\n",
      "R-squared (R2):  0.39724052301439505\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  65.72235577733532\n",
      "Mean Squared Error (MSE):  21692.76687807047\n",
      "R-squared (R2):  0.30026667809502083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train, y_train = prepare_data(train_data_paris, 'price')\n",
    "X_valid, y_valid = prepare_data(valid_data_paris, 'price')\n",
    "X_test, y_test = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Scale features (handling NaN values should be done before scaling if necessary)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Example: Fill NaNs with 0, can use other strategies\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'alpha': trial.suggest_float('alpha', 0.0001, 1.0, log=True)\n",
    "    }\n",
    "\n",
    "    model = Lasso(**param)\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    preds = model.predict(X_valid_scaled)\n",
    "    mae = mean_absolute_error(y_valid, preds)\n",
    "    return mae\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = Lasso(**best_params)\n",
    "\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_best = best_model.predict(X_train_scaled)\n",
    "y_valid_pred_best = best_model.predict(X_valid_scaled)\n",
    "y_test_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with tuned hyperparameters:\")\n",
    "print_metrics(y_train, y_train_pred_best, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred_best, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred_best, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8155aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'lasso_model.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('lasso_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ebeac60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance with loaded model:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  45.57831118911151\n",
      "Mean Squared Error (MSE):  7452.50383243236\n",
      "R-squared (R2):  0.4475298886758706\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  51.78540514848887\n",
      "Mean Squared Error (MSE):  11307.512271468107\n",
      "R-squared (R2):  0.39724052301439505\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  65.72235577733532\n",
      "Mean Squared Error (MSE):  21692.76687807047\n",
      "R-squared (R2):  0.30026667809502083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('lasso_model.joblib')\n",
    "\n",
    "# Assuming you have your data prepared and scaled (X_train_scaled, X_valid_scaled, X_test_scaled)\n",
    "# You can reuse the scaled data from your previous steps or prepare and scale new data similarly\n",
    "\n",
    "# Predict on the training, validation, and test sets with the loaded model\n",
    "y_train_pred_best = loaded_model.predict(X_train_scaled)\n",
    "y_valid_pred_best = loaded_model.predict(X_valid_scaled)\n",
    "y_test_pred_best = loaded_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with loaded model:\")\n",
    "print_metrics(y_train, y_train_pred_best, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred_best, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred_best, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7088dc20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
