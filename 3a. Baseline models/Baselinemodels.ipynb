{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7fbd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Data Cleaning Listings/FINAL_TRAIN_DATA_PARIS.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "train_data_paris= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Data Cleaning Listings/FINAL_VALID_DATA_PARIS.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "valid_data_paris= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Data Cleaning Listings/FINAL_TEST_DATA_PARIS.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "test_data_paris= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2694802b",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aea2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_paris.info()\n",
    "valid_data_paris.info()\n",
    "test_data_paris.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b69d0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data_paris, valid_data_paris, and test_data_paris are already defined\n",
    "\n",
    "# Concatenate the datasets\n",
    "combined_data = pd.concat([train_data_paris, valid_data_paris, test_data_paris], ignore_index=True)\n",
    "\n",
    "# Display information about the combined dataset\n",
    "combined_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1449a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.to_csv('Combined_data_paris.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b511a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the 'date' column is in datetime format\n",
    "combined_data['date'] = pd.to_datetime(combined_data['date'])\n",
    "\n",
    "# Group by date and calculate the mean price\n",
    "mean_price_over_time = combined_data.groupby('date')['price'].mean()\n",
    "\n",
    "# Plot the mean price over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(mean_price_over_time.index, mean_price_over_time.values, marker='o', linestyle='-')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mean Price')\n",
    "plt.title('Mean Price Over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97298c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the necessary imports have been done\n",
    "# import pandas as pd\n",
    "\n",
    "# 1. Extract Date and Price Columns\n",
    "date_price_data = combined_data[['date', 'price']]\n",
    "\n",
    "# 2. Convert Date to Day of the Week\n",
    "date_price_data['day_of_week'] = pd.to_datetime(date_price_data['date']).dt.day_name()\n",
    "\n",
    "# 3. Group By Day of the Week and 4. Calculate Mean Price\n",
    "average_price_per_day = date_price_data.groupby('day_of_week')['price'].mean().reset_index()\n",
    "\n",
    "# 5. Output Results\n",
    "print(average_price_per_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b7d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the unique IDs in the combined dataset\n",
    "unique_ids_count = combined_data['id'].nunique()\n",
    "\n",
    "# Display the count of unique IDs\n",
    "print(f\"Number of unique IDs in the combined dataset: {unique_ids_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed14f88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the combined_data dataframe is already loaded\n",
    "# combined_data = pd.read_csv('path_to_combined_data.csv')\n",
    "\n",
    "# Extract the column names and store them in a set\n",
    "column_names = list(combined_data.columns)\n",
    "\n",
    "# Print the set of column names\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44a71ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the combined_data dataframe is already loaded\n",
    "# combined_data = pd.read_csv('path_to_combined_data.csv')\n",
    "\n",
    "# Count the number of columns starting with 'neighbourhood'\n",
    "neighbourhood_columns = [col for col in combined_data.columns if col.startswith('neighbourhood')]\n",
    "count_neighbourhood_columns = len(neighbourhood_columns)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of columns starting with 'neighbourhood': {count_neighbourhood_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa233a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the dataset is already loaded into a DataFrame named 'df'\n",
    "# Ensure the 'date' column is in datetime format\n",
    "combined_data['date'] = pd.to_datetime(combined_data['date'])\n",
    "\n",
    "# Get the first and last date\n",
    "first_date = combined_data['date'].min()\n",
    "last_date = combined_data['date'].max()\n",
    "\n",
    "print(f\"First date in the dataset: {first_date}\")\n",
    "print(f\"Last date in the dataset: {last_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cc0905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data_paris, valid_data_paris, and test_data_paris are already defined\n",
    "\n",
    "# Count the unique IDs in each dataset\n",
    "train_unique_ids_count = train_data_paris['id'].nunique()\n",
    "valid_unique_ids_count = valid_data_paris['id'].nunique()\n",
    "test_unique_ids_count = test_data_paris['id'].nunique()\n",
    "\n",
    "# Display the count of unique IDs\n",
    "print(f\"Number of unique IDs in the training dataset: {train_unique_ids_count}\")\n",
    "print(f\"Number of unique IDs in the validation dataset: {valid_unique_ids_count}\")\n",
    "print(f\"Number of unique IDs in the test dataset: {test_unique_ids_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec801b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data_paris, valid_data_paris, and test_data_paris are already defined\n",
    "\n",
    "# Ensure the 'date' column is in datetime format for each dataset\n",
    "train_data_paris['date'] = pd.to_datetime(train_data_paris['date'])\n",
    "valid_data_paris['date'] = pd.to_datetime(valid_data_paris['date'])\n",
    "test_data_paris['date'] = pd.to_datetime(test_data_paris['date'])\n",
    "\n",
    "# Get the first and last date for each dataset\n",
    "train_first_date = train_data_paris['date'].min()\n",
    "train_last_date = train_data_paris['date'].max()\n",
    "\n",
    "valid_first_date = valid_data_paris['date'].min()\n",
    "valid_last_date = valid_data_paris['date'].max()\n",
    "\n",
    "test_first_date = test_data_paris['date'].min()\n",
    "test_last_date = test_data_paris['date'].max()\n",
    "\n",
    "# Display the first and last date for each dataset\n",
    "print(f\"First date in the training dataset: {train_first_date}\")\n",
    "print(f\"Last date in the training dataset: {train_last_date}\")\n",
    "\n",
    "print(f\"First date in the validation dataset: {valid_first_date}\")\n",
    "print(f\"Last date in the validation dataset: {valid_last_date}\")\n",
    "\n",
    "print(f\"First date in the test dataset: {test_first_date}\")\n",
    "print(f\"Last date in the test dataset: {test_last_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f010848",
   "metadata": {},
   "source": [
    "# Engineering dataset for baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf9ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming train_data_paris, valid_data_paris, and test_data_paris are already defined\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Extract unique IDs from each dataset\n",
    "unique_train_ids = train_data_paris['id'].unique()\n",
    "unique_valid_ids = valid_data_paris['id'].unique()\n",
    "unique_test_ids = test_data_paris['id'].unique()\n",
    "\n",
    "# Randomly select 6000 unique IDs for training, 2000 for validation, and 2000 for testing\n",
    "selected_train_ids = np.random.choice(unique_train_ids, 6000, replace=False)\n",
    "selected_valid_ids = np.random.choice(unique_valid_ids, 2000, replace=False)\n",
    "selected_test_ids = np.random.choice(unique_test_ids, 2000, replace=False)\n",
    "\n",
    "# Ensure IDs are exclusive to each dataset\n",
    "selected_train_ids = set(selected_train_ids)\n",
    "selected_valid_ids = set(selected_valid_ids)\n",
    "selected_test_ids = set(selected_test_ids)\n",
    "\n",
    "# Filter datasets based on the selected unique IDs\n",
    "train_data_paris_filtered = train_data_paris[train_data_paris['id'].isin(selected_train_ids)]\n",
    "valid_data_paris_filtered = valid_data_paris[valid_data_paris['id'].isin(selected_valid_ids)]\n",
    "test_data_paris_filtered = test_data_paris[test_data_paris['id'].isin(selected_test_ids)]\n",
    "\n",
    "# Check the amount of unique IDs in the filtered datasets\n",
    "train_unique_count = len(train_data_paris_filtered['id'].unique())\n",
    "valid_unique_count = len(valid_data_paris_filtered['id'].unique())\n",
    "test_unique_count = len(test_data_paris_filtered['id'].unique())\n",
    "\n",
    "# Display the result\n",
    "print(\"Filtered Train Data Info:\")\n",
    "print(train_data_paris_filtered.info())\n",
    "print(f\"Number of unique IDs in filtered train data: {train_unique_count}\")\n",
    "\n",
    "print(\"\\nFiltered Validation Data Info:\")\n",
    "print(valid_data_paris_filtered.info())\n",
    "print(f\"Number of unique IDs in filtered validation data: {valid_unique_count}\")\n",
    "\n",
    "print(\"\\nFiltered Test Data Info:\")\n",
    "print(test_data_paris_filtered.info())\n",
    "print(f\"Number of unique IDs in filtered test data: {test_unique_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39147289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data_paris, valid_data_paris, and test_data_paris are already defined\n",
    "\n",
    "# Function to count the number of unique days and get the start and end dates in the dataset\n",
    "def analyze_dates(data):\n",
    "    unique_days = data['date'].nunique()\n",
    "    start_date = data['date'].min()\n",
    "    end_date = data['date'].max()\n",
    "    return unique_days, start_date, end_date\n",
    "\n",
    "# Analyze the dates in each dataset\n",
    "train_unique_days, train_start_date, train_end_date = analyze_dates(train_data_paris_filtered)\n",
    "valid_unique_days, valid_start_date, valid_end_date = analyze_dates(valid_data_paris_filtered)\n",
    "test_unique_days, test_start_date, test_end_date = analyze_dates(test_data_paris_filtered)\n",
    "\n",
    "# Check for overlapping IDs\n",
    "train_ids = set(train_data_paris_filtered['id'])\n",
    "valid_ids = set(valid_data_paris_filtered['id'])\n",
    "test_ids = set(test_data_paris_filtered['id'])\n",
    "\n",
    "no_overlap_train_valid = train_ids.isdisjoint(valid_ids)\n",
    "no_overlap_train_test = train_ids.isdisjoint(test_ids)\n",
    "no_overlap_valid_test = valid_ids.isdisjoint(test_ids)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Train Data: Unique days: {train_unique_days}, Start date: {train_start_date}, End date: {train_end_date}\")\n",
    "print(f\"Validation Data: Unique days: {valid_unique_days}, Start date: {valid_start_date}, End date: {valid_end_date}\")\n",
    "print(f\"Test Data: Unique days: {test_unique_days}, Start date: {test_start_date}, End date: {test_end_date}\")\n",
    "\n",
    "print(f\"\\nNo overlapping IDs between train and validation sets: {no_overlap_train_valid}\")\n",
    "print(f\"No overlapping IDs between train and test sets: {no_overlap_train_test}\")\n",
    "print(f\"No overlapping IDs between validation and test sets: {no_overlap_valid_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c1c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_paris_filtered.to_csv('Train_model_Paris_Vince.csv', index=False)\n",
    "valid_data_paris_filtered.to_csv('Valid_model_Paris_Vince.csv', index=False)\n",
    "test_data_paris_filtered.to_csv('Test_model_Paris_Vince.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff32dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/Train_model_Paris_Vince.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "train_data_paris_filtered= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/Valid_model_Paris_Vince.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "valid_data_paris_filtered= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/Test_model_Paris_Vince.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "test_data_paris_filtered= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e003b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/data met lstm parijs/train_data_lstm_paris_FINAL.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "train_data_lstm_paris= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/data met lstm parijs/val_data_lstm_paris_FINAL.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "valid_data_lstm_paris= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/data met lstm parijs/test_data_lstm_paris_FINAL.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "test_data_lstm_paris= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abbbf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decided columns to drop based on the correlation and multivariate analysis\n",
    "columns_to_drop =['season_Winter']\n",
    "train_data_lstm_paris = train_data_lstm_paris.drop(columns_to_drop, axis=1)\n",
    "valid_data_lstm_paris = valid_data_lstm_paris.drop(columns_to_drop, axis=1)\n",
    "test_data_lstm_paris = test_data_lstm_paris.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdb8d94",
   "metadata": {},
   "source": [
    "#### Without the LSTM features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b850bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data_paris, valid_data_paris, and test_data_paris are already defined\n",
    "\n",
    "# Function to select the last day for each ID in the given dataframe\n",
    "def select_last_day_per_id(data):\n",
    "    data = data.sort_values(by=['id', 'date'])\n",
    "    last_day_data = data.groupby('id').tail(1)\n",
    "    return last_day_data\n",
    "\n",
    "# Apply the function to each split\n",
    "train_data_paris_last_day = select_last_day_per_id(train_data_paris_filtered)\n",
    "valid_data_paris_last_day = select_last_day_per_id(valid_data_paris_filtered)\n",
    "test_data_paris_last_day = select_last_day_per_id(test_data_paris_filtered)\n",
    "\n",
    "# Calculate the total unique IDs across all splits\n",
    "total_unique_ids = set(train_data_paris_filtered['id']).union(valid_data_paris_filtered['id']).union(test_data_paris_filtered['id'])\n",
    "total_unique_count = len(total_unique_ids)\n",
    "\n",
    "# Calculate the percentage of unique IDs in each split based on the total unique IDs\n",
    "train_unique_count = len(set(train_data_paris_last_day['id']))\n",
    "valid_unique_count = len(set(valid_data_paris_last_day['id']))\n",
    "test_unique_count = len(set(test_data_paris_last_day['id']))\n",
    "\n",
    "train_percentage = (train_unique_count / total_unique_count) * 100\n",
    "valid_percentage = (valid_unique_count / total_unique_count) * 100\n",
    "test_percentage = (test_unique_count / total_unique_count) * 100\n",
    "\n",
    "print(\"\\nPercentage of unique IDs per split:\")\n",
    "print(f\"Train: {train_percentage:.2f}%\")\n",
    "print(f\"Validation: {valid_percentage:.2f}%\")\n",
    "print(f\"Test: {test_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f2d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to get the first and last date in the dataset\n",
    "def get_first_last_dates(data):\n",
    "    first_date = data['date'].min()\n",
    "    last_date = data['date'].max()\n",
    "    return first_date, last_date\n",
    "\n",
    "# Get the first and last date for each dataset\n",
    "train_first_date, train_last_date = get_first_last_dates(train_data_paris_last_day)\n",
    "valid_first_date, valid_last_date = get_first_last_dates(valid_data_paris_last_day)\n",
    "test_first_date, test_last_date = get_first_last_dates(test_data_paris_last_day)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Train Data: First date: {train_first_date}, Last date: {train_last_date}\")\n",
    "print(f\"Validation Data: First date: {valid_first_date}, Last date: {valid_last_date}\")\n",
    "print(f\"Test Data: First date: {test_first_date}, Last date: {test_last_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147eeb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_paris_last_day.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1b7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_paris_last_day.to_csv('Baslinemodels_train_paris.csv', index=False)\n",
    "valid_data_paris_last_day.to_csv('Baslinemodels_valid_paris.csv', index=False)\n",
    "test_data_paris_last_day.to_csv('Baslinemodels_test_paris.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdb1055",
   "metadata": {},
   "source": [
    "#### With the LSTM features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b3d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting unique IDs from each dataset\n",
    "train_ids = set(train_data_lstm_paris['id'])\n",
    "valid_ids = set(valid_data_lstm_paris['id'])\n",
    "test_ids = set(test_data_lstm_paris['id'])\n",
    "\n",
    "# Check for overlapping IDs between the datasets\n",
    "train_valid_overlap = train_ids.intersection(valid_ids)\n",
    "train_test_overlap = train_ids.intersection(test_ids)\n",
    "valid_test_overlap = valid_ids.intersection(test_ids)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Overlapping IDs between train and validation sets: {train_valid_overlap}\")\n",
    "print(f\"Number of overlapping IDs between train and validation sets: {len(train_valid_overlap)}\\n\")\n",
    "\n",
    "print(f\"Overlapping IDs between train and test sets: {train_test_overlap}\")\n",
    "print(f\"Number of overlapping IDs between train and test sets: {len(train_test_overlap)}\\n\")\n",
    "\n",
    "print(f\"Overlapping IDs between validation and test sets: {valid_test_overlap}\")\n",
    "print(f\"Number of overlapping IDs between validation and test sets: {len(valid_test_overlap)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bbabb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to get the first and last date in the dataset\n",
    "def get_first_last_dates(data):\n",
    "    first_date = data['date'].min()\n",
    "    last_date = data['date'].max()\n",
    "    return first_date, last_date\n",
    "\n",
    "# Get the first and last date for each dataset\n",
    "train_first_date, train_last_date = get_first_last_dates(train_data_lstm_paris)\n",
    "valid_first_date, valid_last_date = get_first_last_dates(valid_data_lstm_paris)\n",
    "test_first_date, test_last_date = get_first_last_dates(test_data_lstm_paris)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Merged Train Data: First date: {train_first_date}, Last date: {train_last_date}\")\n",
    "print(f\"Merged Validation Data: First date: {valid_first_date}, Last date: {valid_last_date}\")\n",
    "print(f\"Merged Test Data: First date: {test_first_date}, Last date: {test_last_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221dc26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_lstm_paris.to_csv('Baslinemodels_train_lstm_paris.csv', index=False)\n",
    "valid_data_lstm_paris.to_csv('Baslinemodels_valid_lstm_paris.csv', index=False)\n",
    "test_data_lstm_paris.to_csv('Baslinemodels_test_lstm_paris.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422d322d",
   "metadata": {},
   "source": [
    "#### Check if original dataset and lstm dataset are identical except additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf404e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the dataframes are already loaded\n",
    "# train_data_lstm_paris = pd.read_csv('path_to_train_data_lstm_paris.csv')\n",
    "# train_data_paris_last_day = pd.read_csv('path_to_train_data_paris_last_day.csv')\n",
    "\n",
    "# Get the list of columns to compare, excluding the last 16 columns from train_data_lstm_paris\n",
    "columns_to_compare = train_data_lstm_paris.columns[:-16]\n",
    "\n",
    "# Extract the relevant columns from train_data_lstm_paris\n",
    "trimmed_train_data_lstm_paris = train_data_lstm_paris[columns_to_compare]\n",
    "\n",
    "# Align dataframes to ensure they have the same order of columns and index\n",
    "trimmed_train_data_lstm_paris = trimmed_train_data_lstm_paris.reset_index(drop=True)\n",
    "train_data_paris_last_day = train_data_paris_last_day[columns_to_compare].reset_index(drop=True)\n",
    "\n",
    "# Compare the dataframes\n",
    "differences = trimmed_train_data_lstm_paris != train_data_paris_last_day\n",
    "\n",
    "# Find the indices and columns where differences occur\n",
    "diff_indices = differences[differences].stack().index.tolist()\n",
    "\n",
    "if not diff_indices:\n",
    "    print(\"The data in train_data_lstm_paris (excluding the last 16 columns) is identical to train_data_paris_last_day.\")\n",
    "else:\n",
    "    print(\"Differences found:\")\n",
    "    for index, column in diff_indices:\n",
    "        lstm_value = trimmed_train_data_lstm_paris.loc[index, column]\n",
    "        last_day_value = train_data_paris_last_day.loc[index, column]\n",
    "        print(f\"Row {index}, Column '{column}': train_data_lstm_paris = {lstm_value}, train_data_paris_last_day = {last_day_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a01f46",
   "metadata": {},
   "source": [
    "# Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78d7be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WITHOUT LSTM\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/Baslinemodels_train_paris.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "train_data_paris= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/Baslinemodels_valid_paris.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "valid_data_paris= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/Baslinemodels_test_paris.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "test_data_paris= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f64e34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WITH LSTM\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/Baslinemodels_train_lstm_paris.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "train_data_lstm_paris= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/Baslinemodels_valid_lstm_paris.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "valid_data_lstm_paris= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/Baslinemodels_test_lstm_paris.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "test_data_lstm_paris= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "219b7490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6000 entries, 0 to 5999\n",
      "Columns: 143 entries, id to 15\n",
      "dtypes: bool(97), float64(26), int64(19), object(1)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data_lstm_paris.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e424519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data - Columns with NaN values and their counts:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Valid Data - Columns with NaN values and their counts:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Test Data - Columns with NaN values and their counts:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data_lstm_paris, valid_data_lstm_paris, and test_data_lstm_paris are already defined\n",
    "\n",
    "# Check for NaN values in the train dataset\n",
    "train_nan_counts = train_data_lstm_paris.isna().sum()\n",
    "train_columns_with_nan = train_nan_counts[train_nan_counts > 0]\n",
    "print(\"Train Data - Columns with NaN values and their counts:\")\n",
    "print(train_columns_with_nan)\n",
    "\n",
    "# Check for NaN values in the valid dataset\n",
    "valid_nan_counts = valid_data_lstm_paris.isna().sum()\n",
    "valid_columns_with_nan = valid_nan_counts[valid_nan_counts > 0]\n",
    "print(\"\\nValid Data - Columns with NaN values and their counts:\")\n",
    "print(valid_columns_with_nan)\n",
    "\n",
    "# Check for NaN values in the test dataset\n",
    "test_nan_counts = test_data_lstm_paris.isna().sum()\n",
    "test_columns_with_nan = test_nan_counts[test_nan_counts > 0]\n",
    "print(\"\\nTest Data - Columns with NaN values and their counts:\")\n",
    "print(test_columns_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6b8b082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs are unique across the train, validation, and test datasets.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data_lstm_paris, valid_data_lstm_paris, and test_data_lstm_paris are DataFrames\n",
    "\n",
    "# Extract IDs from each dataset\n",
    "train_ids = set(train_data_lstm_paris['id'])\n",
    "valid_ids = set(valid_data_lstm_paris['id'])\n",
    "test_ids = set(test_data_lstm_paris['id'])\n",
    "\n",
    "# Check for overlapping IDs between the datasets\n",
    "overlap_train_valid = train_ids.intersection(valid_ids)\n",
    "overlap_train_test = train_ids.intersection(test_ids)\n",
    "overlap_valid_test = valid_ids.intersection(test_ids)\n",
    "\n",
    "# Output the results\n",
    "if not overlap_train_valid and not overlap_train_test and not overlap_valid_test:\n",
    "    print(\"IDs are unique across the train, validation, and test datasets.\")\n",
    "else:\n",
    "    print(\"There are overlapping IDs between the datasets.\")\n",
    "    if overlap_train_valid:\n",
    "        print(f\"Overlapping IDs between train and validation datasets: {overlap_train_valid}\")\n",
    "    if overlap_train_test:\n",
    "        print(f\"Overlapping IDs between train and test datasets: {overlap_train_test}\")\n",
    "    if overlap_valid_test:\n",
    "        print(f\"Overlapping IDs between validation and test datasets: {overlap_valid_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "358e0070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6000 entries, 0 to 5999\n",
      "Columns: 143 entries, id to 15\n",
      "dtypes: bool(97), float64(26), int64(19), object(1)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data_lstm_paris.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06a3aeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of all columns in the dataset:\n",
      "['id', 'date', 'available', 'price', 'season_Autumn', 'is_holiday', 'is_school_holiday', 'host_id', 'host_response_time', 'host_is_superhost', 'host_listings_count', 'host_total_listings_count', 'host_identity_verified', 'latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'availability_90', 'availability_365', 'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'review_scores_rating', 'instant_bookable', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', 'reviews_per_month', 'Washer', 'Dryer', 'Essentials', 'Free washer – In unit', 'exercise equipment', 'Dedicated workspace', 'First aid kit', 'Wine glasses', 'Long term stays allowed', 'Dishwasher', 'hot tub', 'Baking sheet', 'Elevator', 'Portable fans', 'Cleaning products', 'Dining table', 'soap', 'tv', 'Iron', 'sauna', 'Heating', 'Drying rack for clothing', 'Private entrance', 'Hangers', 'view', 'parking', 'stove', 'Fire extinguisher', 'pool', 'Pets allowed', 'shampoo', 'clothing storage', 'gym', 'coffee', 'Hot water', 'Microwave', 'Books and reading material', 'Room-darkening shades', 'Carbon monoxide alarm', 'Bed linens', 'Hot water kettle', 'broadcast', 'Host greets you', 'bbq', 'wifi', 'game console', 'backyard', 'oven', 'Hair dryer', 'Freezer', 'Kitchen', 'conditioner', 'Bathtub', 'Shower gel', 'Smoke alarm', 'Toaster', 'Self check-in', 'crib', 'Extra pillows and blankets', 'Luggage dropoff allowed', 'toys children', 'Laundromat nearby', 'Lockbox', 'sound system', 'total_amenities', 'listing_reviewed', 'neighbourhood_Batignolles-Monceau', 'neighbourhood_Bourse', 'neighbourhood_Buttes-Chaumont', 'neighbourhood_Buttes-Montmartre', 'neighbourhood_Entrepôt', 'neighbourhood_Gobelins', 'neighbourhood_Hôtel-de-Ville', 'neighbourhood_Louvre', 'neighbourhood_Luxembourg', 'neighbourhood_Ménilmontant', 'neighbourhood_Observatoire', 'neighbourhood_Opéra', 'neighbourhood_Palais-Bourbon', 'neighbourhood_Panthéon', 'neighbourhood_Passy', 'neighbourhood_Popincourt', 'neighbourhood_Reuilly', 'neighbourhood_Temple', 'neighbourhood_Vaugirard', 'neighbourhood_Élysée', 'property_type_Apartment', 'property_type_Hotel', 'property_type_House', 'property_type_Other', 'room_type_Private room', 'nearby_airbnbs_count', 'luxury_amenities_score', 'nearby_restaurants_bars', 'nearby_transport', 'mean_price_neighbors', 'kitchen_amenities', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15']\n"
     ]
    }
   ],
   "source": [
    "# Print a list of all columns in the dataset\n",
    "columns_list = train_data_lstm_paris.columns.tolist()\n",
    "print(\"List of all columns in the dataset:\")\n",
    "print(columns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa037cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First date in the LSTM training dataset (Paris): 2024-01-11 00:00:00\n",
      "Last date in the LSTM training dataset (Paris): 2024-01-12 00:00:00\n",
      "First date in the LSTM validation dataset (Paris): 2024-02-10 00:00:00\n",
      "Last date in the LSTM validation dataset (Paris): 2024-02-11 00:00:00\n",
      "First date in the LSTM test dataset (Paris): 2024-03-11 00:00:00\n",
      "Last date in the LSTM test dataset (Paris): 2024-03-12 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure the 'date' column is in datetime format for each dataset\n",
    "train_data_lstm_paris['date'] = pd.to_datetime(train_data_lstm_paris['date'])\n",
    "valid_data_lstm_paris['date'] = pd.to_datetime(valid_data_lstm_paris['date'])\n",
    "test_data_lstm_paris['date'] = pd.to_datetime(test_data_lstm_paris['date'])\n",
    "\n",
    "# Get the first and last date for each dataset\n",
    "train_lstm_first_date = train_data_lstm_paris['date'].min()\n",
    "train_lstm_last_date = train_data_lstm_paris['date'].max()\n",
    "\n",
    "valid_lstm_first_date = valid_data_lstm_paris['date'].min()\n",
    "valid_lstm_last_date = valid_data_lstm_paris['date'].max()\n",
    "\n",
    "test_lstm_first_date = test_data_lstm_paris['date'].min()\n",
    "test_lstm_last_date = test_data_lstm_paris['date'].max()\n",
    "\n",
    "# Display the first and last date for each dataset\n",
    "print(f\"First date in the LSTM training dataset (Paris): {train_lstm_first_date}\")\n",
    "print(f\"Last date in the LSTM training dataset (Paris): {train_lstm_last_date}\")\n",
    "\n",
    "print(f\"First date in the LSTM validation dataset (Paris): {valid_lstm_first_date}\")\n",
    "print(f\"Last date in the LSTM validation dataset (Paris): {valid_lstm_last_date}\")\n",
    "\n",
    "print(f\"First date in the LSTM test dataset (Paris): {test_lstm_first_date}\")\n",
    "print(f\"Last date in the LSTM test dataset (Paris): {test_lstm_last_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d2860fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First date in the training dataset (Paris): 2024-01-11 00:00:00\n",
      "Last date in the training dataset (Paris): 2024-01-12 00:00:00\n",
      "First date in the validation dataset (Paris): 2024-02-10 00:00:00\n",
      "Last date in the validation dataset (Paris): 2024-02-11 00:00:00\n",
      "First date in the test dataset (Paris): 2024-03-11 00:00:00\n",
      "Last date in the test dataset (Paris): 2024-03-12 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure the 'date' column is in datetime format for each dataset\n",
    "train_data_paris['date'] = pd.to_datetime(train_data_paris['date'])\n",
    "valid_data_paris['date'] = pd.to_datetime(valid_data_paris['date'])\n",
    "test_data_paris['date'] = pd.to_datetime(test_data_paris['date'])\n",
    "\n",
    "# Get the first and last date for each dataset\n",
    "train_paris_first_date = train_data_paris['date'].min()\n",
    "train_paris_last_date = train_data_paris['date'].max()\n",
    "\n",
    "valid_paris_first_date = valid_data_paris['date'].min()\n",
    "valid_paris_last_date = valid_data_paris['date'].max()\n",
    "\n",
    "test_paris_first_date = test_data_paris['date'].min()\n",
    "test_paris_last_date = test_data_paris['date'].max()\n",
    "\n",
    "# Display the first and last date for each dataset\n",
    "print(f\"First date in the training dataset (Paris): {train_paris_first_date}\")\n",
    "print(f\"Last date in the training dataset (Paris): {train_paris_last_date}\")\n",
    "\n",
    "print(f\"First date in the validation dataset (Paris): {valid_paris_first_date}\")\n",
    "print(f\"Last date in the validation dataset (Paris): {valid_paris_last_date}\")\n",
    "\n",
    "print(f\"First date in the test dataset (Paris): {test_paris_first_date}\")\n",
    "print(f\"Last date in the test dataset (Paris): {test_paris_last_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6271d932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in Train Data:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Missing values in Validation Data:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Missing values in Test Data:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of datasets to check\n",
    "datasets = {\n",
    "    \"Train Data\": train_data_lstm_paris,\n",
    "    \"Validation Data\": valid_data_lstm_paris,\n",
    "    \"Test Data\": test_data_lstm_paris\n",
    "}\n",
    "\n",
    "# Loop through each dataset and print the number of missing values per column\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    print(f\"\\nMissing values in {dataset_name}:\")\n",
    "    missing_values_count = dataset.isnull().sum()\n",
    "    print(missing_values_count[missing_values_count > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64bda414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id               0.0\n",
      "date             0.0\n",
      "available        0.0\n",
      "price            0.0\n",
      "season_Autumn    0.0\n",
      "                ... \n",
      "11               0.0\n",
      "12               0.0\n",
      "13               0.0\n",
      "14               0.0\n",
      "15               0.0\n",
      "Length: 143, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train_data_lstm_paris.isnull().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "478f4e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6000 entries, 0 to 5999\n",
      "Columns: 127 entries, id to kitchen_amenities\n",
      "dtypes: bool(97), datetime64[ns](1), float64(10), int64(19)\n",
      "memory usage: 1.9 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6000 entries, 0 to 5999\n",
      "Columns: 143 entries, id to 15\n",
      "dtypes: bool(97), datetime64[ns](1), float64(26), int64(19)\n",
      "memory usage: 2.7 MB\n"
     ]
    }
   ],
   "source": [
    "train_data_paris.info()\n",
    "train_data_lstm_paris.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "375694e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                              18.662475\n",
       "price                                            5.207118\n",
       "host_id                                          1.676409\n",
       "host_response_time                               0.876890\n",
       "host_listings_count                             18.232472\n",
       "host_total_listings_count                       11.485384\n",
       "latitude                                        -0.218011\n",
       "longitude                                       -0.375003\n",
       "accommodates                                     0.939080\n",
       "bathrooms                                        3.329220\n",
       "bedrooms                                         0.579301\n",
       "beds                                             1.361116\n",
       "availability_90                                  1.906520\n",
       "availability_365                                 1.698004\n",
       "number_of_reviews                                3.081093\n",
       "number_of_reviews_ltm                            2.262024\n",
       "number_of_reviews_l30d                           3.155870\n",
       "review_scores_rating                            -1.465339\n",
       "calculated_host_listings_count                   4.915939\n",
       "calculated_host_listings_count_entire_homes      5.931761\n",
       "calculated_host_listings_count_private_rooms     3.901968\n",
       "reviews_per_month                                1.759056\n",
       "total_amenities                                  0.763165\n",
       "nearby_airbnbs_count                             0.863945\n",
       "luxury_amenities_score                           2.766684\n",
       "nearby_restaurants_bars                          0.825845\n",
       "nearby_transport                                 1.049720\n",
       "mean_price_neighbors                             3.308824\n",
       "kitchen_amenities                                0.028238\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating skewness for each numerical feature in the dataset\n",
    "skewness = train_data_paris.select_dtypes(include=['float64', 'int64']).skew()\n",
    "\n",
    "# Display the skewness values\n",
    "skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef3c401b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\n",
      "ERROR: No matching distribution found for pickle\n"
     ]
    }
   ],
   "source": [
    "!pip install pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f6404",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459c38c0",
   "metadata": {},
   "source": [
    "## Without LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea390cd",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "867f4afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 18:09:58,095] A new study created in memory with name: no-name-a23ed1cf-e903-4d17-b354-907d9240b375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df18e54917494000888e649f34e3e604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 18:09:58,870] Trial 0 finished with value: 1.454059889534739 and parameters: {'n_estimators': 219, 'max_depth': 8, 'learning_rate': 0.18667679203753126, 'subsample': 0.8264135936814088, 'colsample_bytree': 0.8858608256894513, 'gamma': 4.530850974683606, 'reg_alpha': 6.120521561184863, 'reg_lambda': 1.791863146441799}. Best is trial 0 with value: 1.454059889534739.\n",
      "[I 2024-08-06 18:09:59,530] Trial 1 finished with value: 1.6082117406691168 and parameters: {'n_estimators': 104, 'max_depth': 5, 'learning_rate': 0.01182760952713782, 'subsample': 0.7851333570015854, 'colsample_bytree': 0.5775186679727726, 'gamma': 3.063313954792393, 'reg_alpha': 3.062698667390836, 'reg_lambda': 3.4579199126884674}. Best is trial 0 with value: 1.454059889534739.\n",
      "[I 2024-08-06 18:10:00,620] Trial 2 finished with value: 1.446089178994273 and parameters: {'n_estimators': 236, 'max_depth': 6, 'learning_rate': 0.06875253465964573, 'subsample': 0.6442578317786496, 'colsample_bytree': 0.9682538822737609, 'gamma': 3.312812227333179, 'reg_alpha': 0.4014313924486046, 'reg_lambda': 1.9150723868573716}. Best is trial 2 with value: 1.446089178994273.\n",
      "[I 2024-08-06 18:10:02,342] Trial 3 finished with value: 1.4879120158657564 and parameters: {'n_estimators': 452, 'max_depth': 8, 'learning_rate': 0.24390050455099246, 'subsample': 0.5099914813128763, 'colsample_bytree': 0.9962263634625944, 'gamma': 4.7040444394775225, 'reg_alpha': 7.160104241172006, 'reg_lambda': 0.9647680514565016}. Best is trial 2 with value: 1.446089178994273.\n",
      "[I 2024-08-06 18:10:05,323] Trial 4 finished with value: 1.4446870544513186 and parameters: {'n_estimators': 618, 'max_depth': 10, 'learning_rate': 0.03553805340780201, 'subsample': 0.5299712995863752, 'colsample_bytree': 0.9765319949239273, 'gamma': 3.396174836837043, 'reg_alpha': 1.7304127888845189, 'reg_lambda': 8.418465307758066}. Best is trial 4 with value: 1.4446870544513186.\n",
      "[I 2024-08-06 18:10:07,183] Trial 5 finished with value: 1.474386664200595 and parameters: {'n_estimators': 574, 'max_depth': 4, 'learning_rate': 0.2258396299915952, 'subsample': 0.5953231654514257, 'colsample_bytree': 0.6578861376582045, 'gamma': 4.66242142226243, 'reg_alpha': 2.6634396103033864, 'reg_lambda': 4.594611004137432}. Best is trial 4 with value: 1.4446870544513186.\n",
      "[I 2024-08-06 18:10:10,273] Trial 6 finished with value: 1.4624929422384003 and parameters: {'n_estimators': 982, 'max_depth': 9, 'learning_rate': 0.13079981199769597, 'subsample': 0.8186049291626732, 'colsample_bytree': 0.513169113995043, 'gamma': 2.303540672201085, 'reg_alpha': 4.617116545390197, 'reg_lambda': 2.453561967070198}. Best is trial 4 with value: 1.4446870544513186.\n",
      "[I 2024-08-06 18:10:13,279] Trial 7 finished with value: 1.4741106820580718 and parameters: {'n_estimators': 943, 'max_depth': 5, 'learning_rate': 0.15209133174755163, 'subsample': 0.7702534894642856, 'colsample_bytree': 0.6934501162114058, 'gamma': 3.9009384774519535, 'reg_alpha': 6.459156106622416, 'reg_lambda': 1.9441319637245291}. Best is trial 4 with value: 1.4446870544513186.\n",
      "[I 2024-08-06 18:10:15,103] Trial 8 finished with value: 1.4447944275434192 and parameters: {'n_estimators': 532, 'max_depth': 9, 'learning_rate': 0.1301662174600086, 'subsample': 0.7920020621033378, 'colsample_bytree': 0.7204970312081351, 'gamma': 2.4926903832067655, 'reg_alpha': 7.307129057641692, 'reg_lambda': 3.890167801048819}. Best is trial 4 with value: 1.4446870544513186.\n",
      "[I 2024-08-06 18:10:16,556] Trial 9 finished with value: 1.478317955416862 and parameters: {'n_estimators': 294, 'max_depth': 4, 'learning_rate': 0.19828024162500676, 'subsample': 0.7359561238609156, 'colsample_bytree': 0.6868656225462315, 'gamma': 0.2832294415601688, 'reg_alpha': 7.756456540616828, 'reg_lambda': 1.9611418994988405}. Best is trial 4 with value: 1.4446870544513186.\n",
      "[I 2024-08-06 18:10:19,393] Trial 10 finished with value: 1.5394035387349172 and parameters: {'n_estimators': 752, 'max_depth': 10, 'learning_rate': 0.285280974130579, 'subsample': 0.9823844590257875, 'colsample_bytree': 0.8318074205269157, 'gamma': 1.3929196695190083, 'reg_alpha': 0.2830544225680216, 'reg_lambda': 8.940428773940825}. Best is trial 4 with value: 1.4446870544513186.\n",
      "[I 2024-08-06 18:10:22,002] Trial 11 finished with value: 1.4691543796993543 and parameters: {'n_estimators': 617, 'max_depth': 10, 'learning_rate': 0.0670688163315078, 'subsample': 0.9456161659377851, 'colsample_bytree': 0.7968298462476663, 'gamma': 2.104215080804472, 'reg_alpha': 9.945468604872678, 'reg_lambda': 7.582733223454873}. Best is trial 4 with value: 1.4446870544513186.\n",
      "[I 2024-08-06 18:10:24,638] Trial 12 finished with value: 1.4579586725203062 and parameters: {'n_estimators': 698, 'max_depth': 8, 'learning_rate': 0.09808331755543265, 'subsample': 0.6621075743319521, 'colsample_bytree': 0.9124574838158583, 'gamma': 1.4453622938758146, 'reg_alpha': 9.141677133203572, 'reg_lambda': 6.505795687190187}. Best is trial 4 with value: 1.4446870544513186.\n",
      "[I 2024-08-06 18:10:27,679] Trial 13 finished with value: 1.491102132056615 and parameters: {'n_estimators': 449, 'max_depth': 10, 'learning_rate': 0.011055893291601948, 'subsample': 0.9006623958457012, 'colsample_bytree': 0.7610557484282354, 'gamma': 3.097573356797458, 'reg_alpha': 4.559842018827931, 'reg_lambda': 5.515761958842355}. Best is trial 4 with value: 1.4446870544513186.\n",
      "[I 2024-08-06 18:10:30,495] Trial 14 finished with value: 1.458936912031938 and parameters: {'n_estimators': 820, 'max_depth': 7, 'learning_rate': 0.10874031436611581, 'subsample': 0.5050449140803509, 'colsample_bytree': 0.6176058754833085, 'gamma': 3.7923595065537885, 'reg_alpha': 2.578001290487512, 'reg_lambda': 9.498305413752702}. Best is trial 4 with value: 1.4446870544513186.\n",
      "[I 2024-08-06 18:10:32,556] Trial 15 finished with value: 1.453823075863096 and parameters: {'n_estimators': 405, 'max_depth': 9, 'learning_rate': 0.051720907046355354, 'subsample': 0.6986387381090251, 'colsample_bytree': 0.8490183123912457, 'gamma': 1.8477155998671777, 'reg_alpha': 1.831032738453259, 'reg_lambda': 7.646221134420089}. Best is trial 4 with value: 1.4446870544513186.\n",
      "[I 2024-08-06 18:10:35,202] Trial 16 finished with value: 1.4715617699248038 and parameters: {'n_estimators': 617, 'max_depth': 9, 'learning_rate': 0.1612788478519691, 'subsample': 0.5764734895514736, 'colsample_bytree': 0.7376312184942616, 'gamma': 0.6733730844004726, 'reg_alpha': 8.204912690519258, 'reg_lambda': 3.894062921692677}. Best is trial 4 with value: 1.4446870544513186.\n",
      "[I 2024-08-06 18:10:38,511] Trial 17 finished with value: 1.4841661707589686 and parameters: {'n_estimators': 845, 'max_depth': 7, 'learning_rate': 0.04842161874409169, 'subsample': 0.8777704119857034, 'colsample_bytree': 0.9282488860352569, 'gamma': 2.7315773358297237, 'reg_alpha': 5.805058022218493, 'reg_lambda': 0.048047190410081875}. Best is trial 4 with value: 1.4446870544513186.\n",
      "[I 2024-08-06 18:10:40,518] Trial 18 finished with value: 1.463050358493653 and parameters: {'n_estimators': 509, 'max_depth': 9, 'learning_rate': 0.10079981498962622, 'subsample': 0.7082500491784619, 'colsample_bytree': 0.7538380621934448, 'gamma': 3.5456493261961683, 'reg_alpha': 3.5486910471560016, 'reg_lambda': 5.861386782374161}. Best is trial 4 with value: 1.4446870544513186.\n",
      "[I 2024-08-06 18:10:42,052] Trial 19 finished with value: 1.4387133604498177 and parameters: {'n_estimators': 369, 'max_depth': 3, 'learning_rate': 0.12392816914709047, 'subsample': 0.5877959237413559, 'colsample_bytree': 0.8141356439030047, 'gamma': 2.5666433642375632, 'reg_alpha': 1.6404909433658763, 'reg_lambda': 7.193717414928673}. Best is trial 19 with value: 1.4387133604498177.\n",
      "[I 2024-08-06 18:10:43,540] Trial 20 finished with value: 1.4683531104128178 and parameters: {'n_estimators': 361, 'max_depth': 3, 'learning_rate': 0.03394146927848693, 'subsample': 0.5718564684490267, 'colsample_bytree': 0.8544981181266099, 'gamma': 4.112209767233892, 'reg_alpha': 1.68635587922601, 'reg_lambda': 8.309311819910237}. Best is trial 19 with value: 1.4387133604498177.\n",
      "[I 2024-08-06 18:10:45,440] Trial 21 finished with value: 1.4812224560291256 and parameters: {'n_estimators': 506, 'max_depth': 3, 'learning_rate': 0.13557023422670395, 'subsample': 0.5320615079673894, 'colsample_bytree': 0.8092689722298186, 'gamma': 2.6791813544717256, 'reg_alpha': 1.1643424701363454, 'reg_lambda': 6.784817262166102}. Best is trial 19 with value: 1.4387133604498177.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 18:10:47,822] Trial 22 finished with value: 1.4556274283437276 and parameters: {'n_estimators': 680, 'max_depth': 6, 'learning_rate': 0.1773904664122972, 'subsample': 0.6340772024454612, 'colsample_bytree': 0.717876703747942, 'gamma': 2.6589117431693654, 'reg_alpha': 3.993271381569392, 'reg_lambda': 4.867899409451111}. Best is trial 19 with value: 1.4387133604498177.\n",
      "[I 2024-08-06 18:10:49,742] Trial 23 finished with value: 1.4424466574847639 and parameters: {'n_estimators': 346, 'max_depth': 10, 'learning_rate': 0.08330929015524408, 'subsample': 0.5477426538806848, 'colsample_bytree': 0.9545701764333968, 'gamma': 1.7845865740805704, 'reg_alpha': 0.9713898789509687, 'reg_lambda': 8.05538076441584}. Best is trial 19 with value: 1.4387133604498177.\n",
      "[I 2024-08-06 18:10:51,777] Trial 24 finished with value: 1.4263770343548947 and parameters: {'n_estimators': 340, 'max_depth': 10, 'learning_rate': 0.08331436248671001, 'subsample': 0.5597321501027921, 'colsample_bytree': 0.9509533008066777, 'gamma': 1.6213058678438776, 'reg_alpha': 1.143021722480996, 'reg_lambda': 9.896673995399096}. Best is trial 24 with value: 1.4263770343548947.\n",
      "[I 2024-08-06 18:10:53,596] Trial 25 finished with value: 1.4226106366046956 and parameters: {'n_estimators': 339, 'max_depth': 5, 'learning_rate': 0.07895763208768057, 'subsample': 0.6032245265532089, 'colsample_bytree': 0.943554703182385, 'gamma': 1.1369916814084373, 'reg_alpha': 0.9941983277664036, 'reg_lambda': 9.93969529494126}. Best is trial 25 with value: 1.4226106366046956.\n",
      "[I 2024-08-06 18:10:54,436] Trial 26 finished with value: 1.4377653401243 and parameters: {'n_estimators': 143, 'max_depth': 4, 'learning_rate': 0.11187404805598701, 'subsample': 0.6111324928568157, 'colsample_bytree': 0.8896716066246968, 'gamma': 0.8630486584964125, 'reg_alpha': 0.2129389976027476, 'reg_lambda': 9.961135903147023}. Best is trial 25 with value: 1.4226106366046956.\n",
      "[I 2024-08-06 18:10:55,126] Trial 27 finished with value: 1.468753048569312 and parameters: {'n_estimators': 102, 'max_depth': 5, 'learning_rate': 0.08120841118115457, 'subsample': 0.6780246760483725, 'colsample_bytree': 0.8920507763149339, 'gamma': 0.896949740160307, 'reg_alpha': 0.16669812447846277, 'reg_lambda': 9.750387612967298}. Best is trial 25 with value: 1.4226106366046956.\n",
      "[I 2024-08-06 18:10:56,075] Trial 28 finished with value: 1.4315738848742214 and parameters: {'n_estimators': 191, 'max_depth': 4, 'learning_rate': 0.10654925628498157, 'subsample': 0.6125890052510873, 'colsample_bytree': 0.9336045874671862, 'gamma': 0.005683674138929229, 'reg_alpha': 0.9160318112036052, 'reg_lambda': 9.894903951279687}. Best is trial 25 with value: 1.4226106366046956.\n",
      "[I 2024-08-06 18:10:57,175] Trial 29 finished with value: 1.452538637765799 and parameters: {'n_estimators': 193, 'max_depth': 5, 'learning_rate': 0.08682349376479045, 'subsample': 0.6233592122203312, 'colsample_bytree': 0.9399214131491144, 'gamma': 0.00011374987316892238, 'reg_alpha': 2.39407868848825, 'reg_lambda': 8.821851463258252}. Best is trial 25 with value: 1.4226106366046956.\n",
      "[I 2024-08-06 18:10:58,533] Trial 30 finished with value: 1.4477334901556322 and parameters: {'n_estimators': 274, 'max_depth': 6, 'learning_rate': 0.14900238231297405, 'subsample': 0.5535537776052494, 'colsample_bytree': 0.8709475107729574, 'gamma': 1.1457840520024245, 'reg_alpha': 0.9760537884098578, 'reg_lambda': 9.209147827124228}. Best is trial 25 with value: 1.4226106366046956.\n",
      "[I 2024-08-06 18:10:59,459] Trial 31 finished with value: 1.4324299718818796 and parameters: {'n_estimators': 165, 'max_depth': 4, 'learning_rate': 0.11086661661790154, 'subsample': 0.6096795559888096, 'colsample_bytree': 0.9070047619531345, 'gamma': 0.5006977006386772, 'reg_alpha': 0.7671291134782167, 'reg_lambda': 9.812433533815941}. Best is trial 25 with value: 1.4226106366046956.\n",
      "[I 2024-08-06 18:11:00,391] Trial 32 finished with value: 1.455548604919546 and parameters: {'n_estimators': 175, 'max_depth': 4, 'learning_rate': 0.06531201701711267, 'subsample': 0.6124079557390496, 'colsample_bytree': 0.9137403518638838, 'gamma': 0.47880533504891254, 'reg_alpha': 1.018408718255046, 'reg_lambda': 9.347793212360564}. Best is trial 25 with value: 1.4226106366046956.\n",
      "[I 2024-08-06 18:11:01,544] Trial 33 finished with value: 1.4450139747167015 and parameters: {'n_estimators': 242, 'max_depth': 4, 'learning_rate': 0.089778295329179, 'subsample': 0.6580725493116346, 'colsample_bytree': 0.9986992606220677, 'gamma': 0.2734653569980338, 'reg_alpha': 3.436030491443468, 'reg_lambda': 9.933798285951143}. Best is trial 25 with value: 1.4226106366046956.\n",
      "[I 2024-08-06 18:11:02,984] Trial 34 finished with value: 1.4731119623313764 and parameters: {'n_estimators': 293, 'max_depth': 5, 'learning_rate': 0.10983475134857426, 'subsample': 0.5628447983463165, 'colsample_bytree': 0.9576779319670059, 'gamma': 1.186689271342912, 'reg_alpha': 0.6666616013546469, 'reg_lambda': 8.794490598932173}. Best is trial 25 with value: 1.4226106366046956.\n",
      "[I 2024-08-06 18:11:04,060] Trial 35 finished with value: 1.4692898148921678 and parameters: {'n_estimators': 163, 'max_depth': 6, 'learning_rate': 0.028846731261284037, 'subsample': 0.7227355627686901, 'colsample_bytree': 0.9067502415414741, 'gamma': 0.07695355225861289, 'reg_alpha': 2.1960876351506435, 'reg_lambda': 8.30913614997008}. Best is trial 25 with value: 1.4226106366046956.\n",
      "[I 2024-08-06 18:11:05,138] Trial 36 finished with value: 1.4767396051735695 and parameters: {'n_estimators': 228, 'max_depth': 3, 'learning_rate': 0.053944722261130486, 'subsample': 0.6790156934719276, 'colsample_bytree': 0.9773795190404885, 'gamma': 0.6063826087226933, 'reg_alpha': 1.3933409253564957, 'reg_lambda': 9.995307017951674}. Best is trial 25 with value: 1.4226106366046956.\n",
      "[I 2024-08-06 18:11:06,869] Trial 37 finished with value: 1.439099869429866 and parameters: {'n_estimators': 307, 'max_depth': 7, 'learning_rate': 0.07217472271186402, 'subsample': 0.6407131763130994, 'colsample_bytree': 0.9388411261045069, 'gamma': 1.626323139311627, 'reg_alpha': 2.8845638399288007, 'reg_lambda': 9.076883186003116}. Best is trial 25 with value: 1.4226106366046956.\n",
      "[I 2024-08-06 18:11:09,117] Trial 38 finished with value: 1.4609657316534346 and parameters: {'n_estimators': 429, 'max_depth': 5, 'learning_rate': 0.18127992810006738, 'subsample': 0.529481533741979, 'colsample_bytree': 0.9716163401147375, 'gamma': 0.9485090215295879, 'reg_alpha': 0.6823911771917551, 'reg_lambda': 7.977228402657624}. Best is trial 25 with value: 1.4226106366046956.\n",
      "[I 2024-08-06 18:11:10,324] Trial 39 finished with value: 1.4380319830950206 and parameters: {'n_estimators': 245, 'max_depth': 4, 'learning_rate': 0.11809292765728886, 'subsample': 0.6001653389598738, 'colsample_bytree': 0.8703656082695004, 'gamma': 0.40878572382087913, 'reg_alpha': 2.0696667884110242, 'reg_lambda': 9.351338318698286}. Best is trial 25 with value: 1.4226106366046956.\n",
      "[I 2024-08-06 18:11:11,174] Trial 40 finished with value: 1.4666978079284883 and parameters: {'n_estimators': 145, 'max_depth': 6, 'learning_rate': 0.2138990678316653, 'subsample': 0.5051942988521203, 'colsample_bytree': 0.9926011531633355, 'gamma': 1.087749157713651, 'reg_alpha': 5.531018594981912, 'reg_lambda': 8.561879912948344}. Best is trial 25 with value: 1.4226106366046956.\n",
      "[I 2024-08-06 18:11:11,900] Trial 41 finished with value: 1.4408111503632772 and parameters: {'n_estimators': 132, 'max_depth': 4, 'learning_rate': 0.14148265876290755, 'subsample': 0.6089331230545385, 'colsample_bytree': 0.8895065224922115, 'gamma': 0.8041522458700098, 'reg_alpha': 0.0033964030348641594, 'reg_lambda': 9.83739353820999}. Best is trial 25 with value: 1.4226106366046956.\n",
      "[I 2024-08-06 18:11:12,926] Trial 42 finished with value: 1.4518580303671955 and parameters: {'n_estimators': 205, 'max_depth': 4, 'learning_rate': 0.11466134800245271, 'subsample': 0.58733407250238, 'colsample_bytree': 0.936193939761012, 'gamma': 1.3716359093383519, 'reg_alpha': 0.503484165766715, 'reg_lambda': 9.293924949477399}. Best is trial 25 with value: 1.4226106366046956.\n",
      "[I 2024-08-06 18:11:13,699] Trial 43 finished with value: 1.444969701840316 and parameters: {'n_estimators': 129, 'max_depth': 5, 'learning_rate': 0.16788974090599423, 'subsample': 0.6160779658280537, 'colsample_bytree': 0.8888640391460993, 'gamma': 0.2295046071107797, 'reg_alpha': 1.1985796787895202, 'reg_lambda': 9.913040121877142}. Best is trial 25 with value: 1.4226106366046956.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 18:11:15,122] Trial 44 finished with value: 1.4366012382189823 and parameters: {'n_estimators': 322, 'max_depth': 4, 'learning_rate': 0.09224084801682549, 'subsample': 0.7500545677996568, 'colsample_bytree': 0.7810632111210705, 'gamma': 0.6166717432587177, 'reg_alpha': 0.013198018431158753, 'reg_lambda': 2.7884395657895062}. Best is trial 25 with value: 1.4226106366046956.\n",
      "[I 2024-08-06 18:11:16,456] Trial 45 finished with value: 1.417512156825367 and parameters: {'n_estimators': 321, 'max_depth': 3, 'learning_rate': 0.09899706043120735, 'subsample': 0.8081585518033757, 'colsample_bytree': 0.542748287896859, 'gamma': 0.6421637682708077, 'reg_alpha': 0.7227399764561786, 'reg_lambda': 3.997981074506276}. Best is trial 45 with value: 1.417512156825367.\n",
      "[I 2024-08-06 18:11:17,939] Trial 46 finished with value: 1.4601399072850456 and parameters: {'n_estimators': 400, 'max_depth': 3, 'learning_rate': 0.07316796462417548, 'subsample': 0.8302773791090041, 'colsample_bytree': 0.6000984644374703, 'gamma': 2.088174403121261, 'reg_alpha': 1.4765649941284034, 'reg_lambda': 3.5316222124063916}. Best is trial 45 with value: 1.417512156825367.\n",
      "[I 2024-08-06 18:11:19,047] Trial 47 finished with value: 1.4494751641060064 and parameters: {'n_estimators': 278, 'max_depth': 3, 'learning_rate': 0.06028000357108278, 'subsample': 0.8396922788637442, 'colsample_bytree': 0.5107234285446904, 'gamma': 0.4224040272878139, 'reg_alpha': 2.016712086504712, 'reg_lambda': 4.443586243883435}. Best is trial 45 with value: 1.417512156825367.\n",
      "[I 2024-08-06 18:11:20,177] Trial 48 finished with value: 1.4517828251649865 and parameters: {'n_estimators': 254, 'max_depth': 5, 'learning_rate': 0.09848061666710253, 'subsample': 0.7723099624907388, 'colsample_bytree': 0.55561611529715, 'gamma': 1.2827272418094844, 'reg_alpha': 3.1154327194501223, 'reg_lambda': 5.379967188930236}. Best is trial 45 with value: 1.417512156825367.\n",
      "[I 2024-08-06 18:11:21,555] Trial 49 finished with value: 1.47270399818661 and parameters: {'n_estimators': 327, 'max_depth': 3, 'learning_rate': 0.025639189149202407, 'subsample': 0.8616161884444494, 'colsample_bytree': 0.6544717513419669, 'gamma': 1.6491646539517941, 'reg_alpha': 0.7321706769811096, 'reg_lambda': 2.9734801086449485}. Best is trial 45 with value: 1.417512156825367.\n",
      "Best hyperparameters:  {'n_estimators': 321, 'max_depth': 3, 'learning_rate': 0.09899706043120735, 'subsample': 0.8081585518033757, 'colsample_bytree': 0.542748287896859, 'gamma': 0.6421637682708077, 'reg_alpha': 0.7227399764561786, 'reg_lambda': 3.997981074506276}\n",
      "Performance with tuned hyperparameters:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  36.644476334253945\n",
      "Mean Squared Error (MSE):  4924.651669635254\n",
      "R-squared (R2):  0.7628094884252068\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  50.85375169754028\n",
      "Mean Squared Error (MSE):  14177.908055155269\n",
      "R-squared (R2):  0.47273364627296655\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  48.96247000209584\n",
      "Mean Squared Error (MSE):  10728.283319639326\n",
      "R-squared (R2):  0.5272717884651876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'date']\n",
    "\n",
    "# Prepare the data by dropping unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the features and the target variable\n",
    "def scale_features_and_target(X, y, excluded_columns):\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    y_scaled = scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "    return X_scaled, y_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train_raw, y_train_raw = prepare_data(train_data_paris, 'price')\n",
    "X_valid_raw, y_valid_raw = prepare_data(valid_data_paris, 'price')\n",
    "X_test_raw, y_test_raw = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Apply scaling to both features and target\n",
    "X_train, y_train, scaler = scale_features_and_target(X_train_raw, y_train_raw, excluded_columns)\n",
    "X_valid, y_valid, _ = scale_features_and_target(X_valid_raw, y_valid_raw, excluded_columns)\n",
    "X_test, y_test, _ = scale_features_and_target(X_test_raw, y_test_raw, excluded_columns)\n",
    "\n",
    "# Save the scaler\n",
    "with open('scaler_without_LSTM.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save the scaled datasets\n",
    "scaled_train_df = pd.DataFrame(X_train, columns=X_train_raw.columns)\n",
    "scaled_train_df['price'] = y_train\n",
    "scaled_valid_df = pd.DataFrame(X_valid, columns=X_valid_raw.columns)\n",
    "scaled_valid_df['price'] = y_valid\n",
    "scaled_test_df = pd.DataFrame(X_test, columns=X_test_raw.columns)\n",
    "scaled_test_df['price'] = y_test\n",
    "\n",
    "scaled_train_df.to_csv('scaled_train_data_without_LSTM.csv', index=False)\n",
    "scaled_valid_df.to_csv('scaled_valid_data_without_LSTM.csv', index=False)\n",
    "scaled_test_df.to_csv('scaled_test_data_without_LSTM.csv', index=False)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10)\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        eval_metric='rmse',\n",
    "        **param\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "    preds = model.predict(X_valid)\n",
    "    mse = mean_squared_error(y_valid, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    eval_metric='rmse',\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_scaled = best_model.predict(X_train)\n",
    "y_valid_pred_scaled = best_model.predict(X_valid)\n",
    "y_test_pred_scaled = best_model.predict(X_test)\n",
    "\n",
    "# Inverse transform the scaled predictions and actual target values\n",
    "y_train_pred = scaler.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_valid_pred = scaler.inverse_transform(y_valid_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_pred = scaler.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_train_orig = scaler.inverse_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_valid_orig = scaler.inverse_transform(y_valid.reshape(-1, 1)).flatten()\n",
    "y_test_orig = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate performance metrics on the original scale\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with tuned hyperparameters:\")\n",
    "print_metrics(y_train_orig, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid_orig, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test_orig, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad0b6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'xgb_model_without_lstm_paris.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('xgb_model_without_lstm_paris.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8ec9e2",
   "metadata": {},
   "source": [
    "## With LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eb980a",
   "metadata": {},
   "source": [
    "#### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bca3cd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 18:23:53,969] A new study created in memory with name: no-name-4dbe4952-dfc8-4d41-93b3-4e6ea91f2089\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f516aee339864dd58db92d929d129e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 18:23:55,018] Trial 0 finished with value: 0.3353086895516498 and parameters: {'n_estimators': 233, 'max_depth': 5, 'learning_rate': 0.25502366387164443, 'subsample': 0.6759669880650095, 'colsample_bytree': 0.650652874871725, 'gamma': 4.562184165527251, 'reg_alpha': 6.639510348260748, 'reg_lambda': 3.0769978958667012}. Best is trial 0 with value: 0.3353086895516498.\n",
      "[I 2024-08-06 18:23:57,430] Trial 1 finished with value: 0.3781541654017212 and parameters: {'n_estimators': 662, 'max_depth': 4, 'learning_rate': 0.15213057015167372, 'subsample': 0.9600225967570184, 'colsample_bytree': 0.6112179512122483, 'gamma': 4.009606764544883, 'reg_alpha': 1.9845493371756262, 'reg_lambda': 5.020996051198361}. Best is trial 0 with value: 0.3353086895516498.\n",
      "[I 2024-08-06 18:23:58,575] Trial 2 finished with value: 0.38488840143621983 and parameters: {'n_estimators': 283, 'max_depth': 7, 'learning_rate': 0.11890563868635161, 'subsample': 0.8486468580365027, 'colsample_bytree': 0.9634396356046351, 'gamma': 0.8149009269552265, 'reg_alpha': 9.138022832490245, 'reg_lambda': 9.805533692139882}. Best is trial 0 with value: 0.3353086895516498.\n",
      "[I 2024-08-06 18:24:01,073] Trial 3 finished with value: 0.3340492980381625 and parameters: {'n_estimators': 657, 'max_depth': 9, 'learning_rate': 0.03669470418001082, 'subsample': 0.7881525538900491, 'colsample_bytree': 0.790749735487757, 'gamma': 4.3857245070241575, 'reg_alpha': 2.1942936540934177, 'reg_lambda': 7.108556157298947}. Best is trial 3 with value: 0.3340492980381625.\n",
      "[I 2024-08-06 18:24:04,188] Trial 4 finished with value: 0.33764825947044597 and parameters: {'n_estimators': 931, 'max_depth': 3, 'learning_rate': 0.04405571542605234, 'subsample': 0.6755078382801829, 'colsample_bytree': 0.7201776999967828, 'gamma': 2.6907216141611605, 'reg_alpha': 6.761182084727819, 'reg_lambda': 6.882463369069739}. Best is trial 3 with value: 0.3340492980381625.\n",
      "[I 2024-08-06 18:24:05,590] Trial 5 finished with value: 0.3557185994268997 and parameters: {'n_estimators': 386, 'max_depth': 6, 'learning_rate': 0.18260798224484553, 'subsample': 0.9919440862113519, 'colsample_bytree': 0.6487476924125316, 'gamma': 0.4801470759829207, 'reg_alpha': 1.1951296513212717, 'reg_lambda': 6.431582082051744}. Best is trial 3 with value: 0.3340492980381625.\n",
      "[I 2024-08-06 18:24:08,434] Trial 6 finished with value: 0.4532178026943828 and parameters: {'n_estimators': 913, 'max_depth': 9, 'learning_rate': 0.2758397352494297, 'subsample': 0.7285086007633806, 'colsample_bytree': 0.9033903113039811, 'gamma': 2.3911412211946024, 'reg_alpha': 1.6049115687236648, 'reg_lambda': 0.9291027132582386}. Best is trial 3 with value: 0.3340492980381625.\n",
      "[I 2024-08-06 18:24:09,360] Trial 7 finished with value: 0.3757248743901243 and parameters: {'n_estimators': 202, 'max_depth': 7, 'learning_rate': 0.21355028209533897, 'subsample': 0.912809698711998, 'colsample_bytree': 0.7387781098030478, 'gamma': 3.9188323571643977, 'reg_alpha': 4.167459232799838, 'reg_lambda': 8.158539926000909}. Best is trial 3 with value: 0.3340492980381625.\n",
      "[I 2024-08-06 18:24:12,133] Trial 8 finished with value: 0.27629280400719997 and parameters: {'n_estimators': 729, 'max_depth': 3, 'learning_rate': 0.2645676826608423, 'subsample': 0.7574586834656667, 'colsample_bytree': 0.6613892730703705, 'gamma': 3.057070614248113, 'reg_alpha': 2.708501450816482, 'reg_lambda': 7.477318172315517}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:24:14,802] Trial 9 finished with value: 0.32161235801198074 and parameters: {'n_estimators': 785, 'max_depth': 3, 'learning_rate': 0.1630997217693443, 'subsample': 0.6944818236620068, 'colsample_bytree': 0.9248727405255263, 'gamma': 1.4228983950484408, 'reg_alpha': 2.0493069842864275, 'reg_lambda': 0.4323721556898952}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:24:16,472] Trial 10 finished with value: 0.3027856045345024 and parameters: {'n_estimators': 478, 'max_depth': 5, 'learning_rate': 0.2982243022881718, 'subsample': 0.5158420198821875, 'colsample_bytree': 0.5298510818429325, 'gamma': 2.9875958883087, 'reg_alpha': 4.245346965741749, 'reg_lambda': 3.782714427635141}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:24:18,079] Trial 11 finished with value: 0.3139546167556089 and parameters: {'n_estimators': 472, 'max_depth': 5, 'learning_rate': 0.29864815314757837, 'subsample': 0.5733043897299013, 'colsample_bytree': 0.5116614887418564, 'gamma': 3.003425783254106, 'reg_alpha': 4.3874259558859565, 'reg_lambda': 3.529982025296436}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:24:19,982] Trial 12 finished with value: 0.35905719369625944 and parameters: {'n_estimators': 569, 'max_depth': 4, 'learning_rate': 0.2317312111117918, 'subsample': 0.513817191388227, 'colsample_bytree': 0.5003238944090195, 'gamma': 3.200794842855136, 'reg_alpha': 3.626674556558239, 'reg_lambda': 4.610413745271071}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:24:22,422] Trial 13 finished with value: 0.34486894552400726 and parameters: {'n_estimators': 763, 'max_depth': 5, 'learning_rate': 0.29600092627818675, 'subsample': 0.5990590036640269, 'colsample_bytree': 0.5781172101939689, 'gamma': 1.9872637014953032, 'reg_alpha': 6.106270080149795, 'reg_lambda': 9.611605955223405}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:24:23,907] Trial 14 finished with value: 0.35295924489706454 and parameters: {'n_estimators': 422, 'max_depth': 3, 'learning_rate': 0.23703179536036678, 'subsample': 0.7906010151379937, 'colsample_bytree': 0.8235778652779897, 'gamma': 3.5069847165407575, 'reg_alpha': 0.05338721370396193, 'reg_lambda': 2.283273317415973}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:24:26,300] Trial 15 finished with value: 0.3415467742323225 and parameters: {'n_estimators': 552, 'max_depth': 6, 'learning_rate': 0.20045201300296595, 'subsample': 0.6274580057885681, 'colsample_bytree': 0.5599522893519884, 'gamma': 1.638813875337473, 'reg_alpha': 3.1703949809937746, 'reg_lambda': 5.185070369029957}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:24:29,637] Trial 16 finished with value: 0.3380791854144525 and parameters: {'n_estimators': 794, 'max_depth': 4, 'learning_rate': 0.10787815242994175, 'subsample': 0.5086020526395263, 'colsample_bytree': 0.6753739707097332, 'gamma': 2.3363314923177856, 'reg_alpha': 5.575973427942491, 'reg_lambda': 8.046676299502158}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:24:31,986] Trial 17 finished with value: 0.36392779545773524 and parameters: {'n_estimators': 677, 'max_depth': 8, 'learning_rate': 0.26030366447169634, 'subsample': 0.8567726890818878, 'colsample_bytree': 0.5697412554614686, 'gamma': 3.5187390316130314, 'reg_alpha': 8.05470052419259, 'reg_lambda': 5.617159866606966}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:24:35,096] Trial 18 finished with value: 0.35027672627939843 and parameters: {'n_estimators': 998, 'max_depth': 10, 'learning_rate': 0.26764921716901413, 'subsample': 0.7613892252924753, 'colsample_bytree': 0.6947114139515552, 'gamma': 4.846394005989065, 'reg_alpha': 2.9766019976501283, 'reg_lambda': 3.813826278509163}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:24:36,400] Trial 19 finished with value: 0.3886147644846576 and parameters: {'n_estimators': 328, 'max_depth': 4, 'learning_rate': 0.08185437360129061, 'subsample': 0.8215080933683124, 'colsample_bytree': 0.6127764538035165, 'gamma': 2.8280286078260435, 'reg_alpha': 4.832187813050224, 'reg_lambda': 1.712529854878733}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:24:38,056] Trial 20 finished with value: 0.30889947139093066 and parameters: {'n_estimators': 471, 'max_depth': 5, 'learning_rate': 0.22492587609229175, 'subsample': 0.8974744890372692, 'colsample_bytree': 0.7859821222954243, 'gamma': 1.326674483912581, 'reg_alpha': 0.024167616292645633, 'reg_lambda': 8.265299587064778}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:24:38,760] Trial 21 finished with value: 0.3311668910959296 and parameters: {'n_estimators': 130, 'max_depth': 5, 'learning_rate': 0.23237780187789742, 'subsample': 0.9179240299581372, 'colsample_bytree': 0.8099019659694427, 'gamma': 0.9834737851439384, 'reg_alpha': 0.060491716821792586, 'reg_lambda': 8.333852266855974}. Best is trial 8 with value: 0.27629280400719997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 18:24:40,635] Trial 22 finished with value: 0.2828133155164741 and parameters: {'n_estimators': 546, 'max_depth': 6, 'learning_rate': 0.28256775450615446, 'subsample': 0.9105496431901446, 'colsample_bytree': 0.8572105301170079, 'gamma': 1.9682581896172673, 'reg_alpha': 0.7937368528980129, 'reg_lambda': 8.579693595633799}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:24:42,880] Trial 23 finished with value: 0.37769981774917644 and parameters: {'n_estimators': 564, 'max_depth': 6, 'learning_rate': 0.2848168998461879, 'subsample': 0.7227353347705556, 'colsample_bytree': 0.8618753381185758, 'gamma': 2.026369725117418, 'reg_alpha': 1.1140102282173396, 'reg_lambda': 9.184304312714236}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:24:45,637] Trial 24 finished with value: 0.3715028852104971 and parameters: {'n_estimators': 720, 'max_depth': 7, 'learning_rate': 0.2547686820251817, 'subsample': 0.6470882799921862, 'colsample_bytree': 0.9964777631699607, 'gamma': 1.9467932017323113, 'reg_alpha': 2.793188322920593, 'reg_lambda': 6.093462319664609}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:24:47,723] Trial 25 finished with value: 0.387117484548336 and parameters: {'n_estimators': 617, 'max_depth': 8, 'learning_rate': 0.29764965170424823, 'subsample': 0.5688084015891199, 'colsample_bytree': 0.7593272749154307, 'gamma': 3.3257097724201827, 'reg_alpha': 0.9537036527723443, 'reg_lambda': 7.406094884405903}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:24:49,453] Trial 26 finished with value: 0.318164279828525 and parameters: {'n_estimators': 490, 'max_depth': 6, 'learning_rate': 0.27092576410159236, 'subsample': 0.8739824463019296, 'colsample_bytree': 0.5431147974444993, 'gamma': 2.5140996793356125, 'reg_alpha': 3.779943123927526, 'reg_lambda': 8.985962257240393}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:24:50,850] Trial 27 finished with value: 0.3476572134878831 and parameters: {'n_estimators': 368, 'max_depth': 3, 'learning_rate': 0.19355663646971166, 'subsample': 0.9607392807712247, 'colsample_bytree': 0.8615474195761894, 'gamma': 0.1746957023925173, 'reg_alpha': 5.113387796437811, 'reg_lambda': 4.0874078338405}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:24:53,590] Trial 28 finished with value: 0.34081239601501107 and parameters: {'n_estimators': 855, 'max_depth': 4, 'learning_rate': 0.2441719688835387, 'subsample': 0.794774273276024, 'colsample_bytree': 0.6282710526801416, 'gamma': 3.8896320089532326, 'reg_alpha': 2.701608878424061, 'reg_lambda': 2.82318684879748}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:24:55,327] Trial 29 finished with value: 0.3466218141165481 and parameters: {'n_estimators': 494, 'max_depth': 6, 'learning_rate': 0.2532888081657373, 'subsample': 0.752924032226514, 'colsample_bytree': 0.690763181087487, 'gamma': 2.9929516778934806, 'reg_alpha': 6.565448907460405, 'reg_lambda': 6.0708216762041545}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:24:57,468] Trial 30 finished with value: 0.3659971514960937 and parameters: {'n_estimators': 621, 'max_depth': 8, 'learning_rate': 0.2759413436847091, 'subsample': 0.8275827054596595, 'colsample_bytree': 0.8794667115604674, 'gamma': 2.189500374286722, 'reg_alpha': 7.538488268490815, 'reg_lambda': 7.55694178914377}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:24:59,522] Trial 31 finished with value: 0.3313892755487508 and parameters: {'n_estimators': 451, 'max_depth': 5, 'learning_rate': 0.21954770189955877, 'subsample': 0.897758847014792, 'colsample_bytree': 0.7617342094644661, 'gamma': 1.4576424173705378, 'reg_alpha': 0.6353750825723927, 'reg_lambda': 8.69688458228365}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:25:01,645] Trial 32 finished with value: 0.32824620030694196 and parameters: {'n_estimators': 503, 'max_depth': 5, 'learning_rate': 0.28176691539269444, 'subsample': 0.9451182224023565, 'colsample_bytree': 0.8226673250877271, 'gamma': 1.0483480960318654, 'reg_alpha': 0.5237268046860324, 'reg_lambda': 7.699771866625445}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:25:02,735] Trial 33 finished with value: 0.36862960788278204 and parameters: {'n_estimators': 262, 'max_depth': 4, 'learning_rate': 0.21846969397007981, 'subsample': 0.884404068719685, 'colsample_bytree': 0.7816793689648297, 'gamma': 1.5363590557503257, 'reg_alpha': 1.7184267092270895, 'reg_lambda': 9.922075776758604}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:25:04,826] Trial 34 finished with value: 0.32883965305227925 and parameters: {'n_estimators': 605, 'max_depth': 5, 'learning_rate': 0.15462297859210455, 'subsample': 0.9358418173561913, 'colsample_bytree': 0.8442169093291116, 'gamma': 1.140542668976308, 'reg_alpha': 2.31193574717423, 'reg_lambda': 4.648492780050631}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:25:07,110] Trial 35 finished with value: 0.3550752836301944 and parameters: {'n_estimators': 698, 'max_depth': 7, 'learning_rate': 0.24857789425825508, 'subsample': 0.9720039949389472, 'colsample_bytree': 0.7258772995668339, 'gamma': 2.658718371270043, 'reg_alpha': 1.3269643473884183, 'reg_lambda': 8.663790848636332}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:25:08,639] Trial 36 finished with value: 0.32668335026236633 and parameters: {'n_estimators': 410, 'max_depth': 6, 'learning_rate': 0.2651780043212341, 'subsample': 0.8250649271437877, 'colsample_bytree': 0.6497241374767896, 'gamma': 1.8371717235818241, 'reg_alpha': 3.4868248499113728, 'reg_lambda': 6.940818491068309}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:25:09,916] Trial 37 finished with value: 0.33813377304403536 and parameters: {'n_estimators': 331, 'max_depth': 3, 'learning_rate': 0.12502430178344115, 'subsample': 0.9990240538531587, 'colsample_bytree': 0.9363484469629526, 'gamma': 0.6679642207205995, 'reg_alpha': 0.5824486437138878, 'reg_lambda': 9.248806794580545}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:25:11,918] Trial 38 finished with value: 0.33501099935766104 and parameters: {'n_estimators': 526, 'max_depth': 4, 'learning_rate': 0.02123909100192453, 'subsample': 0.6918108338039146, 'colsample_bytree': 0.6021744995688431, 'gamma': 4.323967385466117, 'reg_alpha': 2.2546595832252447, 'reg_lambda': 6.33609917828985}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:25:13,607] Trial 39 finished with value: 0.3352216878913815 and parameters: {'n_estimators': 448, 'max_depth': 7, 'learning_rate': 0.18572283719766775, 'subsample': 0.8614786312118885, 'colsample_bytree': 0.7815592504745127, 'gamma': 3.658801395036728, 'reg_alpha': 1.706416273489503, 'reg_lambda': 6.68681252789276}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:25:16,149] Trial 40 finished with value: 0.35112959015134504 and parameters: {'n_estimators': 637, 'max_depth': 5, 'learning_rate': 0.2864746501472033, 'subsample': 0.6645893705327017, 'colsample_bytree': 0.7046525964402924, 'gamma': 1.2462928284285641, 'reg_alpha': 9.829455164221375, 'reg_lambda': 5.583311147412045}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:25:17,873] Trial 41 finished with value: 0.31324706635442184 and parameters: {'n_estimators': 442, 'max_depth': 5, 'learning_rate': 0.299578731418685, 'subsample': 0.5747305716919846, 'colsample_bytree': 0.5056768952401656, 'gamma': 3.012447438247564, 'reg_alpha': 4.433662815569409, 'reg_lambda': 3.3801133337534672}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:25:19,191] Trial 42 finished with value: 0.38659933284414394 and parameters: {'n_estimators': 357, 'max_depth': 5, 'learning_rate': 0.2998804975895225, 'subsample': 0.5359084160394082, 'colsample_bytree': 0.530995963386438, 'gamma': 2.955657087555732, 'reg_alpha': 4.329665645919632, 'reg_lambda': 2.92573237081293}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:25:21,013] Trial 43 finished with value: 0.35757787504666816 and parameters: {'n_estimators': 543, 'max_depth': 6, 'learning_rate': 0.2806032463598931, 'subsample': 0.5443893510492169, 'colsample_bytree': 0.525135528083502, 'gamma': 3.2171660838947975, 'reg_alpha': 5.066498203178843, 'reg_lambda': 3.5334627229005178}. Best is trial 8 with value: 0.27629280400719997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 18:25:22,551] Trial 44 finished with value: 0.3654758675422386 and parameters: {'n_estimators': 431, 'max_depth': 5, 'learning_rate': 0.1725534931243119, 'subsample': 0.5611437389758526, 'colsample_bytree': 0.5021126213873072, 'gamma': 2.611619461899239, 'reg_alpha': 3.9562165406538017, 'reg_lambda': 2.299417342710411}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:25:23,941] Trial 45 finished with value: 0.33684352070302087 and parameters: {'n_estimators': 388, 'max_depth': 4, 'learning_rate': 0.2874202057743957, 'subsample': 0.5947346252293598, 'colsample_bytree': 0.5895927509680828, 'gamma': 2.364255307099701, 'reg_alpha': 5.660324769269286, 'reg_lambda': 4.065384190713934}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:25:26,033] Trial 46 finished with value: 0.3525070123612434 and parameters: {'n_estimators': 575, 'max_depth': 6, 'learning_rate': 0.2671460521059218, 'subsample': 0.5014159019541012, 'colsample_bytree': 0.5576958613457637, 'gamma': 1.7882086848807326, 'reg_alpha': 3.3964354951171574, 'reg_lambda': 4.500162464420667}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:25:28,593] Trial 47 finished with value: 0.39863966637242637 and parameters: {'n_estimators': 738, 'max_depth': 3, 'learning_rate': 0.20812123473406632, 'subsample': 0.6093847849784073, 'colsample_bytree': 0.894666372193575, 'gamma': 2.760041432491997, 'reg_alpha': 4.595991176465787, 'reg_lambda': 1.283693555327675}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:25:29,891] Trial 48 finished with value: 0.3569791089923177 and parameters: {'n_estimators': 290, 'max_depth': 7, 'learning_rate': 0.23930841267144823, 'subsample': 0.7235345172597436, 'colsample_bytree': 0.6644929385556381, 'gamma': 2.223527921760681, 'reg_alpha': 0.025021388384564327, 'reg_lambda': 2.259874793701155}. Best is trial 8 with value: 0.27629280400719997.\n",
      "[I 2024-08-06 18:25:33,473] Trial 49 finished with value: 0.3226412188254236 and parameters: {'n_estimators': 852, 'max_depth': 4, 'learning_rate': 0.22710701539030917, 'subsample': 0.7715624900144988, 'colsample_bytree': 0.7992114698332585, 'gamma': 4.138806460857131, 'reg_alpha': 1.372213678529111, 'reg_lambda': 8.08403649005986}. Best is trial 8 with value: 0.27629280400719997.\n",
      "Best hyperparameters:  {'n_estimators': 729, 'max_depth': 3, 'learning_rate': 0.2645676826608423, 'subsample': 0.7574586834656667, 'colsample_bytree': 0.6613892730703705, 'gamma': 3.057070614248113, 'reg_alpha': 2.708501450816482, 'reg_lambda': 7.477318172315517}\n",
      "Performance with original scale:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  6.505569696426392\n",
      "Mean Squared Error (MSE):  392.35060487230567\n",
      "R-squared (R2):  0.9811028582467775\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  6.456500926971436\n",
      "Mean Squared Error (MSE):  538.6389038415542\n",
      "R-squared (R2):  0.9799684008600412\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  6.9422107796164125\n",
      "Mean Squared Error (MSE):  455.8693024884253\n",
      "R-squared (R2):  0.9799126967811826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'date']\n",
    "\n",
    "# Prepare the data by separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the features and the target variable\n",
    "def scale_features_and_target(X, y, excluded_columns):\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    y_scaled = scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "    return X_scaled, y_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets for LSTM data\n",
    "X_train_raw, y_train_raw = prepare_data(train_data_lstm_paris, 'price')\n",
    "X_valid_raw, y_valid_raw = prepare_data(valid_data_lstm_paris, 'price')\n",
    "X_test_raw, y_test_raw = prepare_data(test_data_lstm_paris, 'price')\n",
    "\n",
    "# Apply scaling to both features and target\n",
    "X_train_scaled, y_train_scaled, scaler = scale_features_and_target(X_train_raw, y_train_raw, excluded_columns)\n",
    "X_valid_scaled, y_valid_scaled, _ = scale_features_and_target(X_valid_raw, y_valid_raw, excluded_columns)\n",
    "X_test_scaled, y_test_scaled, _ = scale_features_and_target(X_test_raw, y_test_raw, excluded_columns)\n",
    "\n",
    "# Save the scaler\n",
    "with open('scaler_with_LSTM.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save the scaled datasets\n",
    "scaled_train_df = pd.DataFrame(X_train_scaled, columns=X_train_raw.columns)\n",
    "scaled_train_df['price'] = y_train_scaled\n",
    "scaled_valid_df = pd.DataFrame(X_valid_scaled, columns=X_valid_raw.columns)\n",
    "scaled_valid_df['price'] = y_valid_scaled\n",
    "scaled_test_df = pd.DataFrame(X_test_scaled, columns=X_test_raw.columns)\n",
    "scaled_test_df['price'] = y_test_scaled\n",
    "\n",
    "scaled_train_df.to_csv('scaled_train_data_with_LSTM.csv', index=False)\n",
    "scaled_valid_df.to_csv('scaled_valid_data_with_LSTM.csv', index=False)\n",
    "scaled_test_df.to_csv('scaled_test_data_with_LSTM.csv', index=False)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10)\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        eval_metric='rmse',  # Use rmse for evaluation\n",
    "        **param\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train_scaled, eval_set=[(X_valid_scaled, y_valid_scaled)], verbose=False)\n",
    "    preds = model.predict(X_valid_scaled)\n",
    "    mse = mean_squared_error(y_valid_scaled, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    eval_metric='rmse',  # Use rmse for evaluation\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "best_model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_scaled = best_model.predict(X_train_scaled)\n",
    "y_valid_pred_scaled = best_model.predict(X_valid_scaled)\n",
    "y_test_pred_scaled = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Inverse transform the scaled predictions and actual target values\n",
    "y_train_pred = scaler.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_valid_pred = scaler.inverse_transform(y_valid_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_pred = scaler.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_train_orig = scaler.inverse_transform(y_train_scaled.reshape(-1, 1)).flatten()\n",
    "y_valid_orig = scaler.inverse_transform(y_valid_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_orig = scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate performance metrics on the original scale\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with original scale:\")\n",
    "print_metrics(y_train_orig, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid_orig, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test_orig, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f6d725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'xgb_model_with_lstm_paris.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('xgb_model_with_lstm_paris.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1948251",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c38a99",
   "metadata": {},
   "source": [
    "## Without LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce82fca7",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3fc0c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:36:21,822] A new study created in memory with name: no-name-7df8ca75-d71c-4d8d-a62a-53c4e1727c42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0216307501c04709a79d75ad0db5d39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:36:22,797] Trial 0 finished with value: 2.827298646556331 and parameters: {'n_estimators': 323, 'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 15, 'max_features': 'log2'}. Best is trial 0 with value: 2.827298646556331.\n",
      "[I 2024-08-06 17:36:24,720] Trial 1 finished with value: 2.44157148806668 and parameters: {'n_estimators': 272, 'max_depth': 15, 'min_samples_split': 6, 'min_samples_leaf': 14, 'max_features': 'log2'}. Best is trial 1 with value: 2.44157148806668.\n",
      "[I 2024-08-06 17:36:50,832] Trial 2 finished with value: 2.3373075512466723 and parameters: {'n_estimators': 493, 'max_depth': 7, 'min_samples_split': 13, 'min_samples_leaf': 13, 'max_features': None}. Best is trial 2 with value: 2.3373075512466723.\n",
      "[I 2024-08-06 17:36:51,348] Trial 3 finished with value: 2.8676727885956206 and parameters: {'n_estimators': 230, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 2 with value: 2.3373075512466723.\n",
      "[I 2024-08-06 17:36:52,409] Trial 4 finished with value: 2.6912910731484616 and parameters: {'n_estimators': 334, 'max_depth': 3, 'min_samples_split': 12, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 2 with value: 2.3373075512466723.\n",
      "[I 2024-08-06 17:36:54,935] Trial 5 finished with value: 2.423054499599359 and parameters: {'n_estimators': 496, 'max_depth': 6, 'min_samples_split': 12, 'min_samples_leaf': 13, 'max_features': 'sqrt'}. Best is trial 2 with value: 2.3373075512466723.\n",
      "[I 2024-08-06 17:36:55,836] Trial 6 finished with value: 2.3377996686091485 and parameters: {'n_estimators': 156, 'max_depth': 11, 'min_samples_split': 11, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 2 with value: 2.3373075512466723.\n",
      "[I 2024-08-06 17:36:58,270] Trial 7 finished with value: 2.4724542681806825 and parameters: {'n_estimators': 437, 'max_depth': 11, 'min_samples_split': 11, 'min_samples_leaf': 15, 'max_features': 'log2'}. Best is trial 2 with value: 2.3373075512466723.\n",
      "[I 2024-08-06 17:36:58,599] Trial 8 finished with value: 2.719377737235231 and parameters: {'n_estimators': 107, 'max_depth': 4, 'min_samples_split': 15, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 2 with value: 2.3373075512466723.\n",
      "[I 2024-08-06 17:37:10,195] Trial 9 finished with value: 2.447030640270381 and parameters: {'n_estimators': 456, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 13, 'max_features': None}. Best is trial 2 with value: 2.3373075512466723.\n",
      "[I 2024-08-06 17:37:32,583] Trial 10 finished with value: 2.332099855984007 and parameters: {'n_estimators': 396, 'max_depth': 8, 'min_samples_split': 15, 'min_samples_leaf': 11, 'max_features': None}. Best is trial 10 with value: 2.332099855984007.\n",
      "[I 2024-08-06 17:37:55,795] Trial 11 finished with value: 2.323171564733915 and parameters: {'n_estimators': 392, 'max_depth': 8, 'min_samples_split': 15, 'min_samples_leaf': 11, 'max_features': None}. Best is trial 11 with value: 2.323171564733915.\n",
      "[I 2024-08-06 17:38:18,681] Trial 12 finished with value: 2.308972164484823 and parameters: {'n_estimators': 384, 'max_depth': 9, 'min_samples_split': 15, 'min_samples_leaf': 10, 'max_features': None}. Best is trial 12 with value: 2.308972164484823.\n",
      "[I 2024-08-06 17:38:50,226] Trial 13 finished with value: 2.2971718069316958 and parameters: {'n_estimators': 391, 'max_depth': 10, 'min_samples_split': 14, 'min_samples_leaf': 9, 'max_features': None}. Best is trial 13 with value: 2.2971718069316958.\n",
      "[I 2024-08-06 17:39:21,617] Trial 14 finished with value: 2.295238156616026 and parameters: {'n_estimators': 370, 'max_depth': 11, 'min_samples_split': 13, 'min_samples_leaf': 9, 'max_features': None}. Best is trial 14 with value: 2.295238156616026.\n",
      "[I 2024-08-06 17:39:47,724] Trial 15 finished with value: 2.290741803366718 and parameters: {'n_estimators': 263, 'max_depth': 12, 'min_samples_split': 13, 'min_samples_leaf': 8, 'max_features': None}. Best is trial 15 with value: 2.290741803366718.\n",
      "[I 2024-08-06 17:40:08,822] Trial 16 finished with value: 2.2748925670885884 and parameters: {'n_estimators': 222, 'max_depth': 14, 'min_samples_split': 9, 'min_samples_leaf': 8, 'max_features': None}. Best is trial 16 with value: 2.2748925670885884.\n",
      "[I 2024-08-06 17:40:28,292] Trial 17 finished with value: 2.265663325010837 and parameters: {'n_estimators': 205, 'max_depth': 14, 'min_samples_split': 9, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 17 with value: 2.265663325010837.\n",
      "[I 2024-08-06 17:40:30,486] Trial 18 finished with value: 2.284050703866085 and parameters: {'n_estimators': 197, 'max_depth': 15, 'min_samples_split': 9, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 17 with value: 2.265663325010837.\n",
      "[I 2024-08-06 17:40:44,872] Trial 19 finished with value: 2.277916609204748 and parameters: {'n_estimators': 170, 'max_depth': 13, 'min_samples_split': 8, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 17 with value: 2.265663325010837.\n",
      "[I 2024-08-06 17:40:56,301] Trial 20 finished with value: 2.238168717471982 and parameters: {'n_estimators': 121, 'max_depth': 14, 'min_samples_split': 9, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 20 with value: 2.238168717471982.\n",
      "[I 2024-08-06 17:41:05,997] Trial 21 finished with value: 2.232787140144167 and parameters: {'n_estimators': 100, 'max_depth': 14, 'min_samples_split': 9, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:41:15,716] Trial 22 finished with value: 2.2484077759545023 and parameters: {'n_estimators': 102, 'max_depth': 13, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:41:24,801] Trial 23 finished with value: 2.2791683030400116 and parameters: {'n_estimators': 100, 'max_depth': 13, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:41:38,174] Trial 24 finished with value: 2.247367594410028 and parameters: {'n_estimators': 146, 'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:41:39,644] Trial 25 finished with value: 2.2556207449368375 and parameters: {'n_estimators': 140, 'max_depth': 14, 'min_samples_split': 7, 'min_samples_leaf': 6, 'max_features': 'sqrt'}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:41:51,762] Trial 26 finished with value: 2.2435472029686028 and parameters: {'n_estimators': 144, 'max_depth': 12, 'min_samples_split': 7, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:42:06,604] Trial 27 finished with value: 2.2722674437192736 and parameters: {'n_estimators': 179, 'max_depth': 12, 'min_samples_split': 8, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:42:18,776] Trial 28 finished with value: 2.284208341221213 and parameters: {'n_estimators': 135, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:42:20,142] Trial 29 finished with value: 2.263918278458804 and parameters: {'n_estimators': 131, 'max_depth': 12, 'min_samples_split': 6, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:42:40,880] Trial 30 finished with value: 2.289799959748988 and parameters: {'n_estimators': 306, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 8, 'max_features': None}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:42:53,513] Trial 31 finished with value: 2.2355167250337535 and parameters: {'n_estimators': 133, 'max_depth': 14, 'min_samples_split': 7, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:43:05,345] Trial 32 finished with value: 2.236684031230733 and parameters: {'n_estimators': 127, 'max_depth': 15, 'min_samples_split': 6, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 21 with value: 2.232787140144167.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:43:16,676] Trial 33 finished with value: 2.2572838233790558 and parameters: {'n_estimators': 121, 'max_depth': 15, 'min_samples_split': 6, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:43:33,059] Trial 34 finished with value: 2.2630035533337676 and parameters: {'n_estimators': 178, 'max_depth': 14, 'min_samples_split': 6, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:43:34,972] Trial 35 finished with value: 2.3097151927997985 and parameters: {'n_estimators': 260, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:43:49,569] Trial 36 finished with value: 2.27298027443858 and parameters: {'n_estimators': 164, 'max_depth': 14, 'min_samples_split': 8, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:44:12,550] Trial 37 finished with value: 2.2600637275463376 and parameters: {'n_estimators': 241, 'max_depth': 15, 'min_samples_split': 7, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:44:13,859] Trial 38 finished with value: 2.2699218816819084 and parameters: {'n_estimators': 121, 'max_depth': 13, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:44:15,200] Trial 39 finished with value: 2.35175728915173 and parameters: {'n_estimators': 200, 'max_depth': 14, 'min_samples_split': 11, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:44:27,121] Trial 40 finished with value: 2.2412253472824966 and parameters: {'n_estimators': 120, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:44:44,917] Trial 41 finished with value: 2.2706375593386703 and parameters: {'n_estimators': 117, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:45:08,831] Trial 42 finished with value: 2.2536155233243127 and parameters: {'n_estimators': 160, 'max_depth': 14, 'min_samples_split': 6, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:45:16,346] Trial 43 finished with value: 2.3320658926093136 and parameters: {'n_estimators': 122, 'max_depth': 6, 'min_samples_split': 5, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:45:28,992] Trial 44 finished with value: 2.3011654444418737 and parameters: {'n_estimators': 102, 'max_depth': 15, 'min_samples_split': 6, 'min_samples_leaf': 12, 'max_features': None}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:45:30,547] Trial 45 finished with value: 2.4774784258329148 and parameters: {'n_estimators': 153, 'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 15, 'max_features': 'log2'}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:45:46,633] Trial 46 finished with value: 2.2498761488485344 and parameters: {'n_estimators': 139, 'max_depth': 14, 'min_samples_split': 8, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:46:01,422] Trial 47 finished with value: 2.3166709935126475 and parameters: {'n_estimators': 189, 'max_depth': 15, 'min_samples_split': 9, 'min_samples_leaf': 14, 'max_features': None}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:46:20,421] Trial 48 finished with value: 2.257810717523078 and parameters: {'n_estimators': 218, 'max_depth': 12, 'min_samples_split': 11, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 21 with value: 2.232787140144167.\n",
      "[I 2024-08-06 17:46:21,332] Trial 49 finished with value: 2.3730675681862574 and parameters: {'n_estimators': 156, 'max_depth': 11, 'min_samples_split': 5, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 21 with value: 2.232787140144167.\n",
      "Best hyperparameters:  {'n_estimators': 100, 'max_depth': 14, 'min_samples_split': 9, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Performance with tuned hyperparameters:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  32.724046259755916\n",
      "Mean Squared Error (MSE):  5534.215659142715\n",
      "R-squared (R2):  0.7334504993620159\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  51.10326043151875\n",
      "Mean Squared Error (MSE):  15802.573221538216\n",
      "R-squared (R2):  0.4123135000162994\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  48.02129857291418\n",
      "Mean Squared Error (MSE):  10614.17164510828\n",
      "R-squared (R2):  0.5322999748216715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'date']\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the features and the target variable\n",
    "def scale_features_and_target(X, y, excluded_columns):\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    y_scaled = scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "    return X_scaled, y_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train_raw, y_train_raw = prepare_data(train_data_paris, 'price')\n",
    "X_valid_raw, y_valid_raw = prepare_data(valid_data_paris, 'price')\n",
    "X_test_raw, y_test_raw = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Apply scaling to both features and target\n",
    "X_train, y_train, scaler = scale_features_and_target(X_train_raw, y_train_raw, excluded_columns)\n",
    "X_valid, y_valid, _ = scale_features_and_target(X_valid_raw, y_valid_raw, excluded_columns)\n",
    "X_test, y_test, _ = scale_features_and_target(X_test_raw, y_test_raw, excluded_columns)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 5, 15),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5, 15),\n",
    "        'max_features': trial.suggest_categorical('max_features', [None, 'sqrt', 'log2'])\n",
    "    }\n",
    "\n",
    "    model = RandomForestRegressor(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    mse = mean_squared_error(y_valid, preds)\n",
    "    return mse\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = RandomForestRegressor(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_scaled = best_model.predict(X_train)\n",
    "y_valid_pred_scaled = best_model.predict(X_valid)\n",
    "y_test_pred_scaled = best_model.predict(X_test)\n",
    "\n",
    "# Inverse transform the scaled predictions and actual target values\n",
    "y_train_pred = scaler.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_valid_pred = scaler.inverse_transform(y_valid_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_pred = scaler.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_train_orig = scaler.inverse_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_valid_orig = scaler.inverse_transform(y_valid.reshape(-1, 1)).flatten()\n",
    "y_test_orig = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate performance metrics on the original scale\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with tuned hyperparameters:\")\n",
    "print_metrics(y_train_orig, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid_orig, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test_orig, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b68d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'rf_model_without_lstm_paris.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('rf_model_without_lstm_paris.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86e5681",
   "metadata": {},
   "source": [
    "## With LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ebac99",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "790941dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:54:19,859] A new study created in memory with name: no-name-0b7d51d4-bb58-4c94-9b9d-5f88d04b6c57\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ec802df8524221ba04d729e37118cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:54:41,852] Trial 0 finished with value: 0.42570944016917184 and parameters: {'n_estimators': 284, 'max_depth': 7, 'min_samples_split': 7, 'min_samples_leaf': 13, 'max_features': None}. Best is trial 0 with value: 0.42570944016917184.\n",
      "[I 2024-08-06 17:54:43,183] Trial 1 finished with value: 0.5688792132766374 and parameters: {'n_estimators': 200, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 14, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.42570944016917184.\n",
      "[I 2024-08-06 17:54:49,351] Trial 2 finished with value: 0.4405433068018446 and parameters: {'n_estimators': 188, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 12, 'max_features': None}. Best is trial 0 with value: 0.42570944016917184.\n",
      "[I 2024-08-06 17:54:50,333] Trial 3 finished with value: 0.38241653086605004 and parameters: {'n_estimators': 104, 'max_depth': 7, 'min_samples_split': 14, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 3 with value: 0.38241653086605004.\n",
      "[I 2024-08-06 17:54:51,910] Trial 4 finished with value: 0.5986903979689003 and parameters: {'n_estimators': 334, 'max_depth': 3, 'min_samples_split': 14, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 3 with value: 0.38241653086605004.\n",
      "[I 2024-08-06 17:54:53,031] Trial 5 finished with value: 0.956956238960041 and parameters: {'n_estimators': 305, 'max_depth': 3, 'min_samples_split': 13, 'min_samples_leaf': 15, 'max_features': 'log2'}. Best is trial 3 with value: 0.38241653086605004.\n",
      "[I 2024-08-06 17:55:15,388] Trial 6 finished with value: 0.4532130380970464 and parameters: {'n_estimators': 231, 'max_depth': 10, 'min_samples_split': 13, 'min_samples_leaf': 15, 'max_features': None}. Best is trial 3 with value: 0.38241653086605004.\n",
      "[I 2024-08-06 17:55:26,041] Trial 7 finished with value: 0.1874654898249438 and parameters: {'n_estimators': 180, 'max_depth': 5, 'min_samples_split': 6, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 7 with value: 0.1874654898249438.\n",
      "[I 2024-08-06 17:56:01,307] Trial 8 finished with value: 0.34684759329382553 and parameters: {'n_estimators': 354, 'max_depth': 9, 'min_samples_split': 15, 'min_samples_leaf': 10, 'max_features': None}. Best is trial 7 with value: 0.1874654898249438.\n",
      "[I 2024-08-06 17:56:05,808] Trial 9 finished with value: 0.5338826511301014 and parameters: {'n_estimators': 489, 'max_depth': 13, 'min_samples_split': 8, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 7 with value: 0.1874654898249438.\n",
      "[I 2024-08-06 17:56:20,069] Trial 10 finished with value: 0.193239811587326 and parameters: {'n_estimators': 115, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 7 with value: 0.1874654898249438.\n",
      "[I 2024-08-06 17:56:33,810] Trial 11 finished with value: 0.17648343181974974 and parameters: {'n_estimators': 105, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 11 with value: 0.17648343181974974.\n",
      "[I 2024-08-06 17:56:54,170] Trial 12 finished with value: 0.17586945068499932 and parameters: {'n_estimators': 159, 'max_depth': 12, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 12 with value: 0.17586945068499932.\n",
      "[I 2024-08-06 17:57:11,809] Trial 13 finished with value: 0.17555440020345345 and parameters: {'n_estimators': 140, 'max_depth': 13, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 13 with value: 0.17555440020345345.\n",
      "[I 2024-08-06 17:57:15,828] Trial 14 finished with value: 0.5306770928655395 and parameters: {'n_estimators': 423, 'max_depth': 12, 'min_samples_split': 7, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 13 with value: 0.17555440020345345.\n",
      "[I 2024-08-06 17:57:46,115] Trial 15 finished with value: 0.16532791243150533 and parameters: {'n_estimators': 246, 'max_depth': 12, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 15 with value: 0.16532791243150533.\n",
      "[I 2024-08-06 17:58:14,961] Trial 16 finished with value: 0.23494818477406726 and parameters: {'n_estimators': 258, 'max_depth': 10, 'min_samples_split': 11, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 15 with value: 0.16532791243150533.\n",
      "[I 2024-08-06 17:58:43,591] Trial 17 finished with value: 0.18445818892314053 and parameters: {'n_estimators': 226, 'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 15 with value: 0.16532791243150533.\n",
      "[I 2024-08-06 17:58:44,839] Trial 18 finished with value: 0.5950286749529655 and parameters: {'n_estimators': 150, 'max_depth': 11, 'min_samples_split': 9, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 15 with value: 0.16532791243150533.\n",
      "[I 2024-08-06 17:58:50,107] Trial 19 finished with value: 0.3356612210298687 and parameters: {'n_estimators': 389, 'max_depth': 14, 'min_samples_split': 6, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 15 with value: 0.16532791243150533.\n",
      "[I 2024-08-06 17:59:15,651] Trial 20 finished with value: 0.3239651883118119 and parameters: {'n_estimators': 259, 'max_depth': 9, 'min_samples_split': 11, 'min_samples_leaf': 9, 'max_features': None}. Best is trial 15 with value: 0.16532791243150533.\n",
      "[I 2024-08-06 17:59:35,409] Trial 21 finished with value: 0.17469541566067004 and parameters: {'n_estimators': 156, 'max_depth': 12, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 15 with value: 0.16532791243150533.\n",
      "[I 2024-08-06 17:59:53,373] Trial 22 finished with value: 0.1908961276142473 and parameters: {'n_estimators': 143, 'max_depth': 13, 'min_samples_split': 6, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 15 with value: 0.16532791243150533.\n",
      "[I 2024-08-06 18:00:23,647] Trial 23 finished with value: 0.15503375588079285 and parameters: {'n_estimators': 223, 'max_depth': 11, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:00:49,683] Trial 24 finished with value: 0.23323354818931882 and parameters: {'n_estimators': 224, 'max_depth': 11, 'min_samples_split': 8, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:01:19,995] Trial 25 finished with value: 0.2159604353046632 and parameters: {'n_estimators': 272, 'max_depth': 11, 'min_samples_split': 6, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:01:41,276] Trial 26 finished with value: 0.17733226534875904 and parameters: {'n_estimators': 210, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:02:09,721] Trial 27 finished with value: 0.23301944603986324 and parameters: {'n_estimators': 248, 'max_depth': 12, 'min_samples_split': 5, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:02:12,888] Trial 28 finished with value: 0.49697230208472243 and parameters: {'n_estimators': 302, 'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 11, 'max_features': 'sqrt'}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:02:14,077] Trial 29 finished with value: 0.48969222502002874 and parameters: {'n_estimators': 177, 'max_depth': 8, 'min_samples_split': 6, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:02:43,336] Trial 30 finished with value: 0.4354920940942642 and parameters: {'n_estimators': 281, 'max_depth': 14, 'min_samples_split': 7, 'min_samples_leaf': 13, 'max_features': None}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:02:59,780] Trial 31 finished with value: 0.17262842167858228 and parameters: {'n_estimators': 131, 'max_depth': 14, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:03:24,472] Trial 32 finished with value: 0.16662032397066784 and parameters: {'n_estimators': 199, 'max_depth': 14, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 23 with value: 0.15503375588079285.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 18:03:48,090] Trial 33 finished with value: 0.19451141029478253 and parameters: {'n_estimators': 199, 'max_depth': 14, 'min_samples_split': 6, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:04:15,770] Trial 34 finished with value: 0.1668040288329924 and parameters: {'n_estimators': 202, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:04:19,988] Trial 35 finished with value: 0.3986269660087716 and parameters: {'n_estimators': 242, 'max_depth': 15, 'min_samples_split': 7, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:04:47,908] Trial 36 finished with value: 0.23244473710654495 and parameters: {'n_estimators': 205, 'max_depth': 15, 'min_samples_split': 9, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:05:23,852] Trial 37 finished with value: 0.3260866800752832 and parameters: {'n_estimators': 318, 'max_depth': 14, 'min_samples_split': 6, 'min_samples_leaf': 9, 'max_features': None}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:05:26,632] Trial 38 finished with value: 0.3077190431218044 and parameters: {'n_estimators': 179, 'max_depth': 13, 'min_samples_split': 10, 'min_samples_leaf': 6, 'max_features': 'sqrt'}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:05:52,432] Trial 39 finished with value: 0.17242225848400397 and parameters: {'n_estimators': 216, 'max_depth': 11, 'min_samples_split': 8, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:05:54,709] Trial 40 finished with value: 0.6648730010578927 and parameters: {'n_estimators': 286, 'max_depth': 12, 'min_samples_split': 5, 'min_samples_leaf': 13, 'max_features': 'log2'}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:06:22,345] Trial 41 finished with value: 0.16939757676163014 and parameters: {'n_estimators': 221, 'max_depth': 11, 'min_samples_split': 11, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:06:43,832] Trial 42 finished with value: 0.19986608425178548 and parameters: {'n_estimators': 188, 'max_depth': 10, 'min_samples_split': 11, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:06:57,145] Trial 43 finished with value: 0.16762229203153836 and parameters: {'n_estimators': 237, 'max_depth': 5, 'min_samples_split': 12, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:07:10,836] Trial 44 finished with value: 0.21186998573861018 and parameters: {'n_estimators': 248, 'max_depth': 5, 'min_samples_split': 13, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:07:18,225] Trial 45 finished with value: 0.17675918855244202 and parameters: {'n_estimators': 167, 'max_depth': 4, 'min_samples_split': 14, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:07:33,642] Trial 46 finished with value: 0.4194243512965114 and parameters: {'n_estimators': 198, 'max_depth': 7, 'min_samples_split': 12, 'min_samples_leaf': 12, 'max_features': None}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:07:56,330] Trial 47 finished with value: 0.24435827328623247 and parameters: {'n_estimators': 338, 'max_depth': 6, 'min_samples_split': 12, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:08:19,620] Trial 48 finished with value: 0.45517278579547027 and parameters: {'n_estimators': 270, 'max_depth': 8, 'min_samples_split': 15, 'min_samples_leaf': 14, 'max_features': None}. Best is trial 23 with value: 0.15503375588079285.\n",
      "[I 2024-08-06 18:08:22,941] Trial 49 finished with value: 0.2779850916643357 and parameters: {'n_estimators': 240, 'max_depth': 15, 'min_samples_split': 12, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 23 with value: 0.15503375588079285.\n",
      "Best hyperparameters:  {'n_estimators': 223, 'max_depth': 11, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Performance with tuned hyperparameters:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  3.40618536990776\n",
      "Mean Squared Error (MSE):  530.1383397866745\n",
      "R-squared (R2):  0.9744664612941597\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  4.54619843699643\n",
      "Mean Squared Error (MSE):  1229.184622269261\n",
      "R-squared (R2):  0.9542874949308482\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  5.236758044469782\n",
      "Mean Squared Error (MSE):  673.2919321240834\n",
      "R-squared (R2):  0.9703322440850176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'date']\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the features and the target variable\n",
    "def scale_features_and_target(X, y, excluded_columns):\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    y_scaled = scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "    return X_scaled, y_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train_raw, y_train_raw = prepare_data(train_data_lstm_paris, 'price')\n",
    "X_valid_raw, y_valid_raw = prepare_data(valid_data_lstm_paris, 'price')\n",
    "X_test_raw, y_test_raw = prepare_data(test_data_lstm_paris, 'price')\n",
    "\n",
    "# Apply scaling to both features and target\n",
    "X_train, y_train, scaler = scale_features_and_target(X_train_raw, y_train_raw, excluded_columns)\n",
    "X_valid, y_valid, _ = scale_features_and_target(X_valid_raw, y_valid_raw, excluded_columns)\n",
    "X_test, y_test, _ = scale_features_and_target(X_test_raw, y_test_raw, excluded_columns)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 5, 15),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5, 15),\n",
    "        'max_features': trial.suggest_categorical('max_features', [None, 'sqrt', 'log2'])\n",
    "    }\n",
    "\n",
    "    model = RandomForestRegressor(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    mse = mean_squared_error(y_valid, preds)\n",
    "    return mse\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = RandomForestRegressor(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_scaled = best_model.predict(X_train)\n",
    "y_valid_pred_scaled = best_model.predict(X_valid)\n",
    "y_test_pred_scaled = best_model.predict(X_test)\n",
    "\n",
    "# Inverse transform the scaled predictions and actual target values\n",
    "y_train_pred = scaler.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_valid_pred = scaler.inverse_transform(y_valid_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_pred = scaler.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_train_orig = scaler.inverse_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_valid_orig = scaler.inverse_transform(y_valid.reshape(-1, 1)).flatten()\n",
    "y_test_orig = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate performance metrics on the original scale\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with tuned hyperparameters:\")\n",
    "print_metrics(y_train_orig, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid_orig, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test_orig, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74376dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'rf_model_with_lstm_paris.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('rf_model_with_lstm_paris.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b66a802",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f66bd8b",
   "metadata": {},
   "source": [
    "## Without LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecde530f",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a581262e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:20,790] A new study created in memory with name: no-name-3691bdf3-bcb8-41c2-9c6f-49f85bfa3c62\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4070520b4243b0944bd385af6193cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:20,847] Trial 0 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:20,880] Trial 1 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:20,918] Trial 2 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:20,955] Trial 3 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:20,987] Trial 4 finished with value: 4.22938161981129e+22 and parameters: {'fit_intercept': True}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,020] Trial 5 finished with value: 4.22938161981129e+22 and parameters: {'fit_intercept': True}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,055] Trial 6 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,085] Trial 7 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,116] Trial 8 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,151] Trial 9 finished with value: 4.22938161981129e+22 and parameters: {'fit_intercept': True}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,205] Trial 10 finished with value: 4.22938161981129e+22 and parameters: {'fit_intercept': True}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,246] Trial 11 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,281] Trial 12 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,317] Trial 13 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,348] Trial 14 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,381] Trial 15 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,416] Trial 16 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,448] Trial 17 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,486] Trial 18 finished with value: 4.22938161981129e+22 and parameters: {'fit_intercept': True}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,522] Trial 19 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,554] Trial 20 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,588] Trial 21 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,621] Trial 22 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,653] Trial 23 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,687] Trial 24 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,721] Trial 25 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,761] Trial 26 finished with value: 4.22938161981129e+22 and parameters: {'fit_intercept': True}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,809] Trial 27 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,849] Trial 28 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,889] Trial 29 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,928] Trial 30 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:21,966] Trial 31 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:22,006] Trial 32 finished with value: 4.22938161981129e+22 and parameters: {'fit_intercept': True}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:22,038] Trial 33 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:22,070] Trial 34 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:22,105] Trial 35 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:22,141] Trial 36 finished with value: 4.22938161981129e+22 and parameters: {'fit_intercept': True}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:22,179] Trial 37 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:22,219] Trial 38 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:22,256] Trial 39 finished with value: 4.22938161981129e+22 and parameters: {'fit_intercept': True}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:22,290] Trial 40 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:22,326] Trial 41 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:22,358] Trial 42 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:22,393] Trial 43 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:22,432] Trial 44 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:22,467] Trial 45 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:22,501] Trial 46 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:22,536] Trial 47 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "[I 2024-08-06 17:52:22,571] Trial 48 finished with value: 4.22938161981129e+22 and parameters: {'fit_intercept': True}. Best is trial 0 with value: 1.067615731264326e+20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:22,603] Trial 49 finished with value: 1.067615731264326e+20 and parameters: {'fit_intercept': False}. Best is trial 0 with value: 1.067615731264326e+20.\n",
      "Best hyperparameters:  {'fit_intercept': False}\n",
      "Performance with tuned hyperparameters:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  53.20830629783894\n",
      "Mean Squared Error (MSE):  11289.73504708572\n",
      "R-squared (R2):  0.4562421444194311\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  867934133434.1613\n",
      "Mean Squared Error (MSE):  7.533096599801085e+23\n",
      "R-squared (R2):  -2.8015052439322178e+19\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  56.18090688899944\n",
      "Mean Squared Error (MSE):  12370.961783025352\n",
      "R-squared (R2):  0.4548892432818695\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'date']\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the features and the target variable\n",
    "def scale_features_and_target(X, y, excluded_columns):\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    y_scaled = scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "    return X_scaled, y_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train_raw, y_train_raw = prepare_data(train_data_paris, 'price')\n",
    "X_valid_raw, y_valid_raw = prepare_data(valid_data_paris, 'price')\n",
    "X_test_raw, y_test_raw = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Apply scaling to both features and target\n",
    "X_train_scaled, y_train_scaled, scaler = scale_features_and_target(X_train_raw, y_train_raw, excluded_columns)\n",
    "X_valid_scaled, y_valid_scaled, _ = scale_features_and_target(X_valid_raw, y_valid_raw, excluded_columns)\n",
    "X_test_scaled, y_test_scaled, _ = scale_features_and_target(X_test_raw, y_test_raw, excluded_columns)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    fit_intercept = trial.suggest_categorical('fit_intercept', [True, False])\n",
    "\n",
    "    model = LinearRegression(fit_intercept=fit_intercept)\n",
    "    model.fit(X_train_scaled, y_train_scaled)\n",
    "    preds = model.predict(X_valid_scaled)\n",
    "    mse = mean_squared_error(y_valid_scaled, preds)\n",
    "    return mse\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = LinearRegression(**best_params)\n",
    "best_model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_scaled = best_model.predict(X_train_scaled)\n",
    "y_valid_pred_scaled = best_model.predict(X_valid_scaled)\n",
    "y_test_pred_scaled = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Inverse transform the scaled predictions and actual target values\n",
    "y_train_pred = scaler.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_valid_pred = scaler.inverse_transform(y_valid_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_pred = scaler.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_train_orig = scaler.inverse_transform(y_train_scaled.reshape(-1, 1)).flatten()\n",
    "y_valid_orig = scaler.inverse_transform(y_valid_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_orig = scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate performance metrics on the original scale\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with tuned hyperparameters:\")\n",
    "print_metrics(y_train_orig, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid_orig, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test_orig, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2dfcdc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'linear_model_without_lstm_paris.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('linear_model_without_lstm_paris.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947128e9",
   "metadata": {},
   "source": [
    "## With LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84939f87",
   "metadata": {},
   "source": [
    "#### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9690fe5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:22,813] A new study created in memory with name: no-name-8d3afaf0-12eb-4d1e-a4ec-c82c1e54579f\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a3624342f949eea02307d99fd90778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:22,870] Trial 0 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:22,911] Trial 1 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:22,949] Trial 2 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:22,991] Trial 3 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,027] Trial 4 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,064] Trial 5 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,103] Trial 6 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,137] Trial 7 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,172] Trial 8 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,214] Trial 9 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,249] Trial 10 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,287] Trial 11 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,326] Trial 12 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,363] Trial 13 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,399] Trial 14 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,439] Trial 15 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,475] Trial 16 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,515] Trial 17 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,553] Trial 18 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,590] Trial 19 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,632] Trial 20 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,679] Trial 21 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,722] Trial 22 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,766] Trial 23 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,811] Trial 24 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,851] Trial 25 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,897] Trial 26 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,942] Trial 27 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:23,979] Trial 28 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:24,019] Trial 29 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:24,057] Trial 30 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:24,090] Trial 31 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:24,126] Trial 32 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:24,164] Trial 33 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:24,215] Trial 34 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:24,267] Trial 35 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:24,308] Trial 36 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:24,348] Trial 37 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:24,386] Trial 38 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:24,427] Trial 39 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:24,466] Trial 40 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:24,507] Trial 41 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:24,546] Trial 42 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:24,581] Trial 43 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:24,619] Trial 44 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:24,659] Trial 45 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:24,699] Trial 46 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:24,741] Trial 47 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:24,785] Trial 48 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "[I 2024-08-06 17:52:24,829] Trial 49 finished with value: 9.785562439210102e+17 and parameters: {}. Best is trial 0 with value: 9.785562439210102e+17.\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  10.281503537495931\n",
      "Mean Squared Error (MSE):  702.106766822049\n",
      "R-squared (R2):  0.9661837883419315\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  83094481508.14015\n",
      "Mean Squared Error (MSE):  6.904692857106648e+21\n",
      "R-squared (R2):  -2.5678063450608582e+17\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  14.03146406734691\n",
      "Mean Squared Error (MSE):  1306.749774167129\n",
      "R-squared (R2):  0.9424197268788839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'date']\n",
    "\n",
    "# Prepare the data by dropping unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the features and the target variable\n",
    "def scale_features_and_target(X, y, excluded_columns):\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    y_scaled = scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "    return X_scaled, y_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets for LSTM data\n",
    "X_train_raw, y_train_raw = prepare_data(train_data_lstm_paris, 'price')\n",
    "X_valid_raw, y_valid_raw = prepare_data(valid_data_lstm_paris, 'price')\n",
    "X_test_raw, y_test_raw = prepare_data(test_data_lstm_paris, 'price')\n",
    "\n",
    "# Apply scaling to both features and target\n",
    "X_train_scaled, y_train_scaled, scaler = scale_features_and_target(X_train_raw, y_train_raw, excluded_columns)\n",
    "X_valid_scaled, y_valid_scaled, _ = scale_features_and_target(X_valid_raw, y_valid_raw, excluded_columns)\n",
    "X_test_scaled, y_test_scaled, _ = scale_features_and_target(X_test_raw, y_test_raw, excluded_columns)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    # No hyperparameters to tune for Linear Regression, just using it to follow the structure\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_scaled, y_train_scaled)\n",
    "    preds = model.predict(X_valid_scaled)\n",
    "    mse = mean_squared_error(y_valid_scaled, preds)\n",
    "    return mse\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Since there are no hyperparameters to tune, we use a default Linear Regression model\n",
    "best_model = LinearRegression()\n",
    "best_model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_scaled = best_model.predict(X_train_scaled)\n",
    "y_valid_pred_scaled = best_model.predict(X_valid_scaled)\n",
    "y_test_pred_scaled = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Inverse transform the scaled predictions and actual target values\n",
    "y_train_pred = scaler.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_valid_pred = scaler.inverse_transform(y_valid_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_pred = scaler.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_train_orig = scaler.inverse_transform(y_train_scaled.reshape(-1, 1)).flatten()\n",
    "y_valid_orig = scaler.inverse_transform(y_valid_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_orig = scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate performance metrics on the original scale\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print_metrics(y_train_orig, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid_orig, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test_orig, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03e4bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'linear_model_with_lstm_paris.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('linear_model_with_lstm_paris.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034e777f",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a548a18",
   "metadata": {},
   "source": [
    "## Without LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377df321",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "732031c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:25,034] A new study created in memory with name: no-name-db1dc44a-95d4-420f-9b80-5c3c9cadca50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c145b1d60eb4a499c1e12abd89ec23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:25,079] Trial 0 finished with value: 2.329790094711496 and parameters: {'alpha': 0.8253606422843964, 'fit_intercept': False}. Best is trial 0 with value: 2.329790094711496.\n",
      "[I 2024-08-06 17:52:25,116] Trial 1 finished with value: 2.3299432237673514 and parameters: {'alpha': 2.260822491814495, 'fit_intercept': True}. Best is trial 0 with value: 2.329790094711496.\n",
      "[I 2024-08-06 17:52:25,153] Trial 2 finished with value: 2.329766684722215 and parameters: {'alpha': 0.789847447350629, 'fit_intercept': True}. Best is trial 2 with value: 2.329766684722215.\n",
      "[I 2024-08-06 17:52:25,189] Trial 3 finished with value: 2.330206729531649 and parameters: {'alpha': 0.5073956810599429, 'fit_intercept': True}. Best is trial 2 with value: 2.329766684722215.\n",
      "[I 2024-08-06 17:52:25,218] Trial 4 finished with value: 2.3300309453014463 and parameters: {'alpha': 0.6150529165623205, 'fit_intercept': False}. Best is trial 2 with value: 2.329766684722215.\n",
      "[I 2024-08-06 17:52:25,251] Trial 5 finished with value: 2.3320089828641066 and parameters: {'alpha': 7.241871051347427, 'fit_intercept': True}. Best is trial 2 with value: 2.329766684722215.\n",
      "[I 2024-08-06 17:52:25,278] Trial 6 finished with value: 2.3297519063494487 and parameters: {'alpha': 1.652275646267679, 'fit_intercept': False}. Best is trial 6 with value: 2.3297519063494487.\n",
      "[I 2024-08-06 17:52:25,311] Trial 7 finished with value: 2.332262584587701 and parameters: {'alpha': 7.935594316900224, 'fit_intercept': True}. Best is trial 6 with value: 2.3297519063494487.\n",
      "[I 2024-08-06 17:52:25,340] Trial 8 finished with value: 2.3313775771414793 and parameters: {'alpha': 0.2429473194705199, 'fit_intercept': True}. Best is trial 6 with value: 2.3297519063494487.\n",
      "[I 2024-08-06 17:52:25,368] Trial 9 finished with value: 2.3313001573979135 and parameters: {'alpha': 0.25389328270101497, 'fit_intercept': True}. Best is trial 6 with value: 2.3297519063494487.\n",
      "[I 2024-08-06 17:52:25,402] Trial 10 finished with value: 2.3299822194287403 and parameters: {'alpha': 2.2712008354026567, 'fit_intercept': False}. Best is trial 6 with value: 2.3297519063494487.\n",
      "[I 2024-08-06 17:52:25,439] Trial 11 finished with value: 2.329869163337584 and parameters: {'alpha': 1.991167190145263, 'fit_intercept': False}. Best is trial 6 with value: 2.3297519063494487.\n",
      "[I 2024-08-06 17:52:25,470] Trial 12 finished with value: 2.329694329125849 and parameters: {'alpha': 1.4144213361421312, 'fit_intercept': False}. Best is trial 12 with value: 2.329694329125849.\n",
      "[I 2024-08-06 17:52:25,506] Trial 13 finished with value: 2.3325971316488503 and parameters: {'alpha': 0.10694416534453931, 'fit_intercept': False}. Best is trial 12 with value: 2.329694329125849.\n",
      "[I 2024-08-06 17:52:25,543] Trial 14 finished with value: 2.330604505004157 and parameters: {'alpha': 3.6907019141897917, 'fit_intercept': False}. Best is trial 12 with value: 2.329694329125849.\n",
      "[I 2024-08-06 17:52:25,580] Trial 15 finished with value: 2.3297074569215988 and parameters: {'alpha': 1.4808864728460014, 'fit_intercept': False}. Best is trial 12 with value: 2.329694329125849.\n",
      "[I 2024-08-06 17:52:25,610] Trial 16 finished with value: 2.3308063024258514 and parameters: {'alpha': 4.159363892306209, 'fit_intercept': False}. Best is trial 12 with value: 2.329694329125849.\n",
      "[I 2024-08-06 17:52:25,644] Trial 17 finished with value: 2.329680065527562 and parameters: {'alpha': 1.3129237097424309, 'fit_intercept': False}. Best is trial 17 with value: 2.329680065527562.\n",
      "[I 2024-08-06 17:52:25,679] Trial 18 finished with value: 2.330569138108558 and parameters: {'alpha': 0.40167611164072364, 'fit_intercept': False}. Best is trial 17 with value: 2.329680065527562.\n",
      "[I 2024-08-06 17:52:25,710] Trial 19 finished with value: 2.329677913940199 and parameters: {'alpha': 1.289284986001422, 'fit_intercept': False}. Best is trial 19 with value: 2.329677913940199.\n",
      "[I 2024-08-06 17:52:25,747] Trial 20 finished with value: 2.33064369257005 and parameters: {'alpha': 3.7808840278607647, 'fit_intercept': False}. Best is trial 19 with value: 2.329677913940199.\n",
      "[I 2024-08-06 17:52:25,788] Trial 21 finished with value: 2.3296748232808797 and parameters: {'alpha': 1.1841977750242183, 'fit_intercept': False}. Best is trial 21 with value: 2.3296748232808797.\n",
      "[I 2024-08-06 17:52:25,822] Trial 22 finished with value: 2.3296797141571015 and parameters: {'alpha': 1.1141918769044141, 'fit_intercept': False}. Best is trial 21 with value: 2.3296748232808797.\n",
      "[I 2024-08-06 17:52:25,859] Trial 23 finished with value: 2.329701638123107 and parameters: {'alpha': 1.0046809782578145, 'fit_intercept': False}. Best is trial 21 with value: 2.3296748232808797.\n",
      "[I 2024-08-06 17:52:25,897] Trial 24 finished with value: 2.3307054284286264 and parameters: {'alpha': 0.3666327794724868, 'fit_intercept': False}. Best is trial 21 with value: 2.3296748232808797.\n",
      "[I 2024-08-06 17:52:25,926] Trial 25 finished with value: 2.3296785317084443 and parameters: {'alpha': 1.125110513298135, 'fit_intercept': False}. Best is trial 21 with value: 2.3296748232808797.\n",
      "[I 2024-08-06 17:52:25,963] Trial 26 finished with value: 2.33029360297609 and parameters: {'alpha': 2.985073825851484, 'fit_intercept': False}. Best is trial 21 with value: 2.3296748232808797.\n",
      "[I 2024-08-06 17:52:26,002] Trial 27 finished with value: 2.3298953298662 and parameters: {'alpha': 0.7122333687402113, 'fit_intercept': False}. Best is trial 21 with value: 2.3296748232808797.\n",
      "[I 2024-08-06 17:52:26,038] Trial 28 finished with value: 2.329689806586087 and parameters: {'alpha': 1.0518263579989566, 'fit_intercept': False}. Best is trial 21 with value: 2.3296748232808797.\n",
      "[I 2024-08-06 17:52:26,075] Trial 29 finished with value: 2.3314370014953814 and parameters: {'alpha': 5.703498014989875, 'fit_intercept': False}. Best is trial 21 with value: 2.3296748232808797.\n",
      "[I 2024-08-06 17:52:26,117] Trial 30 finished with value: 2.3301131010704172 and parameters: {'alpha': 0.5696786779160582, 'fit_intercept': False}. Best is trial 21 with value: 2.3296748232808797.\n",
      "[I 2024-08-06 17:52:26,154] Trial 31 finished with value: 2.3296965941683143 and parameters: {'alpha': 1.022925943998721, 'fit_intercept': False}. Best is trial 21 with value: 2.3296748232808797.\n",
      "[I 2024-08-06 17:52:26,195] Trial 32 finished with value: 2.330167587541162 and parameters: {'alpha': 2.7002431801886706, 'fit_intercept': False}. Best is trial 21 with value: 2.3296748232808797.\n",
      "[I 2024-08-06 17:52:26,233] Trial 33 finished with value: 2.329675336139991 and parameters: {'alpha': 1.1696181386590678, 'fit_intercept': False}. Best is trial 21 with value: 2.3296748232808797.\n",
      "[I 2024-08-06 17:52:26,266] Trial 34 finished with value: 2.3298056708626502 and parameters: {'alpha': 0.8049558512308803, 'fit_intercept': False}. Best is trial 21 with value: 2.3296748232808797.\n",
      "[I 2024-08-06 17:52:26,301] Trial 35 finished with value: 2.329799543760021 and parameters: {'alpha': 1.800614331268212, 'fit_intercept': False}. Best is trial 21 with value: 2.3296748232808797.\n",
      "[I 2024-08-06 17:52:26,340] Trial 36 finished with value: 2.329795434379975 and parameters: {'alpha': 0.7582023539096949, 'fit_intercept': True}. Best is trial 21 with value: 2.3296748232808797.\n",
      "[I 2024-08-06 17:52:26,370] Trial 37 finished with value: 2.3303432617059454 and parameters: {'alpha': 0.4721812047680611, 'fit_intercept': False}. Best is trial 21 with value: 2.3296748232808797.\n",
      "[I 2024-08-06 17:52:26,406] Trial 38 finished with value: 2.329631415886206 and parameters: {'alpha': 1.2909554398291216, 'fit_intercept': True}. Best is trial 38 with value: 2.329631415886206.\n",
      "[I 2024-08-06 17:52:26,448] Trial 39 finished with value: 2.3300832731314003 and parameters: {'alpha': 2.580208388890509, 'fit_intercept': True}. Best is trial 38 with value: 2.329631415886206.\n",
      "[I 2024-08-06 17:52:26,483] Trial 40 finished with value: 2.3297600715398032 and parameters: {'alpha': 1.8015984122240003, 'fit_intercept': True}. Best is trial 38 with value: 2.329631415886206.\n",
      "[I 2024-08-06 17:52:26,526] Trial 41 finished with value: 2.329631380529589 and parameters: {'alpha': 1.29058627850287, 'fit_intercept': True}. Best is trial 41 with value: 2.329631380529589.\n",
      "[I 2024-08-06 17:52:26,569] Trial 42 finished with value: 2.329645111201587 and parameters: {'alpha': 1.3909343722949399, 'fit_intercept': True}. Best is trial 41 with value: 2.329631380529589.\n",
      "[I 2024-08-06 17:52:26,604] Trial 43 finished with value: 2.3296961392821225 and parameters: {'alpha': 0.8931498244402016, 'fit_intercept': True}. Best is trial 41 with value: 2.329631380529589.\n",
      "[I 2024-08-06 17:52:26,643] Trial 44 finished with value: 2.3299317129237442 and parameters: {'alpha': 0.646488897122231, 'fit_intercept': True}. Best is trial 41 with value: 2.329631380529589.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:26,684] Trial 45 finished with value: 2.3298803758618867 and parameters: {'alpha': 2.111620739589073, 'fit_intercept': True}. Best is trial 41 with value: 2.329631380529589.\n",
      "[I 2024-08-06 17:52:26,719] Trial 46 finished with value: 2.329655585584907 and parameters: {'alpha': 1.445256503935237, 'fit_intercept': True}. Best is trial 41 with value: 2.329631380529589.\n",
      "[I 2024-08-06 17:52:26,758] Trial 47 finished with value: 2.329665092015276 and parameters: {'alpha': 1.487953271824821, 'fit_intercept': True}. Best is trial 41 with value: 2.329631380529589.\n",
      "[I 2024-08-06 17:52:26,799] Trial 48 finished with value: 2.329688351120187 and parameters: {'alpha': 1.5781644822463499, 'fit_intercept': True}. Best is trial 41 with value: 2.329631380529589.\n",
      "[I 2024-08-06 17:52:26,838] Trial 49 finished with value: 2.3298923598369066 and parameters: {'alpha': 2.140509958642068, 'fit_intercept': True}. Best is trial 41 with value: 2.329631380529589.\n",
      "Best hyperparameters:  {'alpha': 1.29058627850287, 'fit_intercept': True}\n",
      "Performance with tuned hyperparameters:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  53.18006378830149\n",
      "Mean Squared Error (MSE):  11299.356514865816\n",
      "R-squared (R2):  0.4557787368491186\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  55.61447095693617\n",
      "Mean Squared Error (MSE):  16437.879021016783\n",
      "R-squared (R2):  0.38868692752834333\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  55.89815132269972\n",
      "Mean Squared Error (MSE):  12256.985072927608\n",
      "R-squared (R2):  0.45991148260160264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'date']\n",
    "\n",
    "# Prepare the data by dropping unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the features and the target variable\n",
    "def scale_features_and_target(X, y, excluded_columns):\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    y_scaled = scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "    return X_scaled, y_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train_raw, y_train_raw = prepare_data(train_data_paris, 'price')\n",
    "X_valid_raw, y_valid_raw = prepare_data(valid_data_paris, 'price')\n",
    "X_test_raw, y_test_raw = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Apply scaling to both features and target\n",
    "X_train_scaled, y_train_scaled, scaler = scale_features_and_target(X_train_raw, y_train_raw, excluded_columns)\n",
    "X_valid_scaled, y_valid_scaled, _ = scale_features_and_target(X_valid_raw, y_valid_raw, excluded_columns)\n",
    "X_test_scaled, y_test_scaled, _ = scale_features_and_target(X_test_raw, y_test_raw, excluded_columns)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    alpha = trial.suggest_float('alpha', 0.1, 10.0, log=True)\n",
    "    fit_intercept = trial.suggest_categorical('fit_intercept', [True, False])\n",
    "\n",
    "    model = Ridge(alpha=alpha, fit_intercept=fit_intercept)\n",
    "    model.fit(X_train_scaled, y_train_scaled)\n",
    "    preds = model.predict(X_valid_scaled)\n",
    "    mse = mean_squared_error(y_valid_scaled, preds)\n",
    "    return mse\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = Ridge(**best_params)\n",
    "best_model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_scaled = best_model.predict(X_train_scaled)\n",
    "y_valid_pred_scaled = best_model.predict(X_valid_scaled)\n",
    "y_test_pred_scaled = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Inverse transform the scaled predictions and actual target values\n",
    "y_train_pred = scaler.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_valid_pred = scaler.inverse_transform(y_valid_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_pred = scaler.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_train_orig = scaler.inverse_transform(y_train_scaled.reshape(-1, 1)).flatten()\n",
    "y_valid_orig = scaler.inverse_transform(y_valid_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_orig = scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate performance metrics on the original scale\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with tuned hyperparameters:\")\n",
    "print_metrics(y_train_orig, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid_orig, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test_orig, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f45b6153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'ridge_model_without_lstm_paris.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('ridge_model_without_lstm_paris.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2419306",
   "metadata": {},
   "source": [
    "## With LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a21c7",
   "metadata": {},
   "source": [
    "#### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe538d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:27,041] A new study created in memory with name: no-name-4d42f102-d486-4f63-a8e9-87410a49bccd\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88763d94d0514a76aab6e6d1c47a85d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:27,095] Trial 0 finished with value: 0.16751521796424096 and parameters: {'alpha': 1.6576864136976037}. Best is trial 0 with value: 0.16751521796424096.\n",
      "[I 2024-08-06 17:52:27,133] Trial 1 finished with value: 0.1428184980379934 and parameters: {'alpha': 0.15720202247487827}. Best is trial 1 with value: 0.1428184980379934.\n",
      "[I 2024-08-06 17:52:27,171] Trial 2 finished with value: 0.1537961256287615 and parameters: {'alpha': 0.7180149113318393}. Best is trial 1 with value: 0.1428184980379934.\n",
      "[I 2024-08-06 17:52:27,213] Trial 3 finished with value: 0.1554351921470547 and parameters: {'alpha': 0.8127370290211507}. Best is trial 1 with value: 0.1428184980379934.\n",
      "[I 2024-08-06 17:52:27,251] Trial 4 finished with value: 0.16848118285331753 and parameters: {'alpha': 1.7397079629922567}. Best is trial 1 with value: 0.1428184980379934.\n",
      "[I 2024-08-06 17:52:27,295] Trial 5 finished with value: 0.16863485333873462 and parameters: {'alpha': 1.753002482363543}. Best is trial 1 with value: 0.1428184980379934.\n",
      "[I 2024-08-06 17:52:27,336] Trial 6 finished with value: 0.19313717607074965 and parameters: {'alpha': 5.364310531738366}. Best is trial 1 with value: 0.1428184980379934.\n",
      "[I 2024-08-06 17:52:27,374] Trial 7 finished with value: 0.1426680509441445 and parameters: {'alpha': 0.15007475403309525}. Best is trial 7 with value: 0.1426680509441445.\n",
      "[I 2024-08-06 17:52:27,410] Trial 8 finished with value: 0.1446703045683718 and parameters: {'alpha': 0.24549875888507733}. Best is trial 7 with value: 0.1426680509441445.\n",
      "[I 2024-08-06 17:52:27,445] Trial 9 finished with value: 0.16280434484395193 and parameters: {'alpha': 1.2922562672873457}. Best is trial 7 with value: 0.1426680509441445.\n",
      "[I 2024-08-06 17:52:27,484] Trial 10 finished with value: 0.14172045014457302 and parameters: {'alpha': 0.10523826870290648}. Best is trial 10 with value: 0.14172045014457302.\n",
      "[I 2024-08-06 17:52:27,525] Trial 11 finished with value: 0.14181944277857264 and parameters: {'alpha': 0.10992135583241307}. Best is trial 10 with value: 0.14172045014457302.\n",
      "[I 2024-08-06 17:52:27,566] Trial 12 finished with value: 0.1461324318659843 and parameters: {'alpha': 0.31638577014914354}. Best is trial 10 with value: 0.14172045014457302.\n",
      "[I 2024-08-06 17:52:27,605] Trial 13 finished with value: 0.14189905129133254 and parameters: {'alpha': 0.11368705436093698}. Best is trial 10 with value: 0.14172045014457302.\n",
      "[I 2024-08-06 17:52:27,647] Trial 14 finished with value: 0.1474514196352351 and parameters: {'alpha': 0.3815612534917325}. Best is trial 10 with value: 0.14172045014457302.\n",
      "[I 2024-08-06 17:52:27,688] Trial 15 finished with value: 0.14162263373609732 and parameters: {'alpha': 0.10061003701712448}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:27,731] Trial 16 finished with value: 0.20438580445668916 and parameters: {'alpha': 9.251346412579672}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:27,775] Trial 17 finished with value: 0.15027362631323415 and parameters: {'alpha': 0.5259642949253598}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:27,815] Trial 18 finished with value: 0.14413998351543886 and parameters: {'alpha': 0.22007542655543103}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:27,855] Trial 19 finished with value: 0.18398848465996548 and parameters: {'alpha': 3.5528756783300315}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:27,891] Trial 20 finished with value: 0.14357597534192937 and parameters: {'alpha': 0.19317015961444187}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:27,930] Trial 21 finished with value: 0.1417700095902191 and parameters: {'alpha': 0.10758287980892882}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:27,966] Trial 22 finished with value: 0.14178077487046592 and parameters: {'alpha': 0.10809215235824642}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:28,006] Trial 23 finished with value: 0.1468942944542386 and parameters: {'alpha': 0.3538736177799434}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:28,060] Trial 24 finished with value: 0.14285898487120413 and parameters: {'alpha': 0.1591207957023065}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:28,151] Trial 25 finished with value: 0.14480877340590526 and parameters: {'alpha': 0.25215954540502894}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:28,212] Trial 26 finished with value: 0.14950235975862564 and parameters: {'alpha': 0.4857550633634901}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:28,273] Trial 27 finished with value: 0.14162377718862149 and parameters: {'alpha': 0.1006641457348298}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:28,315] Trial 28 finished with value: 0.14280386213378077 and parameters: {'alpha': 0.1565084725267326}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:28,354] Trial 29 finished with value: 0.14518279627220704 and parameters: {'alpha': 0.27020247256541197}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:28,396] Trial 30 finished with value: 0.14352470100346823 and parameters: {'alpha': 0.1907300640503894}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:28,433] Trial 31 finished with value: 0.14170257903061778 and parameters: {'alpha': 0.10439275563226838}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:28,474] Trial 32 finished with value: 0.1423685877713674 and parameters: {'alpha': 0.1358987090552517}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:28,519] Trial 33 finished with value: 0.14167791612079603 and parameters: {'alpha': 0.10322586465114408}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:28,558] Trial 34 finished with value: 0.14257250041648603 and parameters: {'alpha': 0.14555019825814133}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:28,596] Trial 35 finished with value: 0.14350385645553887 and parameters: {'alpha': 0.1897383464741715}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:28,637] Trial 36 finished with value: 0.1423914198424213 and parameters: {'alpha': 0.13697913975471174}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:28,674] Trial 37 finished with value: 0.15301597001781625 and parameters: {'alpha': 0.6741927228447661}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:28,717] Trial 38 finished with value: 0.14351818558768753 and parameters: {'alpha': 0.1904200655811475}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:28,762] Trial 39 finished with value: 0.15898882655884933 and parameters: {'alpha': 1.0319647390811548}. Best is trial 15 with value: 0.14162263373609732.\n",
      "[I 2024-08-06 17:52:28,807] Trial 40 finished with value: 0.14161322376334232 and parameters: {'alpha': 0.10016474684830554}. Best is trial 40 with value: 0.14161322376334232.\n",
      "[I 2024-08-06 17:52:28,856] Trial 41 finished with value: 0.14162484114339288 and parameters: {'alpha': 0.10071449245651001}. Best is trial 40 with value: 0.14161322376334232.\n",
      "[I 2024-08-06 17:52:28,910] Trial 42 finished with value: 0.1422788905957553 and parameters: {'alpha': 0.1316546501485695}. Best is trial 40 with value: 0.14161322376334232.\n",
      "[I 2024-08-06 17:52:28,956] Trial 43 finished with value: 0.14285139173519787 and parameters: {'alpha': 0.15876091187448113}. Best is trial 40 with value: 0.14161322376334232.\n",
      "[I 2024-08-06 17:52:29,001] Trial 44 finished with value: 0.14223546546623345 and parameters: {'alpha': 0.12960020857902727}. Best is trial 40 with value: 0.14161322376334232.\n",
      "[I 2024-08-06 17:52:29,049] Trial 45 finished with value: 0.17210540252512213 and parameters: {'alpha': 2.072806770191947}. Best is trial 40 with value: 0.14161322376334232.\n",
      "[I 2024-08-06 17:52:29,091] Trial 46 finished with value: 0.14404996102152412 and parameters: {'alpha': 0.21577247942146224}. Best is trial 40 with value: 0.14161322376334232.\n",
      "[I 2024-08-06 17:52:29,130] Trial 47 finished with value: 0.14561244920459937 and parameters: {'alpha': 0.291027947759434}. Best is trial 40 with value: 0.14161322376334232.\n",
      "[I 2024-08-06 17:52:29,171] Trial 48 finished with value: 0.14201513553953896 and parameters: {'alpha': 0.11917789124155216}. Best is trial 40 with value: 0.14161322376334232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:29,208] Trial 49 finished with value: 0.14171917598745404 and parameters: {'alpha': 0.10517798702736278}. Best is trial 40 with value: 0.14161322376334232.\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  10.283741109244705\n",
      "Mean Squared Error (MSE):  702.3771922797854\n",
      "R-squared (R2):  0.9661707635927785\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  10.214476216342955\n",
      "Mean Squared Error (MSE):  999.2229068741436\n",
      "R-squared (R2):  0.9628396081693812\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  13.664691546415867\n",
      "Mean Squared Error (MSE):  1277.930003708425\n",
      "R-squared (R2):  0.943689633548933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'date']\n",
    "\n",
    "# Prepare the data by dropping unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the features and the target variable\n",
    "def scale_features_and_target(X, y, excluded_columns):\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    y_scaled = scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "    return X_scaled, y_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train_raw, y_train_raw = prepare_data(train_data_lstm_paris, 'price')\n",
    "X_valid_raw, y_valid_raw = prepare_data(valid_data_lstm_paris, 'price')\n",
    "X_test_raw, y_test_raw = prepare_data(test_data_lstm_paris, 'price')\n",
    "\n",
    "# Apply scaling to both features and target\n",
    "X_train_scaled, y_train_scaled, scaler = scale_features_and_target(X_train_raw, y_train_raw, excluded_columns)\n",
    "X_valid_scaled, y_valid_scaled, _ = scale_features_and_target(X_valid_raw, y_valid_raw, excluded_columns)\n",
    "X_test_scaled, y_test_scaled, _ = scale_features_and_target(X_test_raw, y_test_raw, excluded_columns)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'alpha': trial.suggest_float('alpha', 0.1, 10.0, log=True)\n",
    "    }\n",
    "\n",
    "    model = Ridge(**param)\n",
    "    model.fit(X_train_scaled, y_train_scaled)\n",
    "    preds = model.predict(X_valid_scaled)\n",
    "    mse = mean_squared_error(y_valid_scaled, preds)\n",
    "    return mse\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters and train the final model\n",
    "best_params = study.best_params\n",
    "best_model = Ridge(**best_params)\n",
    "best_model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_scaled = best_model.predict(X_train_scaled)\n",
    "y_valid_pred_scaled = best_model.predict(X_valid_scaled)\n",
    "y_test_pred_scaled = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Inverse transform the scaled predictions and actual target values\n",
    "y_train_pred = scaler.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_valid_pred = scaler.inverse_transform(y_valid_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_pred = scaler.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_train_orig = scaler.inverse_transform(y_train_scaled.reshape(-1, 1)).flatten()\n",
    "y_valid_orig = scaler.inverse_transform(y_valid_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_orig = scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate performance metrics on the original scale\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print_metrics(y_train_orig, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid_orig, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test_orig, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d562a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'ridge_model_with_lstm_paris.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('ridge_model_with_lstm_paris.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517f8bdb",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd331b2",
   "metadata": {},
   "source": [
    "## Without LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d9410a",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b8380fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:29,364] A new study created in memory with name: no-name-98403534-8f6e-4b9a-a47e-4965089f0e8d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52bb504adb944e6a88927a2b29ba3576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:29,428] Trial 0 finished with value: 2.8437299009932637 and parameters: {'alpha': 0.12398508300385813}. Best is trial 0 with value: 2.8437299009932637.\n",
      "[I 2024-08-06 17:52:29,471] Trial 1 finished with value: 2.7824107324323597 and parameters: {'alpha': 0.10518369868617167}. Best is trial 1 with value: 2.7824107324323597.\n",
      "[I 2024-08-06 17:52:29,705] Trial 2 finished with value: 2.32867491973759 and parameters: {'alpha': 0.0002688819390731974}. Best is trial 2 with value: 2.32867491973759.\n",
      "[I 2024-08-06 17:52:29,805] Trial 3 finished with value: 2.331950047384034 and parameters: {'alpha': 0.0007609477626929325}. Best is trial 2 with value: 2.32867491973759.\n",
      "[I 2024-08-06 17:52:29,875] Trial 4 finished with value: 2.3335650000682318 and parameters: {'alpha': 0.0011473339876725086}. Best is trial 2 with value: 2.32867491973759.\n",
      "[I 2024-08-06 17:52:29,944] Trial 5 finished with value: 2.3456739995418765 and parameters: {'alpha': 0.00466458783232626}. Best is trial 2 with value: 2.32867491973759.\n",
      "[I 2024-08-06 17:52:30,035] Trial 6 finished with value: 2.332258213035038 and parameters: {'alpha': 0.000872008003438473}. Best is trial 2 with value: 2.32867491973759.\n",
      "[I 2024-08-06 17:52:30,082] Trial 7 finished with value: 2.512767460363136 and parameters: {'alpha': 0.03524517667769779}. Best is trial 2 with value: 2.32867491973759.\n",
      "[I 2024-08-06 17:52:30,123] Trial 8 finished with value: 3.0438464574393347 and parameters: {'alpha': 0.24419265849777688}. Best is trial 2 with value: 2.32867491973759.\n",
      "[I 2024-08-06 17:52:30,205] Trial 9 finished with value: 2.332405253129462 and parameters: {'alpha': 0.0009166785288855549}. Best is trial 2 with value: 2.32867491973759.\n",
      "[I 2024-08-06 17:52:31,113] Trial 10 finished with value: 2.327459476957167 and parameters: {'alpha': 0.00011954474421754765}. Best is trial 10 with value: 2.327459476957167.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.690e+00, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:32,132] Trial 11 finished with value: 2.3274080499347938 and parameters: {'alpha': 0.00011556046714980507}. Best is trial 11 with value: 2.3274080499347938.\n",
      "[I 2024-08-06 17:52:32,373] Trial 12 finished with value: 2.327586880385429 and parameters: {'alpha': 0.0001403081642506605}. Best is trial 11 with value: 2.3274080499347938.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.586e+01, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:33,355] Trial 13 finished with value: 2.327231887188545 and parameters: {'alpha': 0.00010272877423060049}. Best is trial 13 with value: 2.327231887188545.\n",
      "[I 2024-08-06 17:52:33,423] Trial 14 finished with value: 2.3492794696041277 and parameters: {'alpha': 0.005654697933198162}. Best is trial 13 with value: 2.327231887188545.\n",
      "[I 2024-08-06 17:52:33,479] Trial 15 finished with value: 2.420395482927309 and parameters: {'alpha': 0.018565980277801552}. Best is trial 13 with value: 2.327231887188545.\n",
      "[I 2024-08-06 17:52:33,623] Trial 16 finished with value: 2.3291436427509433 and parameters: {'alpha': 0.00031414321773781743}. Best is trial 13 with value: 2.327231887188545.\n",
      "[I 2024-08-06 17:52:33,690] Trial 17 finished with value: 2.3410716389641837 and parameters: {'alpha': 0.002990840430371718}. Best is trial 13 with value: 2.327231887188545.\n",
      "[I 2024-08-06 17:52:33,840] Trial 18 finished with value: 2.329437011142639 and parameters: {'alpha': 0.00033770301379394734}. Best is trial 13 with value: 2.327231887188545.\n",
      "[I 2024-08-06 17:52:33,884] Trial 19 finished with value: 3.6157795292052253 and parameters: {'alpha': 0.9283967997579475}. Best is trial 13 with value: 2.327231887188545.\n",
      "[I 2024-08-06 17:52:33,955] Trial 20 finished with value: 2.33632991993386 and parameters: {'alpha': 0.0016829777157443425}. Best is trial 13 with value: 2.327231887188545.\n",
      "[I 2024-08-06 17:52:34,202] Trial 21 finished with value: 2.327524977508862 and parameters: {'alpha': 0.00013100961117351435}. Best is trial 13 with value: 2.327231887188545.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.533e+00, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:35,127] Trial 22 finished with value: 2.3273580128959583 and parameters: {'alpha': 0.00011232198018909452}. Best is trial 13 with value: 2.327231887188545.\n",
      "[I 2024-08-06 17:52:35,296] Trial 23 finished with value: 2.3302118091579485 and parameters: {'alpha': 0.00039637687365212177}. Best is trial 13 with value: 2.327231887188545.\n",
      "[I 2024-08-06 17:52:35,924] Trial 24 finished with value: 2.3278800023665562 and parameters: {'alpha': 0.00019526314299563502}. Best is trial 13 with value: 2.327231887188545.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.317e+01, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:36,960] Trial 25 finished with value: 2.3272804487089953 and parameters: {'alpha': 0.00010722482937026595}. Best is trial 13 with value: 2.327231887188545.\n",
      "[I 2024-08-06 17:52:37,111] Trial 26 finished with value: 2.33099366770607 and parameters: {'alpha': 0.0004910687652811159}. Best is trial 13 with value: 2.327231887188545.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.662e+01, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:38,007] Trial 27 finished with value: 2.3272485493229436 and parameters: {'alpha': 0.0001045328613806108}. Best is trial 13 with value: 2.327231887188545.\n",
      "[I 2024-08-06 17:52:38,088] Trial 28 finished with value: 2.3395291366666595 and parameters: {'alpha': 0.002475201884724804}. Best is trial 13 with value: 2.327231887188545.\n",
      "[I 2024-08-06 17:52:38,317] Trial 29 finished with value: 2.3313238064117225 and parameters: {'alpha': 0.0005512307337409641}. Best is trial 13 with value: 2.327231887188545.\n",
      "[I 2024-08-06 17:52:38,576] Trial 30 finished with value: 2.32825671167611 and parameters: {'alpha': 0.00022963302314278335}. Best is trial 13 with value: 2.327231887188545.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.630e+01, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:39,679] Trial 31 finished with value: 2.327261319108773 and parameters: {'alpha': 0.00010589098706521249}. Best is trial 13 with value: 2.327231887188545.\n",
      "[I 2024-08-06 17:52:40,264] Trial 32 finished with value: 2.327910405312942 and parameters: {'alpha': 0.00019810067684532031}. Best is trial 13 with value: 2.327231887188545.\n",
      "[I 2024-08-06 17:52:41,204] Trial 33 finished with value: 2.327820717551269 and parameters: {'alpha': 0.00018947569953243855}. Best is trial 13 with value: 2.327231887188545.\n",
      "[I 2024-08-06 17:52:41,361] Trial 34 finished with value: 2.3308113068567904 and parameters: {'alpha': 0.0004571680720910379}. Best is trial 13 with value: 2.327231887188545.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.476e+01, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:42,433] Trial 35 finished with value: 2.3272103944429237 and parameters: {'alpha': 0.00010019086642353595}. Best is trial 35 with value: 2.3272103944429237.\n",
      "[I 2024-08-06 17:52:42,775] Trial 36 finished with value: 2.328554245179609 and parameters: {'alpha': 0.000258777819463385}. Best is trial 35 with value: 2.3272103944429237.\n",
      "[I 2024-08-06 17:52:42,861] Trial 37 finished with value: 2.335221614870342 and parameters: {'alpha': 0.0013905795017146775}. Best is trial 35 with value: 2.3272103944429237.\n",
      "[I 2024-08-06 17:52:42,916] Trial 38 finished with value: 2.659225801783845 and parameters: {'alpha': 0.060236463890662124}. Best is trial 35 with value: 2.3272103944429237.\n",
      "[I 2024-08-06 17:52:43,069] Trial 39 finished with value: 2.331792193385255 and parameters: {'alpha': 0.0006998586920496344}. Best is trial 35 with value: 2.3272103944429237.\n",
      "[I 2024-08-06 17:52:43,141] Trial 40 finished with value: 2.415208248465658 and parameters: {'alpha': 0.017647016340741027}. Best is trial 35 with value: 2.3272103944429237.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.006e+00, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:44,246] Trial 41 finished with value: 2.3273399092438356 and parameters: {'alpha': 0.00011118031846088085}. Best is trial 35 with value: 2.3272103944429237.\n",
      "[I 2024-08-06 17:52:45,084] Trial 42 finished with value: 2.3276775049079017 and parameters: {'alpha': 0.00017436234710213914}. Best is trial 35 with value: 2.3272103944429237.\n",
      "[I 2024-08-06 17:52:45,241] Trial 43 finished with value: 2.329305546038294 and parameters: {'alpha': 0.0003272482830792246}. Best is trial 35 with value: 2.3272103944429237.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.648e+01, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:46,115] Trial 44 finished with value: 2.3272465212320674 and parameters: {'alpha': 0.00010432187948486156}. Best is trial 35 with value: 2.3272103944429237.\n",
      "[I 2024-08-06 17:52:46,382] Trial 45 finished with value: 2.3283018537754905 and parameters: {'alpha': 0.00023501179723322546}. Best is trial 35 with value: 2.3272103944429237.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.781e+00, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:47,269] Trial 46 finished with value: 2.327600066936403 and parameters: {'alpha': 0.00016099790809321991}. Best is trial 35 with value: 2.3272103944429237.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.508e+01, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:48,116] Trial 47 finished with value: 2.3272113004146306 and parameters: {'alpha': 0.00010026752039143872}. Best is trial 35 with value: 2.3272103944429237.\n",
      "[I 2024-08-06 17:52:48,215] Trial 48 finished with value: 2.3320458976207847 and parameters: {'alpha': 0.0007949230484043892}. Best is trial 35 with value: 2.3272103944429237.\n",
      "[I 2024-08-06 17:52:48,394] Trial 49 finished with value: 2.3290367707617126 and parameters: {'alpha': 0.00030534076726548723}. Best is trial 35 with value: 2.3272103944429237.\n",
      "Best hyperparameters:  {'alpha': 0.00010019086642353595}\n",
      "Performance with tuned hyperparameters:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  53.145316412638486\n",
      "Mean Squared Error (MSE):  11296.659317336738\n",
      "R-squared (R2):  0.45590864444556545\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  55.487909473116126\n",
      "Mean Squared Error (MSE):  16420.79654318927\n",
      "R-squared (R2):  0.38932221277364876\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  55.83596047165522\n",
      "Mean Squared Error (MSE):  12229.334267126262\n",
      "R-squared (R2):  0.46112988032512636\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.476e+01, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'date']\n",
    "\n",
    "# Prepare the data by dropping unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the features and the target variable\n",
    "def scale_features_and_target(X, y, excluded_columns):\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    y_scaled = scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "    return X_scaled, y_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train_raw, y_train_raw = prepare_data(train_data_paris, 'price')\n",
    "X_valid_raw, y_valid_raw = prepare_data(valid_data_paris, 'price')\n",
    "X_test_raw, y_test_raw = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Apply scaling to both features and target\n",
    "X_train_scaled, y_train_scaled, scaler = scale_features_and_target(X_train_raw, y_train_raw, excluded_columns)\n",
    "X_valid_scaled, y_valid_scaled, _ = scale_features_and_target(X_valid_raw, y_valid_raw, excluded_columns)\n",
    "X_test_scaled, y_test_scaled, _ = scale_features_and_target(X_test_raw, y_test_raw, excluded_columns)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'alpha': trial.suggest_float('alpha', 0.0001, 1.0, log=True)\n",
    "    }\n",
    "\n",
    "    model = Lasso(**param)\n",
    "    model.fit(X_train_scaled, y_train_scaled)\n",
    "    preds = model.predict(X_valid_scaled)\n",
    "    mse = mean_squared_error(y_valid_scaled, preds)\n",
    "    return mse\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = Lasso(**best_params)\n",
    "best_model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_scaled = best_model.predict(X_train_scaled)\n",
    "y_valid_pred_scaled = best_model.predict(X_valid_scaled)\n",
    "y_test_pred_scaled = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Inverse transform the scaled predictions and actual target values\n",
    "y_train_pred = scaler.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_valid_pred = scaler.inverse_transform(y_valid_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_pred = scaler.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_train_orig = scaler.inverse_transform(y_train_scaled.reshape(-1, 1)).flatten()\n",
    "y_valid_orig = scaler.inverse_transform(y_valid_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_orig = scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate performance metrics on the original scale\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with tuned hyperparameters:\")\n",
    "print_metrics(y_train_orig, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid_orig, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test_orig, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8155aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'lasso_model_without_lstm_paris.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('lasso_model_without_lstm_paris.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1856bdf4",
   "metadata": {},
   "source": [
    "## With LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18b231b",
   "metadata": {},
   "source": [
    "#### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55d72c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:49,461] A new study created in memory with name: no-name-294090ad-197e-49ec-a0c0-38a82724df0c\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371f35009d1c4afc99e927de3a9ab2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:49,723] Trial 0 finished with value: 0.2255752565709349 and parameters: {'alpha': 0.008858446616582938}. Best is trial 0 with value: 0.2255752565709349.\n",
      "[I 2024-08-06 17:52:49,815] Trial 1 finished with value: 0.257752108128849 and parameters: {'alpha': 0.1212408147333666}. Best is trial 0 with value: 0.2255752565709349.\n",
      "[I 2024-08-06 17:52:50,030] Trial 2 finished with value: 0.22621838507382225 and parameters: {'alpha': 0.01137514756482503}. Best is trial 0 with value: 0.2255752565709349.\n",
      "[I 2024-08-06 17:52:50,222] Trial 3 finished with value: 0.2303823429567226 and parameters: {'alpha': 0.022873262518559548}. Best is trial 0 with value: 0.2255752565709349.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.618e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:51,010] Trial 4 finished with value: 0.2009896431317697 and parameters: {'alpha': 0.000491596620249169}. Best is trial 4 with value: 0.2009896431317697.\n",
      "[I 2024-08-06 17:52:51,176] Trial 5 finished with value: 0.2311966360477681 and parameters: {'alpha': 0.02597525919412444}. Best is trial 4 with value: 0.2009896431317697.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.544e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:51,837] Trial 6 finished with value: 0.19553747032388522 and parameters: {'alpha': 0.0008983837347601944}. Best is trial 6 with value: 0.19553747032388522.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.039e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:52,331] Trial 7 finished with value: 0.21994798838587218 and parameters: {'alpha': 0.004411164160355902}. Best is trial 6 with value: 0.19553747032388522.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.622e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:52,967] Trial 8 finished with value: 0.203650828702659 and parameters: {'alpha': 0.0003197939441813462}. Best is trial 6 with value: 0.19553747032388522.\n",
      "[I 2024-08-06 17:52:53,016] Trial 9 finished with value: 0.6195752010683229 and parameters: {'alpha': 0.584690331748756}. Best is trial 6 with value: 0.19553747032388522.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.288e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:53,576] Trial 10 finished with value: 0.1900891608846963 and parameters: {'alpha': 0.001487132307813142}. Best is trial 10 with value: 0.1900891608846963.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.430e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:54,182] Trial 11 finished with value: 0.1925625961044875 and parameters: {'alpha': 0.001215412435800426}. Best is trial 10 with value: 0.1900891608846963.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.598e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:54,915] Trial 12 finished with value: 0.20754514657065226 and parameters: {'alpha': 0.00010691052548007904}. Best is trial 10 with value: 0.1900891608846963.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.100e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:55,503] Trial 13 finished with value: 0.18715824433992315 and parameters: {'alpha': 0.0018007910910036523}. Best is trial 13 with value: 0.18715824433992315.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.942e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:56,162] Trial 14 finished with value: 0.18516812559809806 and parameters: {'alpha': 0.001998722807056112}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.195e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:56,827] Trial 15 finished with value: 0.19754134972720988 and parameters: {'alpha': 0.002669062770853728}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.602e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:57,826] Trial 16 finished with value: 0.2072235260358084 and parameters: {'alpha': 0.00012320681481570902}. Best is trial 14 with value: 0.18516812559809806.\n",
      "[I 2024-08-06 17:52:57,986] Trial 17 finished with value: 0.2397952433962178 and parameters: {'alpha': 0.07124150436229157}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.159e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:58,579] Trial 18 finished with value: 0.21583321269967284 and parameters: {'alpha': 0.004129634702503188}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.621e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:52:59,348] Trial 19 finished with value: 0.20167843220200996 and parameters: {'alpha': 0.00044513748215998085}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.280e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:53:00,031] Trial 20 finished with value: 0.19306243977460852 and parameters: {'alpha': 0.0023515199149119993}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.389e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:53:00,740] Trial 21 finished with value: 0.1917360581150905 and parameters: {'alpha': 0.0013017962650111504}. Best is trial 14 with value: 0.18516812559809806.\n",
      "[I 2024-08-06 17:53:00,986] Trial 22 finished with value: 0.22524624440986662 and parameters: {'alpha': 0.00703851574645128}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.559e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:53:01,672] Trial 23 finished with value: 0.18786547124950914 and parameters: {'alpha': 0.002075144730469207}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.620e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:53:02,589] Trial 24 finished with value: 0.204365754927823 and parameters: {'alpha': 0.00027724993777250275}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.586e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:53:03,302] Trial 25 finished with value: 0.19791427122063987 and parameters: {'alpha': 0.0007096164838713097}. Best is trial 14 with value: 0.18516812559809806.\n",
      "[I 2024-08-06 17:53:03,513] Trial 26 finished with value: 0.2283547713644614 and parameters: {'alpha': 0.017542864283924806}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.199e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:53:04,024] Trial 27 finished with value: 0.19726743762182097 and parameters: {'alpha': 0.0026486254485414517}. Best is trial 14 with value: 0.18516812559809806.\n",
      "[I 2024-08-06 17:53:04,154] Trial 28 finished with value: 0.23788299865841883 and parameters: {'alpha': 0.06280110289982584}. Best is trial 14 with value: 0.18516812559809806.\n",
      "[I 2024-08-06 17:53:04,422] Trial 29 finished with value: 0.22520460920756993 and parameters: {'alpha': 0.006637049113450457}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:53:05,157] Trial 30 finished with value: 0.20603025785568296 and parameters: {'alpha': 0.00018480246961344104}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:53:05,736] Trial 31 finished with value: 0.18902779088124372 and parameters: {'alpha': 0.001611526750440484}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.952e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:53:06,320] Trial 32 finished with value: 0.18528479127509467 and parameters: {'alpha': 0.0019868892025790993}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.155e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:53:06,816] Trial 33 finished with value: 0.2168211754507151 and parameters: {'alpha': 0.004204236077333674}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.597e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:53:07,542] Trial 34 finished with value: 0.19884056995428806 and parameters: {'alpha': 0.000642146101151764}. Best is trial 14 with value: 0.18516812559809806.\n",
      "[I 2024-08-06 17:53:07,748] Trial 35 finished with value: 0.22631344490547922 and parameters: {'alpha': 0.011677143422646892}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.095e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:53:08,325] Trial 36 finished with value: 0.18709166651275838 and parameters: {'alpha': 0.001807272315457321}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.547e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:53:08,921] Trial 37 finished with value: 0.19570001454685207 and parameters: {'alpha': 0.0008843876749964448}. Best is trial 14 with value: 0.18516812559809806.\n",
      "[I 2024-08-06 17:53:09,149] Trial 38 finished with value: 0.22582370639537677 and parameters: {'alpha': 0.00999160979897215}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:53:09,782] Trial 39 finished with value: 0.21819608426614565 and parameters: {'alpha': 0.004307320236266625}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.622e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:53:10,620] Trial 40 finished with value: 0.20207312142132008 and parameters: {'alpha': 0.00041902041371816886}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.235e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:53:11,228] Trial 41 finished with value: 0.1949311688732217 and parameters: {'alpha': 0.0024773920646298715}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.274e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:53:11,774] Trial 42 finished with value: 0.1898960519277648 and parameters: {'alpha': 0.0015099167529484267}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.452e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:53:12,287] Trial 43 finished with value: 0.189441896583931 and parameters: {'alpha': 0.0021476779786558735}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.515e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:53:13,026] Trial 44 finished with value: 0.1943891124593164 and parameters: {'alpha': 0.0009988215364198656}. Best is trial 14 with value: 0.18516812559809806.\n",
      "[I 2024-08-06 17:53:13,272] Trial 45 finished with value: 0.22758308728701798 and parameters: {'alpha': 0.015496368557167744}. Best is trial 14 with value: 0.18516812559809806.\n",
      "[I 2024-08-06 17:53:13,460] Trial 46 finished with value: 0.23280379040543794 and parameters: {'alpha': 0.030812002196498364}. Best is trial 14 with value: 0.18516812559809806.\n",
      "[I 2024-08-06 17:53:13,520] Trial 47 finished with value: 0.37307488617962914 and parameters: {'alpha': 0.34201943084930836}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.924e+00, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:53:14,059] Trial 48 finished with value: 0.22659161198315983 and parameters: {'alpha': 0.005867621849451897}. Best is trial 14 with value: 0.18516812559809806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.173e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 17:53:14,602] Trial 49 finished with value: 0.20935177688607173 and parameters: {'alpha': 0.0036269197281932515}. Best is trial 14 with value: 0.18516812559809806.\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  10.722141422180355\n",
      "Mean Squared Error (MSE):  873.2135584240642\n",
      "R-squared (R2):  0.9579426151267284\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  9.385271628614133\n",
      "Mean Squared Error (MSE):  1306.5462942201798\n",
      "R-squared (R2):  0.9514104691715397\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  10.30041852627656\n",
      "Mean Squared Error (MSE):  1123.0592430004785\n",
      "R-squared (R2):  0.9505138173952414\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.942e+02, tolerance: 1.766e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'date']\n",
    "\n",
    "# Prepare the data by dropping unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the features and the target variable\n",
    "def scale_features_and_target(X, y, excluded_columns):\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    y_scaled = scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "    return X_scaled, y_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train_raw, y_train_raw = prepare_data(train_data_lstm_paris, 'price')\n",
    "X_valid_raw, y_valid_raw = prepare_data(valid_data_lstm_paris, 'price')\n",
    "X_test_raw, y_test_raw = prepare_data(test_data_lstm_paris, 'price')\n",
    "\n",
    "# Apply scaling to both features and target\n",
    "X_train_scaled, y_train_scaled, scaler = scale_features_and_target(X_train_raw, y_train_raw, excluded_columns)\n",
    "X_valid_scaled, y_valid_scaled, _ = scale_features_and_target(X_valid_raw, y_valid_raw, excluded_columns)\n",
    "X_test_scaled, y_test_scaled, _ = scale_features_and_target(X_test_raw, y_test_raw, excluded_columns)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'alpha': trial.suggest_float('alpha', 0.0001, 1.0, log=True)\n",
    "    }\n",
    "\n",
    "    model = Lasso(**param)\n",
    "    model.fit(X_train_scaled, y_train_scaled)\n",
    "    preds = model.predict(X_valid_scaled)\n",
    "    mse = mean_squared_error(y_valid_scaled, preds)\n",
    "    return mse\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters and train the final model\n",
    "best_params = study.best_params\n",
    "best_model = Lasso(**best_params)\n",
    "best_model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_scaled = best_model.predict(X_train_scaled)\n",
    "y_valid_pred_scaled = best_model.predict(X_valid_scaled)\n",
    "y_test_pred_scaled = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Inverse transform the scaled predictions and actual target values\n",
    "y_train_pred = scaler.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_valid_pred = scaler.inverse_transform(y_valid_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_pred = scaler.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_train_orig = scaler.inverse_transform(y_train_scaled.reshape(-1, 1)).flatten()\n",
    "y_valid_orig = scaler.inverse_transform(y_valid_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_orig = scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate performance metrics on the original scale\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print_metrics(y_train_orig, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid_orig, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test_orig, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8cc86ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'lasso_model_with_lstm_paris.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('lasso_model_with_lstm_paris.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
