{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7fbd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Data Cleaning Listings/FINAL_TRAIN_DATA_PARIS.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "train_data_paris= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Data Cleaning Listings/FINAL_VALID_DATA_PARIS.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "valid_data_paris= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Data Cleaning Listings/FINAL_TEST_DATA_PARIS.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "test_data_paris= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aea2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_paris.info()\n",
    "valid_data_paris.info()\n",
    "test_data_paris.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b69d0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data_paris, valid_data_paris, and test_data_paris are already defined\n",
    "\n",
    "# Concatenate the datasets\n",
    "combined_data = pd.concat([train_data_paris, valid_data_paris, test_data_paris], ignore_index=True)\n",
    "\n",
    "# Display information about the combined dataset\n",
    "combined_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1449a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.to_csv('Combined_data_paris.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b511a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the 'date' column is in datetime format\n",
    "combined_data['date'] = pd.to_datetime(combined_data['date'])\n",
    "\n",
    "# Group by date and calculate the mean price\n",
    "mean_price_over_time = combined_data.groupby('date')['price'].mean()\n",
    "\n",
    "# Plot the mean price over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(mean_price_over_time.index, mean_price_over_time.values, marker='o', linestyle='-')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mean Price')\n",
    "plt.title('Mean Price Over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97298c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the necessary imports have been done\n",
    "# import pandas as pd\n",
    "\n",
    "# 1. Extract Date and Price Columns\n",
    "date_price_data = combined_data[['date', 'price']]\n",
    "\n",
    "# 2. Convert Date to Day of the Week\n",
    "date_price_data['day_of_week'] = pd.to_datetime(date_price_data['date']).dt.day_name()\n",
    "\n",
    "# 3. Group By Day of the Week and 4. Calculate Mean Price\n",
    "average_price_per_day = date_price_data.groupby('day_of_week')['price'].mean().reset_index()\n",
    "\n",
    "# 5. Output Results\n",
    "print(average_price_per_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a096d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the necessary imports have been done\n",
    "# import pandas as pd\n",
    "\n",
    "# 1. Extract Date and Price Columns\n",
    "date_price_data = combined_data[['date', 'price']]\n",
    "\n",
    "# 2. Convert Date to Day of the Week\n",
    "date_price_data['day_of_week'] = pd.to_datetime(date_price_data['date']).dt.day_name()\n",
    "\n",
    "# 3. Classify Days as Weekdays or Weekends\n",
    "def classify_day(day):\n",
    "    if day in ['Saturday', 'Sunday']:\n",
    "        return 'Weekend'\n",
    "    else:\n",
    "        return 'Weekday'\n",
    "\n",
    "date_price_data['day_type'] = date_price_data['day_of_week'].apply(classify_day)\n",
    "\n",
    "# 4. Calculate the Average Price for Weekdays and Weekends\n",
    "average_price_by_day_type = date_price_data.groupby('day_type')['price'].mean()\n",
    "\n",
    "# 5. Compute the Difference\n",
    "average_weekday_price = average_price_by_day_type['Weekday']\n",
    "average_weekend_price = average_price_by_day_type['Weekend']\n",
    "average_difference = average_weekday_price - average_weekend_price\n",
    "\n",
    "print(f\"Average Weekday Price: {average_weekday_price}\")\n",
    "print(f\"Average Weekend Price: {average_weekend_price}\")\n",
    "print(f\"Average Difference: {average_difference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b7d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the unique IDs in the combined dataset\n",
    "unique_ids_count = combined_data['id'].nunique()\n",
    "\n",
    "# Display the count of unique IDs\n",
    "print(f\"Number of unique IDs in the combined dataset: {unique_ids_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa233a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the dataset is already loaded into a DataFrame named 'df'\n",
    "# Ensure the 'date' column is in datetime format\n",
    "combined_data['date'] = pd.to_datetime(combined_data['date'])\n",
    "\n",
    "# Get the first and last date\n",
    "first_date = combined_data['date'].min()\n",
    "last_date = combined_data['date'].max()\n",
    "\n",
    "print(f\"First date in the dataset: {first_date}\")\n",
    "print(f\"Last date in the dataset: {last_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cc0905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data_paris, valid_data_paris, and test_data_paris are already defined\n",
    "\n",
    "# Count the unique IDs in each dataset\n",
    "train_unique_ids_count = train_data_paris['id'].nunique()\n",
    "valid_unique_ids_count = valid_data_paris['id'].nunique()\n",
    "test_unique_ids_count = test_data_paris['id'].nunique()\n",
    "\n",
    "# Display the count of unique IDs\n",
    "print(f\"Number of unique IDs in the training dataset: {train_unique_ids_count}\")\n",
    "print(f\"Number of unique IDs in the validation dataset: {valid_unique_ids_count}\")\n",
    "print(f\"Number of unique IDs in the test dataset: {test_unique_ids_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec801b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data_paris, valid_data_paris, and test_data_paris are already defined\n",
    "\n",
    "# Ensure the 'date' column is in datetime format for each dataset\n",
    "train_data_paris['date'] = pd.to_datetime(train_data_paris['date'])\n",
    "valid_data_paris['date'] = pd.to_datetime(valid_data_paris['date'])\n",
    "test_data_paris['date'] = pd.to_datetime(test_data_paris['date'])\n",
    "\n",
    "# Get the first and last date for each dataset\n",
    "train_first_date = train_data_paris['date'].min()\n",
    "train_last_date = train_data_paris['date'].max()\n",
    "\n",
    "valid_first_date = valid_data_paris['date'].min()\n",
    "valid_last_date = valid_data_paris['date'].max()\n",
    "\n",
    "test_first_date = test_data_paris['date'].min()\n",
    "test_last_date = test_data_paris['date'].max()\n",
    "\n",
    "# Display the first and last date for each dataset\n",
    "print(f\"First date in the training dataset: {train_first_date}\")\n",
    "print(f\"Last date in the training dataset: {train_last_date}\")\n",
    "\n",
    "print(f\"First date in the validation dataset: {valid_first_date}\")\n",
    "print(f\"Last date in the validation dataset: {valid_last_date}\")\n",
    "\n",
    "print(f\"First date in the test dataset: {test_first_date}\")\n",
    "print(f\"Last date in the test dataset: {test_last_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0fa7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1700ad5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f010848",
   "metadata": {},
   "source": [
    "# Engineering dataset for baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf9ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming train_data_paris, valid_data_paris, and test_data_paris are already defined\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Extract unique IDs from each dataset\n",
    "unique_train_ids = train_data_paris['id'].unique()\n",
    "unique_valid_ids = valid_data_paris['id'].unique()\n",
    "unique_test_ids = test_data_paris['id'].unique()\n",
    "\n",
    "# Randomly select 6000 unique IDs for training, 2000 for validation, and 2000 for testing\n",
    "selected_train_ids = np.random.choice(unique_train_ids, 6000, replace=False)\n",
    "selected_valid_ids = np.random.choice(unique_valid_ids, 2000, replace=False)\n",
    "selected_test_ids = np.random.choice(unique_test_ids, 2000, replace=False)\n",
    "\n",
    "# Ensure IDs are exclusive to each dataset\n",
    "selected_train_ids = set(selected_train_ids)\n",
    "selected_valid_ids = set(selected_valid_ids)\n",
    "selected_test_ids = set(selected_test_ids)\n",
    "\n",
    "# Filter datasets based on the selected unique IDs\n",
    "train_data_paris_filtered = train_data_paris[train_data_paris['id'].isin(selected_train_ids)]\n",
    "valid_data_paris_filtered = valid_data_paris[valid_data_paris['id'].isin(selected_valid_ids)]\n",
    "test_data_paris_filtered = test_data_paris[test_data_paris['id'].isin(selected_test_ids)]\n",
    "\n",
    "# Check the amount of unique IDs in the filtered datasets\n",
    "train_unique_count = len(train_data_paris_filtered['id'].unique())\n",
    "valid_unique_count = len(valid_data_paris_filtered['id'].unique())\n",
    "test_unique_count = len(test_data_paris_filtered['id'].unique())\n",
    "\n",
    "# Display the result\n",
    "print(\"Filtered Train Data Info:\")\n",
    "print(train_data_paris_filtered.info())\n",
    "print(f\"Number of unique IDs in filtered train data: {train_unique_count}\")\n",
    "\n",
    "print(\"\\nFiltered Validation Data Info:\")\n",
    "print(valid_data_paris_filtered.info())\n",
    "print(f\"Number of unique IDs in filtered validation data: {valid_unique_count}\")\n",
    "\n",
    "print(\"\\nFiltered Test Data Info:\")\n",
    "print(test_data_paris_filtered.info())\n",
    "print(f\"Number of unique IDs in filtered test data: {test_unique_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39147289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data_paris, valid_data_paris, and test_data_paris are already defined\n",
    "\n",
    "# Function to count the number of unique days and get the start and end dates in the dataset\n",
    "def analyze_dates(data):\n",
    "    unique_days = data['date'].nunique()\n",
    "    start_date = data['date'].min()\n",
    "    end_date = data['date'].max()\n",
    "    return unique_days, start_date, end_date\n",
    "\n",
    "# Analyze the dates in each dataset\n",
    "train_unique_days, train_start_date, train_end_date = analyze_dates(train_data_paris_filtered)\n",
    "valid_unique_days, valid_start_date, valid_end_date = analyze_dates(valid_data_paris_filtered)\n",
    "test_unique_days, test_start_date, test_end_date = analyze_dates(test_data_paris_filtered)\n",
    "\n",
    "# Check for overlapping IDs\n",
    "train_ids = set(train_data_paris_filtered['id'])\n",
    "valid_ids = set(valid_data_paris_filtered['id'])\n",
    "test_ids = set(test_data_paris_filtered['id'])\n",
    "\n",
    "no_overlap_train_valid = train_ids.isdisjoint(valid_ids)\n",
    "no_overlap_train_test = train_ids.isdisjoint(test_ids)\n",
    "no_overlap_valid_test = valid_ids.isdisjoint(test_ids)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Train Data: Unique days: {train_unique_days}, Start date: {train_start_date}, End date: {train_end_date}\")\n",
    "print(f\"Validation Data: Unique days: {valid_unique_days}, Start date: {valid_start_date}, End date: {valid_end_date}\")\n",
    "print(f\"Test Data: Unique days: {test_unique_days}, Start date: {test_start_date}, End date: {test_end_date}\")\n",
    "\n",
    "print(f\"\\nNo overlapping IDs between train and validation sets: {no_overlap_train_valid}\")\n",
    "print(f\"No overlapping IDs between train and test sets: {no_overlap_train_test}\")\n",
    "print(f\"No overlapping IDs between validation and test sets: {no_overlap_valid_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c1c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_paris_filtered.to_csv('Train_model_Paris_Vince.csv', index=False)\n",
    "valid_data_paris_filtered.to_csv('Valid_model_Paris_Vince.csv', index=False)\n",
    "test_data_paris_filtered.to_csv('Test_model_Paris_Vince.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff32dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/Train_model_Paris_Vince.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "train_data_paris_filtered= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/Valid_model_Paris_Vince.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "valid_data_paris_filtered= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/Test_model_Paris_Vince.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "test_data_paris_filtered= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e003b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/cleaned lstm/train_data_lstm_paris_clean.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "train_data_lstm_paris= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/cleaned lstm/val_data_lstm_paris_clean.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "valid_data_lstm_paris= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/cleaned lstm/test_data_lstm_paris_clean.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "test_data_lstm_paris= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce51e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_lstm_paris.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90789fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to count the number of unique days and get the start and end dates in the dataset\n",
    "def analyze_dates(data):\n",
    "    unique_days = data['date'].nunique()\n",
    "    start_date = data['date'].min()\n",
    "    end_date = data['date'].max()\n",
    "    return unique_days, start_date, end_date\n",
    "\n",
    "# Analyze the dates in each dataset\n",
    "train_unique_days, train_start_date, train_end_date = analyze_dates(train_data_lstm_paris)\n",
    "valid_unique_days, valid_start_date, valid_end_date = analyze_dates(valid_data_lstm_paris)\n",
    "test_unique_days, test_start_date, test_end_date = analyze_dates(test_data_lstm_paris)\n",
    "\n",
    "# Check for overlapping IDs\n",
    "train_ids = set(train_data_lstm_paris['id'])\n",
    "valid_ids = set(valid_data_lstm_paris['id'])\n",
    "test_ids = set(test_data_lstm_paris['id'])\n",
    "\n",
    "no_overlap_train_valid = train_ids.isdisjoint(valid_ids)\n",
    "no_overlap_train_test = train_ids.isdisjoint(test_ids)\n",
    "no_overlap_valid_test = valid_ids.isdisjoint(test_ids)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Train Data: Unique days: {train_unique_days}, Start date: {train_start_date}, End date: {train_end_date}\")\n",
    "print(f\"Validation Data: Unique days: {valid_unique_days}, Start date: {valid_start_date}, End date: {valid_end_date}\")\n",
    "print(f\"Test Data: Unique days: {test_unique_days}, Start date: {test_start_date}, End date: {test_end_date}\")\n",
    "\n",
    "print(f\"\\nNo overlapping IDs between train and validation sets: {no_overlap_train_valid}\")\n",
    "print(f\"No overlapping IDs between train and test sets: {no_overlap_train_test}\")\n",
    "print(f\"No overlapping IDs between validation and test sets: {no_overlap_valid_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d60cde8",
   "metadata": {},
   "source": [
    "#### Merging the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f907b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data_paris_filtered, valid_data_paris_filtered, test_data_paris_filtered,\n",
    "# train_data_lstm_paris, valid_data_lstm_paris, and test_data_lstm_paris are already defined\n",
    "\n",
    "# Extract the last 16 columns from the LSTM datasets\n",
    "train_last_16_columns = train_data_lstm_paris.iloc[:, -16:]\n",
    "valid_last_16_columns = valid_data_lstm_paris.iloc[:, -16:]\n",
    "test_last_16_columns = test_data_lstm_paris.iloc[:, -16:]\n",
    "\n",
    "# Include the 'id' and 'date' columns with the last 16 columns\n",
    "train_last_16_columns = pd.concat([train_data_lstm_paris[['id', 'date']], train_last_16_columns], axis=1)\n",
    "valid_last_16_columns = pd.concat([valid_data_lstm_paris[['id', 'date']], valid_last_16_columns], axis=1)\n",
    "test_last_16_columns = pd.concat([test_data_lstm_paris[['id', 'date']], test_last_16_columns], axis=1)\n",
    "\n",
    "# Perform left merges with the corresponding filtered datasets on 'id' and 'date'\n",
    "merged_train_data = pd.merge(train_data_paris_filtered, train_last_16_columns, on=['id', 'date'], how='left')\n",
    "merged_valid_data = pd.merge(valid_data_paris_filtered, valid_last_16_columns, on=['id', 'date'], how='left')\n",
    "merged_test_data = pd.merge(test_data_paris_filtered, test_last_16_columns, on=['id', 'date'], how='left')\n",
    "\n",
    "# Filter to only include IDs present in the LSTM datasets\n",
    "final_train_data = merged_train_data[merged_train_data['id'].isin(train_data_lstm_paris['id'])]\n",
    "final_valid_data = merged_valid_data[merged_valid_data['id'].isin(valid_data_lstm_paris['id'])]\n",
    "final_test_data = merged_test_data[merged_test_data['id'].isin(test_data_lstm_paris['id'])]\n",
    "\n",
    "# Display the resulting datasets (optional)\n",
    "print(\"Final Train Data:\")\n",
    "print(final_train_data.info())\n",
    "print(\"\\nFinal Valid Data:\")\n",
    "print(final_valid_data.info())\n",
    "print(\"\\nFinal Test Data:\")\n",
    "print(final_test_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdb8d94",
   "metadata": {},
   "source": [
    "#### Without the LSTM features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b850bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data_paris, valid_data_paris, and test_data_paris are already defined\n",
    "\n",
    "# Function to select the last day for each ID in the given dataframe\n",
    "def select_last_day_per_id(data):\n",
    "    data = data.sort_values(by=['id', 'date'])\n",
    "    last_day_data = data.groupby('id').tail(1)\n",
    "    return last_day_data\n",
    "\n",
    "# Apply the function to each split\n",
    "train_data_paris_last_day = select_last_day_per_id(train_data_paris_filtered)\n",
    "valid_data_paris_last_day = select_last_day_per_id(valid_data_paris_filtered)\n",
    "test_data_paris_last_day = select_last_day_per_id(test_data_paris_filtered)\n",
    "\n",
    "# Calculate the total unique IDs across all splits\n",
    "total_unique_ids = set(train_data_paris_filtered['id']).union(valid_data_paris_filtered['id']).union(test_data_paris_filtered['id'])\n",
    "total_unique_count = len(total_unique_ids)\n",
    "\n",
    "# Calculate the percentage of unique IDs in each split based on the total unique IDs\n",
    "train_unique_count = len(set(train_data_paris_last_day['id']))\n",
    "valid_unique_count = len(set(valid_data_paris_last_day['id']))\n",
    "test_unique_count = len(set(test_data_paris_last_day['id']))\n",
    "\n",
    "train_percentage = (train_unique_count / total_unique_count) * 100\n",
    "valid_percentage = (valid_unique_count / total_unique_count) * 100\n",
    "test_percentage = (test_unique_count / total_unique_count) * 100\n",
    "\n",
    "print(\"\\nPercentage of unique IDs per split:\")\n",
    "print(f\"Train: {train_percentage:.2f}%\")\n",
    "print(f\"Validation: {valid_percentage:.2f}%\")\n",
    "print(f\"Test: {test_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f2d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to get the first and last date in the dataset\n",
    "def get_first_last_dates(data):\n",
    "    first_date = data['date'].min()\n",
    "    last_date = data['date'].max()\n",
    "    return first_date, last_date\n",
    "\n",
    "# Get the first and last date for each dataset\n",
    "train_first_date, train_last_date = get_first_last_dates(train_data_paris_last_day)\n",
    "valid_first_date, valid_last_date = get_first_last_dates(valid_data_paris_last_day)\n",
    "test_first_date, test_last_date = get_first_last_dates(test_data_paris_last_day)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Train Data: First date: {train_first_date}, Last date: {train_last_date}\")\n",
    "print(f\"Validation Data: First date: {valid_first_date}, Last date: {valid_last_date}\")\n",
    "print(f\"Test Data: First date: {test_first_date}, Last date: {test_last_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdb1055",
   "metadata": {},
   "source": [
    "#### With the LSTM features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4088382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming merged_train_data, merged_valid_data, and merged_test_data are already defined\n",
    "\n",
    "# Function to select the last day for each ID in the given dataframe\n",
    "def select_last_day_per_id(data):\n",
    "    data = data.sort_values(by=['id', 'date'])\n",
    "    last_day_data = data.groupby('id').tail(1)\n",
    "    return last_day_data\n",
    "\n",
    "# Apply the function to each merged dataset\n",
    "merged_train_data_last_day = select_last_day_per_id(final_train_data)\n",
    "merged_valid_data_last_day = select_last_day_per_id(final_valid_data)\n",
    "merged_test_data_last_day = select_last_day_per_id(final_test_data)\n",
    "\n",
    "# Calculate the total unique IDs across all merged splits\n",
    "total_unique_ids = set(final_train_data['id']).union(final_valid_data['id']).union(final_test_data['id'])\n",
    "total_unique_count = len(total_unique_ids)\n",
    "\n",
    "# Calculate the percentage of unique IDs in each merged split based on the total unique IDs\n",
    "train_unique_count = len(set(merged_train_data_last_day['id']))\n",
    "valid_unique_count = len(set(merged_valid_data_last_day['id']))\n",
    "test_unique_count = len(set(merged_test_data_last_day['id']))\n",
    "\n",
    "train_percentage = (train_unique_count / total_unique_count) * 100\n",
    "valid_percentage = (valid_unique_count / total_unique_count) * 100\n",
    "test_percentage = (test_unique_count / total_unique_count) * 100\n",
    "\n",
    "print(\"\\nPercentage of unique IDs per split in the merged datasets:\")\n",
    "print(f\"Train: {train_percentage:.2f}%\")\n",
    "print(f\"Validation: {valid_percentage:.2f}%\")\n",
    "print(f\"Test: {test_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b3d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting unique IDs from each dataset\n",
    "train_ids = set(merged_train_data_last_day['id'])\n",
    "valid_ids = set(merged_valid_data_last_day['id'])\n",
    "test_ids = set(merged_test_data_last_day['id'])\n",
    "\n",
    "# Check for overlapping IDs between the datasets\n",
    "train_valid_overlap = train_ids.intersection(valid_ids)\n",
    "train_test_overlap = train_ids.intersection(test_ids)\n",
    "valid_test_overlap = valid_ids.intersection(test_ids)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Overlapping IDs between train and validation sets: {train_valid_overlap}\")\n",
    "print(f\"Number of overlapping IDs between train and validation sets: {len(train_valid_overlap)}\\n\")\n",
    "\n",
    "print(f\"Overlapping IDs between train and test sets: {train_test_overlap}\")\n",
    "print(f\"Number of overlapping IDs between train and test sets: {len(train_test_overlap)}\\n\")\n",
    "\n",
    "print(f\"Overlapping IDs between validation and test sets: {valid_test_overlap}\")\n",
    "print(f\"Number of overlapping IDs between validation and test sets: {len(valid_test_overlap)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bbabb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to get the first and last date in the dataset\n",
    "def get_first_last_dates(data):\n",
    "    first_date = data['date'].min()\n",
    "    last_date = data['date'].max()\n",
    "    return first_date, last_date\n",
    "\n",
    "# Get the first and last date for each dataset\n",
    "train_first_date, train_last_date = get_first_last_dates(merged_train_data_last_day)\n",
    "valid_first_date, valid_last_date = get_first_last_dates(merged_valid_data_last_day)\n",
    "test_first_date, test_last_date = get_first_last_dates(merged_test_data_last_day)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Merged Train Data: First date: {train_first_date}, Last date: {train_last_date}\")\n",
    "print(f\"Merged Validation Data: First date: {valid_first_date}, Last date: {valid_last_date}\")\n",
    "print(f\"Merged Test Data: First date: {test_first_date}, Last date: {test_last_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221dc26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train_data_last_day.to_csv('Baslinemodels_train_lstm_paris.csv', index=False)\n",
    "merged_valid_data_last_day.to_csv('Baslinemodels_valid_lstm_paris.csv', index=False)\n",
    "merged_test_data_last_day.to_csv('Baslinemodels_test_lstm_paris.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422d322d",
   "metadata": {},
   "source": [
    "#### Filtering original dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d542893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming merged_train_data_last_day, merged_valid_data_last_day, merged_test_data_last_day,\n",
    "# train_data_paris_last_day, valid_data_paris_last_day, and test_data_paris_last_day are already defined\n",
    "\n",
    "# Extract unique IDs from the merged datasets\n",
    "unique_ids_train = merged_train_data_last_day['id'].unique()\n",
    "unique_ids_valid = merged_valid_data_last_day['id'].unique()\n",
    "unique_ids_test = merged_test_data_last_day['id'].unique()\n",
    "\n",
    "# Filter the last day datasets to only include rows with these unique IDs\n",
    "filtered_train_data_paris_last_day = train_data_paris_last_day[train_data_paris_last_day['id'].isin(unique_ids_train)]\n",
    "filtered_valid_data_paris_last_day = valid_data_paris_last_day[valid_data_paris_last_day['id'].isin(unique_ids_valid)]\n",
    "filtered_test_data_paris_last_day = test_data_paris_last_day[test_data_paris_last_day['id'].isin(unique_ids_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb7ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train_data_paris_last_day.to_csv('Baslinemodels_train_paris.csv', index=False)\n",
    "filtered_valid_data_paris_last_day.to_csv('Baslinemodels_valid_paris.csv', index=False)\n",
    "filtered_test_data_paris_last_day.to_csv('Baslinemodels_test_paris.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf404e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data_paris_last_day and merged_train_data_last_day are already defined\n",
    "\n",
    "# Check if the two datasets have the same number of rows\n",
    "same_number_of_rows = filtered_train_data_paris_last_day.shape[0] == merged_train_data_last_day.shape[0]\n",
    "\n",
    "# Determine the number of columns in train_data_paris_last_day\n",
    "train_num_columns = filtered_train_data_paris_last_day.shape[1]\n",
    "\n",
    "# Determine the number of columns in merged_train_data_last_day, excluding the last 16 columns\n",
    "merged_num_columns_to_compare = merged_train_data_last_day.shape[1] - 16\n",
    "\n",
    "# Ensure the columns to compare are the same in both datasets\n",
    "columns_to_compare = filtered_train_data_paris_last_day.columns[:train_num_columns]\n",
    "\n",
    "# Compare the content of the datasets excluding the last 16 columns in merged_train_data_last_day\n",
    "content_identical = filtered_train_data_paris_last_day[columns_to_compare].equals(\n",
    "    merged_train_data_last_day[columns_to_compare]\n",
    ")\n",
    "\n",
    "# Display the result of the comparison\n",
    "if same_number_of_rows and content_identical:\n",
    "    print(\"The datasets are identical except for the last 16 columns in merged_train_data_last_day.\")\n",
    "else:\n",
    "    print(\"The datasets are not identical.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a01f46",
   "metadata": {},
   "source": [
    "# Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78d7be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WITHOUT LSTM\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/Baslinemodels_train_paris.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "train_data_paris= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/Baslinemodels_valid_paris.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "valid_data_paris= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/Baslinemodels_test_paris.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "test_data_paris= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f64e34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WITH LSTM\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/Baslinemodels_train_lstm_paris.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "train_data_lstm_paris= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/Baslinemodels_valid_lstm_paris.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "valid_data_lstm_paris= pd.read_csv(file_path)\n",
    "\n",
    "import pandas as pd\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Baseline models/Baslinemodels_test_lstm_paris.csv'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "test_data_lstm_paris= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "219b7490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5734 entries, 0 to 5733\n",
      "Columns: 138 entries, id to kitchen_amenities\n",
      "dtypes: bool(98), float64(15), int64(24), object(1)\n",
      "memory usage: 2.3+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5734 entries, 0 to 5733\n",
      "Columns: 154 entries, id to lstm_feature_15\n",
      "dtypes: bool(98), float64(31), int64(24), object(1)\n",
      "memory usage: 3.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data_lstm_paris.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e424519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data - Columns with NaN values and their counts:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Valid Data - Columns with NaN values and their counts:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Test Data - Columns with NaN values and their counts:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data_lstm_paris, valid_data_lstm_paris, and test_data_lstm_paris are already defined\n",
    "\n",
    "# Check for NaN values in the train dataset\n",
    "train_nan_counts = train_data_lstm_paris.isna().sum()\n",
    "train_columns_with_nan = train_nan_counts[train_nan_counts > 0]\n",
    "print(\"Train Data - Columns with NaN values and their counts:\")\n",
    "print(train_columns_with_nan)\n",
    "\n",
    "# Check for NaN values in the valid dataset\n",
    "valid_nan_counts = valid_data_lstm_paris.isna().sum()\n",
    "valid_columns_with_nan = valid_nan_counts[valid_nan_counts > 0]\n",
    "print(\"\\nValid Data - Columns with NaN values and their counts:\")\n",
    "print(valid_columns_with_nan)\n",
    "\n",
    "# Check for NaN values in the test dataset\n",
    "test_nan_counts = test_data_lstm_paris.isna().sum()\n",
    "test_columns_with_nan = test_nan_counts[test_nan_counts > 0]\n",
    "print(\"\\nTest Data - Columns with NaN values and their counts:\")\n",
    "print(test_columns_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6b8b082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs are unique across the train, validation, and test datasets.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data_lstm_paris, valid_data_lstm_paris, and test_data_lstm_paris are DataFrames\n",
    "\n",
    "# Extract IDs from each dataset\n",
    "train_ids = set(train_data_lstm_paris['id'])\n",
    "valid_ids = set(valid_data_lstm_paris['id'])\n",
    "test_ids = set(test_data_lstm_paris['id'])\n",
    "\n",
    "# Check for overlapping IDs between the datasets\n",
    "overlap_train_valid = train_ids.intersection(valid_ids)\n",
    "overlap_train_test = train_ids.intersection(test_ids)\n",
    "overlap_valid_test = valid_ids.intersection(test_ids)\n",
    "\n",
    "# Output the results\n",
    "if not overlap_train_valid and not overlap_train_test and not overlap_valid_test:\n",
    "    print(\"IDs are unique across the train, validation, and test datasets.\")\n",
    "else:\n",
    "    print(\"There are overlapping IDs between the datasets.\")\n",
    "    if overlap_train_valid:\n",
    "        print(f\"Overlapping IDs between train and validation datasets: {overlap_train_valid}\")\n",
    "    if overlap_train_test:\n",
    "        print(f\"Overlapping IDs between train and test datasets: {overlap_train_test}\")\n",
    "    if overlap_valid_test:\n",
    "        print(f\"Overlapping IDs between validation and test datasets: {overlap_valid_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "358e0070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5734 entries, 0 to 5733\n",
      "Columns: 154 entries, id to lstm_feature_15\n",
      "dtypes: bool(98), float64(31), int64(24), object(1)\n",
      "memory usage: 3.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data_lstm_paris.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06a3aeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of all columns in the dataset:\n",
      "['id', 'date', 'available', 'price', 'weekday', 'season_Autumn', 'season_Winter', 'year', 'month', 'day', 'is_holiday', 'is_school_holiday', 'host_id', 'host_response_time', 'host_response_rate', 'host_is_superhost', 'host_listings_count', 'host_total_listings_count', 'host_identity_verified', 'latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'availability_90', 'availability_365', 'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'review_scores_rating', 'instant_bookable', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', 'reviews_per_month', 'Wine glasses', 'clothing storage', 'Elevator', 'bbq', 'Hangers', 'coffee', 'Private entrance', 'backyard', 'Luggage dropoff allowed', 'Microwave', 'conditioner', 'Dryer', 'Shower gel', 'Fire extinguisher', 'Dedicated workspace', 'Smoke alarm', 'tv', 'Baking sheet', 'crib', 'Freezer', 'exercise equipment', 'Hot water', 'shampoo', 'Kitchen', 'Free washer – In unit', 'Extra pillows and blankets', 'Portable fans', 'Dining table', 'Heating', 'Bed linens', 'Long term stays allowed', 'Hair dryer', 'wifi', 'hot tub', 'Toaster', 'Bathtub', 'soap', 'Drying rack for clothing', 'Iron', 'Room-darkening shades', 'Pets allowed', 'sauna', 'Hot water kettle', 'Laundromat nearby', 'gym', 'Washer', 'stove', 'First aid kit', 'Essentials', 'view', 'Lockbox', 'toys children', 'Books and reading material', 'oven', 'Self check-in', 'game console', 'sound system', 'Host greets you', 'Dishwasher', 'Cleaning products', 'parking', 'Carbon monoxide alarm', 'broadcast', 'pool', 'total_amenities', 'listing_reviewed', 'neighbourhood_Batignolles-Monceau', 'neighbourhood_Bourse', 'neighbourhood_Buttes-Chaumont', 'neighbourhood_Buttes-Montmartre', 'neighbourhood_Entrepôt', 'neighbourhood_Gobelins', 'neighbourhood_Hôtel-de-Ville', 'neighbourhood_Louvre', 'neighbourhood_Luxembourg', 'neighbourhood_Ménilmontant', 'neighbourhood_Observatoire', 'neighbourhood_Opéra', 'neighbourhood_Palais-Bourbon', 'neighbourhood_Panthéon', 'neighbourhood_Passy', 'neighbourhood_Popincourt', 'neighbourhood_Reuilly', 'neighbourhood_Temple', 'neighbourhood_Vaugirard', 'neighbourhood_Élysée', 'property_type_Apartment', 'property_type_Hotel', 'property_type_House', 'property_type_Other', 'room_type_Private room', 'Eiffel Tower Distance', 'Louvre Museum Distance', 'Notre-Dame Cathedral Distance', 'Sacré-Cœur Basilica Distance', 'Arc de Triomphe Distance', 'nearby_airbnbs_count', 'luxury_amenities_score', 'nearby_restaurants_bars', 'nearby_transport', 'mean_price_neighbors', 'kitchen_amenities', 'lstm_feature_0', 'lstm_feature_1', 'lstm_feature_2', 'lstm_feature_3', 'lstm_feature_4', 'lstm_feature_5', 'lstm_feature_6', 'lstm_feature_7', 'lstm_feature_8', 'lstm_feature_9', 'lstm_feature_10', 'lstm_feature_11', 'lstm_feature_12', 'lstm_feature_13', 'lstm_feature_14', 'lstm_feature_15']\n"
     ]
    }
   ],
   "source": [
    "# Print a list of all columns in the dataset\n",
    "columns_list = train_data_lstm_paris.columns.tolist()\n",
    "print(\"List of all columns in the dataset:\")\n",
    "print(columns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa037cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First date in the LSTM training dataset (Paris): 2023-12-30 00:00:00\n",
      "Last date in the LSTM training dataset (Paris): 2024-01-12 00:00:00\n",
      "First date in the LSTM validation dataset (Paris): 2024-02-10 00:00:00\n",
      "Last date in the LSTM validation dataset (Paris): 2024-02-11 00:00:00\n",
      "First date in the LSTM test dataset (Paris): 2024-03-11 00:00:00\n",
      "Last date in the LSTM test dataset (Paris): 2024-03-12 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure the 'date' column is in datetime format for each dataset\n",
    "train_data_lstm_paris['date'] = pd.to_datetime(train_data_lstm_paris['date'])\n",
    "valid_data_lstm_paris['date'] = pd.to_datetime(valid_data_lstm_paris['date'])\n",
    "test_data_lstm_paris['date'] = pd.to_datetime(test_data_lstm_paris['date'])\n",
    "\n",
    "# Get the first and last date for each dataset\n",
    "train_lstm_first_date = train_data_lstm_paris['date'].min()\n",
    "train_lstm_last_date = train_data_lstm_paris['date'].max()\n",
    "\n",
    "valid_lstm_first_date = valid_data_lstm_paris['date'].min()\n",
    "valid_lstm_last_date = valid_data_lstm_paris['date'].max()\n",
    "\n",
    "test_lstm_first_date = test_data_lstm_paris['date'].min()\n",
    "test_lstm_last_date = test_data_lstm_paris['date'].max()\n",
    "\n",
    "# Display the first and last date for each dataset\n",
    "print(f\"First date in the LSTM training dataset (Paris): {train_lstm_first_date}\")\n",
    "print(f\"Last date in the LSTM training dataset (Paris): {train_lstm_last_date}\")\n",
    "\n",
    "print(f\"First date in the LSTM validation dataset (Paris): {valid_lstm_first_date}\")\n",
    "print(f\"Last date in the LSTM validation dataset (Paris): {valid_lstm_last_date}\")\n",
    "\n",
    "print(f\"First date in the LSTM test dataset (Paris): {test_lstm_first_date}\")\n",
    "print(f\"Last date in the LSTM test dataset (Paris): {test_lstm_last_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d2860fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First date in the training dataset (Paris): 2023-12-30 00:00:00\n",
      "Last date in the training dataset (Paris): 2024-01-12 00:00:00\n",
      "First date in the validation dataset (Paris): 2024-02-10 00:00:00\n",
      "Last date in the validation dataset (Paris): 2024-02-11 00:00:00\n",
      "First date in the test dataset (Paris): 2024-03-11 00:00:00\n",
      "Last date in the test dataset (Paris): 2024-03-12 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure the 'date' column is in datetime format for each dataset\n",
    "train_data_paris['date'] = pd.to_datetime(train_data_paris['date'])\n",
    "valid_data_paris['date'] = pd.to_datetime(valid_data_paris['date'])\n",
    "test_data_paris['date'] = pd.to_datetime(test_data_paris['date'])\n",
    "\n",
    "# Get the first and last date for each dataset\n",
    "train_paris_first_date = train_data_paris['date'].min()\n",
    "train_paris_last_date = train_data_paris['date'].max()\n",
    "\n",
    "valid_paris_first_date = valid_data_paris['date'].min()\n",
    "valid_paris_last_date = valid_data_paris['date'].max()\n",
    "\n",
    "test_paris_first_date = test_data_paris['date'].min()\n",
    "test_paris_last_date = test_data_paris['date'].max()\n",
    "\n",
    "# Display the first and last date for each dataset\n",
    "print(f\"First date in the training dataset (Paris): {train_paris_first_date}\")\n",
    "print(f\"Last date in the training dataset (Paris): {train_paris_last_date}\")\n",
    "\n",
    "print(f\"First date in the validation dataset (Paris): {valid_paris_first_date}\")\n",
    "print(f\"Last date in the validation dataset (Paris): {valid_paris_last_date}\")\n",
    "\n",
    "print(f\"First date in the test dataset (Paris): {test_paris_first_date}\")\n",
    "print(f\"Last date in the test dataset (Paris): {test_paris_last_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6271d932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in Train Data:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Missing values in Validation Data:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Missing values in Test Data:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of datasets to check\n",
    "datasets = {\n",
    "    \"Train Data\": train_data_lstm_paris,\n",
    "    \"Validation Data\": valid_data_lstm_paris,\n",
    "    \"Test Data\": test_data_lstm_paris\n",
    "}\n",
    "\n",
    "# Loop through each dataset and print the number of missing values per column\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    print(f\"\\nMissing values in {dataset_name}:\")\n",
    "    missing_values_count = dataset.isnull().sum()\n",
    "    print(missing_values_count[missing_values_count > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64bda414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                 0.0\n",
      "date               0.0\n",
      "available          0.0\n",
      "price              0.0\n",
      "weekday            0.0\n",
      "                  ... \n",
      "lstm_feature_11    0.0\n",
      "lstm_feature_12    0.0\n",
      "lstm_feature_13    0.0\n",
      "lstm_feature_14    0.0\n",
      "lstm_feature_15    0.0\n",
      "Length: 154, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train_data_lstm_paris.isnull().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "478f4e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5734 entries, 0 to 5733\n",
      "Columns: 138 entries, id to kitchen_amenities\n",
      "dtypes: bool(98), datetime64[ns](1), float64(15), int64(24)\n",
      "memory usage: 2.3 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5734 entries, 0 to 5733\n",
      "Columns: 154 entries, id to lstm_feature_15\n",
      "dtypes: bool(98), datetime64[ns](1), float64(31), int64(24)\n",
      "memory usage: 3.0 MB\n"
     ]
    }
   ],
   "source": [
    "train_data_paris.info()\n",
    "train_data_lstm_paris.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "375694e7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                              20.574575\n",
       "price                                            4.581917\n",
       "weekday                                         -4.417781\n",
       "year                                           -37.831858\n",
       "month                                           37.831858\n",
       "day                                             33.427361\n",
       "host_id                                          1.706011\n",
       "host_response_time                               1.039477\n",
       "host_response_rate                              -0.789847\n",
       "host_listings_count                             15.492427\n",
       "host_total_listings_count                       14.028154\n",
       "latitude                                        -0.220196\n",
       "longitude                                       -0.347711\n",
       "accommodates                                     0.908152\n",
       "bathrooms                                        3.265576\n",
       "bedrooms                                         0.558549\n",
       "beds                                             1.396520\n",
       "availability_90                                  2.022363\n",
       "availability_365                                 1.851547\n",
       "number_of_reviews                                2.784092\n",
       "number_of_reviews_ltm                            2.489144\n",
       "number_of_reviews_l30d                           3.374365\n",
       "review_scores_rating                            -1.398431\n",
       "calculated_host_listings_count                   5.085388\n",
       "calculated_host_listings_count_entire_homes      5.958592\n",
       "calculated_host_listings_count_private_rooms     3.757901\n",
       "reviews_per_month                                1.729163\n",
       "total_amenities                                  0.796680\n",
       "Eiffel Tower Distance                           -0.088896\n",
       "Louvre Museum Distance                          -0.023910\n",
       "Notre-Dame Cathedral Distance                   -0.010286\n",
       "Sacré-Cœur Basilica Distance                     0.145946\n",
       "Arc de Triomphe Distance                         0.019721\n",
       "nearby_airbnbs_count                             0.824270\n",
       "luxury_amenities_score                           3.235612\n",
       "nearby_restaurants_bars                          0.827155\n",
       "nearby_transport                                 1.084795\n",
       "mean_price_neighbors                             3.198952\n",
       "kitchen_amenities                                0.082019\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating skewness for each numerical feature in the dataset\n",
    "skewness = train_data_paris.select_dtypes(include=['float64', 'int64']).skew()\n",
    "\n",
    "# Display the skewness values\n",
    "skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5498887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_data_paris.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f6404",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459c38c0",
   "metadata": {},
   "source": [
    "## Without LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95effab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Model: 100%|██████████ [time left: 00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  13.417643428805151\n",
      "Mean Squared Error (MSE):  352.0321077081457\n",
      "R-squared (R2):  0.9802534925801054\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  53.85018105625212\n",
      "Mean Squared Error (MSE):  11537.685359690988\n",
      "R-squared (R2):  0.4173203768711632\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  48.647106657947205\n",
      "Mean Squared Error (MSE):  9627.170389932062\n",
      "R-squared (R2):  0.4358358393166474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'month', 'day', 'year', 'weekday', 'date']\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the data excluding specific features\n",
    "def scale_features(X, excluded_columns):\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    return X_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train_raw, y_train = prepare_data(train_data_paris, 'price')\n",
    "X_valid_raw, y_valid = prepare_data(valid_data_paris, 'price')\n",
    "X_test_raw, y_test = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Apply scaling\n",
    "X_train, scaler = scale_features(X_train_raw, excluded_columns)\n",
    "X_valid, _ = scale_features(X_valid_raw, excluded_columns)\n",
    "X_test, _ = scale_features(X_test_raw, excluded_columns)\n",
    "\n",
    "# Define the model with default parameters\n",
    "xgb = XGBRegressor(objective='reg:squarederror', eval_metric='rmse')\n",
    "\n",
    "# Wrap the fitting process with tqdm for visualization\n",
    "with tqdm(total=1, desc=\"Fitting Model\", bar_format=\"{l_bar}{bar} [time left: {remaining}]\") as pbar:\n",
    "    xgb.fit(X_train, y_train)\n",
    "    pbar.update()  # Simulate completion\n",
    "\n",
    "# Predict on the training, validation, and test sets\n",
    "y_train_pred = xgb.predict(X_train)\n",
    "y_valid_pred = xgb.predict(X_valid)\n",
    "y_test_pred = xgb.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print_metrics(y_train, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea390cd",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "867f4afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 08:35:52,816] A new study created in memory with name: no-name-7925dc19-f5d4-40c3-b22c-434d9282d23f\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732c0710a4af4722a4fa6573fa02aa6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 08:35:58,931] Trial 0 finished with value: 99.7189280189116 and parameters: {'n_estimators': 871, 'max_depth': 8, 'learning_rate': 0.1775154110617777, 'subsample': 0.5044025224313493, 'colsample_bytree': 0.7211195385608825, 'gamma': 1.4613296000055271, 'reg_alpha': 5.721941939416535, 'reg_lambda': 5.400154389750174}. Best is trial 0 with value: 99.7189280189116.\n",
      "[I 2024-08-02 08:36:02,409] Trial 1 finished with value: 98.40511851179677 and parameters: {'n_estimators': 437, 'max_depth': 8, 'learning_rate': 0.025524804356012832, 'subsample': 0.8124305350974038, 'colsample_bytree': 0.8546942963492362, 'gamma': 4.423461025532498, 'reg_alpha': 7.3820752934917735, 'reg_lambda': 4.390398024821113}. Best is trial 1 with value: 98.40511851179677.\n",
      "[I 2024-08-02 08:36:06,276] Trial 2 finished with value: 100.97016245548346 and parameters: {'n_estimators': 892, 'max_depth': 5, 'learning_rate': 0.12527269860469822, 'subsample': 0.7847901767966347, 'colsample_bytree': 0.7542050155315931, 'gamma': 2.1373658050732964, 'reg_alpha': 8.895905449005626, 'reg_lambda': 2.196202761116001}. Best is trial 1 with value: 98.40511851179677.\n",
      "[I 2024-08-02 08:36:09,332] Trial 3 finished with value: 99.83288975622678 and parameters: {'n_estimators': 473, 'max_depth': 7, 'learning_rate': 0.1616673660494746, 'subsample': 0.5867694223002786, 'colsample_bytree': 0.8662225875398546, 'gamma': 4.635284761885357, 'reg_alpha': 6.231263061898628, 'reg_lambda': 8.874494074347332}. Best is trial 1 with value: 98.40511851179677.\n",
      "[I 2024-08-02 08:36:14,736] Trial 4 finished with value: 96.38260204708833 and parameters: {'n_estimators': 896, 'max_depth': 7, 'learning_rate': 0.048322765011557585, 'subsample': 0.8776779278117617, 'colsample_bytree': 0.5817442299607303, 'gamma': 2.668163372803779, 'reg_alpha': 6.435186017421771, 'reg_lambda': 9.51114244561537}. Best is trial 4 with value: 96.38260204708833.\n",
      "[I 2024-08-02 08:36:15,957] Trial 5 finished with value: 99.25266784814383 and parameters: {'n_estimators': 148, 'max_depth': 7, 'learning_rate': 0.08595661246345421, 'subsample': 0.8440142184076956, 'colsample_bytree': 0.7242540555315637, 'gamma': 0.40011379488468835, 'reg_alpha': 3.1815349754473976, 'reg_lambda': 4.321365702576924}. Best is trial 4 with value: 96.38260204708833.\n",
      "[I 2024-08-02 08:36:18,337] Trial 6 finished with value: 100.58649702644313 and parameters: {'n_estimators': 467, 'max_depth': 6, 'learning_rate': 0.20489562198786004, 'subsample': 0.8108562604022749, 'colsample_bytree': 0.5959225487938529, 'gamma': 2.0389681785122544, 'reg_alpha': 3.74262074846341, 'reg_lambda': 4.35386905015184}. Best is trial 4 with value: 96.38260204708833.\n",
      "[I 2024-08-02 08:36:19,685] Trial 7 finished with value: 97.47636086772948 and parameters: {'n_estimators': 236, 'max_depth': 6, 'learning_rate': 0.040919850376480944, 'subsample': 0.6317015322395435, 'colsample_bytree': 0.9011578389638677, 'gamma': 0.8720652189923506, 'reg_alpha': 0.5117292301696985, 'reg_lambda': 8.281841107216067}. Best is trial 4 with value: 96.38260204708833.\n",
      "[I 2024-08-02 08:36:22,120] Trial 8 finished with value: 100.68326533989597 and parameters: {'n_estimators': 481, 'max_depth': 6, 'learning_rate': 0.1290395877058407, 'subsample': 0.9332955171124175, 'colsample_bytree': 0.6994787983096168, 'gamma': 3.785190019698354, 'reg_alpha': 8.503592413238486, 'reg_lambda': 2.3232576430091534}. Best is trial 4 with value: 96.38260204708833.\n",
      "[I 2024-08-02 08:36:26,696] Trial 9 finished with value: 99.91735673420287 and parameters: {'n_estimators': 886, 'max_depth': 6, 'learning_rate': 0.05020592879440305, 'subsample': 0.6377190556521879, 'colsample_bytree': 0.963628951698348, 'gamma': 0.38444916385858063, 'reg_alpha': 8.077885391312968, 'reg_lambda': 3.865229912843502}. Best is trial 4 with value: 96.38260204708833.\n",
      "[I 2024-08-02 08:36:30,497] Trial 10 finished with value: 99.15874811861364 and parameters: {'n_estimators': 717, 'max_depth': 10, 'learning_rate': 0.24623531122417447, 'subsample': 0.9962176515288007, 'colsample_bytree': 0.504879618112614, 'gamma': 3.031201864836924, 'reg_alpha': 1.4347600513128822, 'reg_lambda': 6.611492611627388}. Best is trial 4 with value: 96.38260204708833.\n",
      "[I 2024-08-02 08:36:31,492] Trial 11 finished with value: 108.44157202418613 and parameters: {'n_estimators': 169, 'max_depth': 3, 'learning_rate': 0.010181691930919654, 'subsample': 0.6867717151032279, 'colsample_bytree': 0.9928644695127766, 'gamma': 1.2896915735000989, 'reg_alpha': 0.6184086166536509, 'reg_lambda': 9.783725399739353}. Best is trial 4 with value: 96.38260204708833.\n",
      "[I 2024-08-02 08:36:37,273] Trial 12 finished with value: 97.60995621355458 and parameters: {'n_estimators': 675, 'max_depth': 9, 'learning_rate': 0.07770091317841865, 'subsample': 0.7076701986112253, 'colsample_bytree': 0.5802961563277433, 'gamma': 2.5211837432847504, 'reg_alpha': 4.054604526702835, 'reg_lambda': 7.816202322974033}. Best is trial 4 with value: 96.38260204708833.\n",
      "[I 2024-08-02 08:36:38,717] Trial 13 finished with value: 102.77166727846193 and parameters: {'n_estimators': 292, 'max_depth': 4, 'learning_rate': 0.29700869454621304, 'subsample': 0.8921688745448645, 'colsample_bytree': 0.8665452663192723, 'gamma': 3.2224035813418586, 'reg_alpha': 2.2501903138766544, 'reg_lambda': 7.859929135849145}. Best is trial 4 with value: 96.38260204708833.\n",
      "[I 2024-08-02 08:36:42,057] Trial 14 finished with value: 97.01934042697945 and parameters: {'n_estimators': 672, 'max_depth': 5, 'learning_rate': 0.07754320948914027, 'subsample': 0.5857685869378092, 'colsample_bytree': 0.6286247343505094, 'gamma': 1.2057313264574807, 'reg_alpha': 6.4628446056040865, 'reg_lambda': 9.997594454487075}. Best is trial 4 with value: 96.38260204708833.\n",
      "[I 2024-08-02 08:36:45,100] Trial 15 finished with value: 100.4932452355269 and parameters: {'n_estimators': 723, 'max_depth': 4, 'learning_rate': 0.08680849697551185, 'subsample': 0.5174100193438713, 'colsample_bytree': 0.6005804521601983, 'gamma': 1.5429083697163384, 'reg_alpha': 6.901963762814546, 'reg_lambda': 9.842091139271306}. Best is trial 4 with value: 96.38260204708833.\n",
      "[I 2024-08-02 08:36:49,986] Trial 16 finished with value: 99.44875556900914 and parameters: {'n_estimators': 985, 'max_depth': 5, 'learning_rate': 0.1164154872369807, 'subsample': 0.7133422280394895, 'colsample_bytree': 0.6585089578645243, 'gamma': 2.8376660461981027, 'reg_alpha': 9.972576062437126, 'reg_lambda': 6.406488801985237}. Best is trial 4 with value: 96.38260204708833.\n",
      "[I 2024-08-02 08:36:54,702] Trial 17 finished with value: 96.54222078242496 and parameters: {'n_estimators': 620, 'max_depth': 8, 'learning_rate': 0.05416374060558672, 'subsample': 0.9136378488528498, 'colsample_bytree': 0.5031830288828403, 'gamma': 3.7552369926317484, 'reg_alpha': 5.3974402279915354, 'reg_lambda': 0.18010856071402692}. Best is trial 4 with value: 96.38260204708833.\n",
      "[I 2024-08-02 08:36:59,886] Trial 18 finished with value: 97.74624216465422 and parameters: {'n_estimators': 593, 'max_depth': 9, 'learning_rate': 0.053673274596049814, 'subsample': 0.9424832876658857, 'colsample_bytree': 0.5228099768619618, 'gamma': 3.9027804614343964, 'reg_alpha': 5.388039863194713, 'reg_lambda': 0.045054508570835594}. Best is trial 4 with value: 96.38260204708833.\n",
      "[I 2024-08-02 08:37:04,172] Trial 19 finished with value: 102.66780177337728 and parameters: {'n_estimators': 821, 'max_depth': 8, 'learning_rate': 0.2060467438551235, 'subsample': 0.8718634327133145, 'colsample_bytree': 0.5443670771609174, 'gamma': 3.80485105267928, 'reg_alpha': 4.803393512965968, 'reg_lambda': 0.5125523718208234}. Best is trial 4 with value: 96.38260204708833.\n",
      "[I 2024-08-02 08:37:07,493] Trial 20 finished with value: 99.29458821648365 and parameters: {'n_estimators': 346, 'max_depth': 10, 'learning_rate': 0.10882792834569088, 'subsample': 0.9679967848931346, 'colsample_bytree': 0.5474539494593502, 'gamma': 4.2616525831457315, 'reg_alpha': 4.727284899768799, 'reg_lambda': 2.014725606951334}. Best is trial 4 with value: 96.38260204708833.\n",
      "[I 2024-08-02 08:37:11,352] Trial 21 finished with value: 97.12677442609878 and parameters: {'n_estimators': 602, 'max_depth': 7, 'learning_rate': 0.0632889784066659, 'subsample': 0.9014381074881568, 'colsample_bytree': 0.6516109264904906, 'gamma': 3.3766878279725834, 'reg_alpha': 6.476498445288374, 'reg_lambda': 9.330073661981032}. Best is trial 4 with value: 96.38260204708833.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 08:37:14,526] Trial 22 finished with value: 95.9490121994292 and parameters: {'n_estimators': 675, 'max_depth': 5, 'learning_rate': 0.02958412429597648, 'subsample': 0.7446547313584032, 'colsample_bytree': 0.5968666127834945, 'gamma': 2.279995320349391, 'reg_alpha': 7.4993017592319156, 'reg_lambda': 6.428788340992353}. Best is trial 22 with value: 95.9490121994292.\n",
      "[I 2024-08-02 08:37:20,758] Trial 23 finished with value: 96.41739781828555 and parameters: {'n_estimators': 786, 'max_depth': 8, 'learning_rate': 0.021606386232275142, 'subsample': 0.7503011888595829, 'colsample_bytree': 0.5641883822229588, 'gamma': 2.1992526284267453, 'reg_alpha': 7.672727685403485, 'reg_lambda': 6.73036004661479}. Best is trial 22 with value: 95.9490121994292.\n",
      "[I 2024-08-02 08:37:28,009] Trial 24 finished with value: 98.19792474715426 and parameters: {'n_estimators': 792, 'max_depth': 9, 'learning_rate': 0.012654086501603469, 'subsample': 0.7453601795879227, 'colsample_bytree': 0.7779737731110707, 'gamma': 2.3255337599881116, 'reg_alpha': 7.599580726669445, 'reg_lambda': 6.703391510161586}. Best is trial 22 with value: 95.9490121994292.\n",
      "[I 2024-08-02 08:37:31,968] Trial 25 finished with value: 97.45977080144527 and parameters: {'n_estimators': 992, 'max_depth': 4, 'learning_rate': 0.033994757821996124, 'subsample': 0.7544191834902055, 'colsample_bytree': 0.5642191084837779, 'gamma': 1.7288666257160274, 'reg_alpha': 9.202130440266199, 'reg_lambda': 5.581533375571157}. Best is trial 22 with value: 95.9490121994292.\n",
      "[I 2024-08-02 08:37:37,418] Trial 26 finished with value: 96.31653974174392 and parameters: {'n_estimators': 789, 'max_depth': 7, 'learning_rate': 0.030909622283929038, 'subsample': 0.840124060433931, 'colsample_bytree': 0.6738223142909638, 'gamma': 2.7241614034331505, 'reg_alpha': 7.565094317446236, 'reg_lambda': 7.53788064384015}. Best is trial 22 with value: 95.9490121994292.\n",
      "[I 2024-08-02 08:37:42,974] Trial 27 finished with value: 98.63538151795477 and parameters: {'n_estimators': 917, 'max_depth': 7, 'learning_rate': 0.09838203871385243, 'subsample': 0.8514144435366575, 'colsample_bytree': 0.6799074378210586, 'gamma': 2.744038824375428, 'reg_alpha': 9.418853640073488, 'reg_lambda': 8.552943137278652}. Best is trial 22 with value: 95.9490121994292.\n",
      "[I 2024-08-02 08:37:46,406] Trial 28 finished with value: 98.94392015344292 and parameters: {'n_estimators': 745, 'max_depth': 5, 'learning_rate': 0.14138902455392777, 'subsample': 0.8171189795341539, 'colsample_bytree': 0.6212176179068384, 'gamma': 2.621294105240127, 'reg_alpha': 6.985536268398641, 'reg_lambda': 7.6178125981793645}. Best is trial 22 with value: 95.9490121994292.\n",
      "[I 2024-08-02 08:37:50,347] Trial 29 finished with value: 100.11807310689832 and parameters: {'n_estimators': 942, 'max_depth': 3, 'learning_rate': 0.06620390075467487, 'subsample': 0.7803247762288795, 'colsample_bytree': 0.7729944152462251, 'gamma': 3.381834612636134, 'reg_alpha': 8.379419161472546, 'reg_lambda': 5.640978361086044}. Best is trial 22 with value: 95.9490121994292.\n",
      "[I 2024-08-02 08:37:55,624] Trial 30 finished with value: 97.66392847005216 and parameters: {'n_estimators': 838, 'max_depth': 7, 'learning_rate': 0.03057625173091844, 'subsample': 0.8770959858310183, 'colsample_bytree': 0.7171923988879464, 'gamma': 1.8648141129108646, 'reg_alpha': 6.008573220322631, 'reg_lambda': 7.244636969484905}. Best is trial 22 with value: 95.9490121994292.\n",
      "[I 2024-08-02 08:38:01,380] Trial 31 finished with value: 96.67176966767353 and parameters: {'n_estimators': 779, 'max_depth': 8, 'learning_rate': 0.010343678451491006, 'subsample': 0.735288897304494, 'colsample_bytree': 0.6257202436769294, 'gamma': 2.3653246733084106, 'reg_alpha': 7.747242479530416, 'reg_lambda': 6.103352082015291}. Best is trial 22 with value: 95.9490121994292.\n",
      "[I 2024-08-02 08:38:08,313] Trial 32 finished with value: 96.72179738443012 and parameters: {'n_estimators': 845, 'max_depth': 8, 'learning_rate': 0.030724019445349127, 'subsample': 0.6702760825414542, 'colsample_bytree': 0.6683249562504787, 'gamma': 2.157794532997067, 'reg_alpha': 7.096694114369647, 'reg_lambda': 7.150885075981402}. Best is trial 22 with value: 95.9490121994292.\n",
      "[I 2024-08-02 08:38:11,864] Trial 33 finished with value: 96.30011991945187 and parameters: {'n_estimators': 661, 'max_depth': 6, 'learning_rate': 0.02566240153479367, 'subsample': 0.7809244912447384, 'colsample_bytree': 0.5630373051789465, 'gamma': 2.9677470068334046, 'reg_alpha': 7.679446311338094, 'reg_lambda': 8.796234065646475}. Best is trial 22 with value: 95.9490121994292.\n",
      "[I 2024-08-02 08:38:14,791] Trial 34 finished with value: 96.83205219788222 and parameters: {'n_estimators': 520, 'max_depth': 6, 'learning_rate': 0.042934769707762735, 'subsample': 0.7901190550402022, 'colsample_bytree': 0.602499304605814, 'gamma': 2.9275582140870915, 'reg_alpha': 8.431584417982313, 'reg_lambda': 9.0784247940905}. Best is trial 22 with value: 95.9490121994292.\n",
      "[I 2024-08-02 08:38:18,069] Trial 35 finished with value: 97.56753891132459 and parameters: {'n_estimators': 685, 'max_depth': 5, 'learning_rate': 0.06618481090546405, 'subsample': 0.8205683585155266, 'colsample_bytree': 0.6448642152190376, 'gamma': 3.207995413059932, 'reg_alpha': 5.862435892588886, 'reg_lambda': 8.396306498974237}. Best is trial 22 with value: 95.9490121994292.\n",
      "[I 2024-08-02 08:38:22,053] Trial 36 finished with value: 96.3412926832559 and parameters: {'n_estimators': 557, 'max_depth': 7, 'learning_rate': 0.0927519537609788, 'subsample': 0.8514203083280844, 'colsample_bytree': 0.530638984480748, 'gamma': 2.607371382978986, 'reg_alpha': 6.612247926387913, 'reg_lambda': 9.279095516847013}. Best is trial 22 with value: 95.9490121994292.\n",
      "[I 2024-08-02 08:38:24,617] Trial 37 finished with value: 99.10920745249193 and parameters: {'n_estimators': 403, 'max_depth': 6, 'learning_rate': 0.16598536011305604, 'subsample': 0.8502736967278944, 'colsample_bytree': 0.8047218990359333, 'gamma': 1.8868198515751935, 'reg_alpha': 8.973000634990719, 'reg_lambda': 5.042727695462313}. Best is trial 22 with value: 95.9490121994292.\n",
      "[I 2024-08-02 08:38:28,161] Trial 38 finished with value: 94.56988766520756 and parameters: {'n_estimators': 554, 'max_depth': 7, 'learning_rate': 0.09974639781407674, 'subsample': 0.7796929304636099, 'colsample_bytree': 0.5424828015933447, 'gamma': 2.5589267332878523, 'reg_alpha': 7.339475841645578, 'reg_lambda': 8.94308678288658}. Best is trial 38 with value: 94.56988766520756.\n",
      "[I 2024-08-02 08:38:31,106] Trial 39 finished with value: 99.12365890515731 and parameters: {'n_estimators': 634, 'max_depth': 5, 'learning_rate': 0.19024381006303612, 'subsample': 0.7878312339852618, 'colsample_bytree': 0.6917010013361875, 'gamma': 4.9926535434724055, 'reg_alpha': 7.346158614155388, 'reg_lambda': 8.738290016697482}. Best is trial 38 with value: 94.56988766520756.\n",
      "[I 2024-08-02 08:38:34,004] Trial 40 finished with value: 97.52568694412705 and parameters: {'n_estimators': 535, 'max_depth': 6, 'learning_rate': 0.1081143429655651, 'subsample': 0.7708147251117433, 'colsample_bytree': 0.56450355697371, 'gamma': 3.539542565812477, 'reg_alpha': 8.138806343391767, 'reg_lambda': 8.143432061554256}. Best is trial 38 with value: 94.56988766520756.\n",
      "[I 2024-08-02 08:38:38,002] Trial 41 finished with value: 94.35882204179781 and parameters: {'n_estimators': 559, 'max_depth': 7, 'learning_rate': 0.13674586599046043, 'subsample': 0.8264971878569441, 'colsample_bytree': 0.5344585845304358, 'gamma': 2.5026740440532462, 'reg_alpha': 6.62431601361672, 'reg_lambda': 9.126175968144148}. Best is trial 41 with value: 94.35882204179781.\n",
      "[I 2024-08-02 08:38:41,104] Trial 42 finished with value: 98.4873693209873 and parameters: {'n_estimators': 393, 'max_depth': 7, 'learning_rate': 0.1491844054173302, 'subsample': 0.8121165925322118, 'colsample_bytree': 0.5861973035601336, 'gamma': 3.0763025477090524, 'reg_alpha': 7.906212158674359, 'reg_lambda': 7.375484821479901}. Best is trial 41 with value: 94.35882204179781.\n",
      "[I 2024-08-02 08:38:44,269] Trial 43 finished with value: 96.24653441659511 and parameters: {'n_estimators': 564, 'max_depth': 6, 'learning_rate': 0.12402849021596529, 'subsample': 0.723493798804371, 'colsample_bytree': 0.5280341348173991, 'gamma': 2.439042571692337, 'reg_alpha': 8.738274599588692, 'reg_lambda': 8.823429233415062}. Best is trial 41 with value: 94.35882204179781.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 08:38:47,064] Trial 44 finished with value: 97.76145784532483 and parameters: {'n_estimators': 502, 'max_depth': 6, 'learning_rate': 0.13337544552058478, 'subsample': 0.7064771274044949, 'colsample_bytree': 0.5244813350133968, 'gamma': 0.0226765253107426, 'reg_alpha': 8.764162500629157, 'reg_lambda': 8.954872872862877}. Best is trial 41 with value: 94.35882204179781.\n",
      "[I 2024-08-02 08:38:49,595] Trial 45 finished with value: 100.43298548680676 and parameters: {'n_estimators': 447, 'max_depth': 6, 'learning_rate': 0.1701590947716889, 'subsample': 0.6656549152809808, 'colsample_bytree': 0.5026272324051252, 'gamma': 2.0594809726903702, 'reg_alpha': 9.738981924058724, 'reg_lambda': 3.8755838038703785}. Best is trial 41 with value: 94.35882204179781.\n",
      "[I 2024-08-02 08:38:52,466] Trial 46 finished with value: 98.79521561588547 and parameters: {'n_estimators': 581, 'max_depth': 5, 'learning_rate': 0.15270305531482786, 'subsample': 0.726879378529009, 'colsample_bytree': 0.5438583779803665, 'gamma': 2.3467868994947545, 'reg_alpha': 8.676622778933611, 'reg_lambda': 9.39937267249769}. Best is trial 41 with value: 94.35882204179781.\n",
      "[I 2024-08-02 08:38:55,525] Trial 47 finished with value: 100.93471829703584 and parameters: {'n_estimators': 650, 'max_depth': 4, 'learning_rate': 0.12399974266203492, 'subsample': 0.6839705837920114, 'colsample_bytree': 0.5694424487611831, 'gamma': 1.5106052985144873, 'reg_alpha': 7.084602663644491, 'reg_lambda': 8.26922700370396}. Best is trial 41 with value: 94.35882204179781.\n",
      "[I 2024-08-02 08:38:59,127] Trial 48 finished with value: 98.69198219787148 and parameters: {'n_estimators': 553, 'max_depth': 7, 'learning_rate': 0.1946979616068938, 'subsample': 0.7683435859886665, 'colsample_bytree': 0.6040227096601559, 'gamma': 2.479102765017136, 'reg_alpha': 8.106576947827666, 'reg_lambda': 8.738601555732444}. Best is trial 41 with value: 94.35882204179781.\n",
      "[I 2024-08-02 08:39:01,837] Trial 49 finished with value: 97.49935780077287 and parameters: {'n_estimators': 502, 'max_depth': 6, 'learning_rate': 0.13615013490793512, 'subsample': 0.7988298077188826, 'colsample_bytree': 0.5456513660060847, 'gamma': 1.2244859908104462, 'reg_alpha': 9.435236790919816, 'reg_lambda': 9.553949063139699}. Best is trial 41 with value: 94.35882204179781.\n",
      "Best hyperparameters:  {'n_estimators': 559, 'max_depth': 7, 'learning_rate': 0.13674586599046043, 'subsample': 0.8264971878569441, 'colsample_bytree': 0.5344585845304358, 'gamma': 2.5026740440532462, 'reg_alpha': 6.62431601361672, 'reg_lambda': 9.126175968144148}\n",
      "Performance with tuned hyperparameters:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  3.1271561744697927\n",
      "Mean Squared Error (MSE):  27.29252628049549\n",
      "R-squared (R2):  0.9984690826180199\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  46.74772423701828\n",
      "Mean Squared Error (MSE):  8903.587297115668\n",
      "R-squared (R2):  0.550348381929094\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  46.14017136249663\n",
      "Mean Squared Error (MSE):  8421.926314255277\n",
      "R-squared (R2):  0.5064646414290378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'month', 'day', 'year', 'weekday', 'date']\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the data excluding specific features\n",
    "def scale_features(X, excluded_columns):\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    return X_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train_raw, y_train = prepare_data(train_data_paris, 'price')\n",
    "X_valid_raw, y_valid = prepare_data(valid_data_paris, 'price')\n",
    "X_test_raw, y_test = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Apply scaling\n",
    "X_train, scaler = scale_features(X_train_raw, excluded_columns)\n",
    "X_valid, _ = scale_features(X_valid_raw, excluded_columns)\n",
    "X_test, _ = scale_features(X_test_raw, excluded_columns)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10)\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        eval_metric='rmse',\n",
    "        **param\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "    preds = model.predict(X_valid)\n",
    "    mse = mean_squared_error(y_valid, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    eval_metric='rmse',\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_best = best_model.predict(X_train)\n",
    "y_valid_pred_best = best_model.predict(X_valid)\n",
    "y_test_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with tuned hyperparameters:\")\n",
    "print_metrics(y_train, y_train_pred_best, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred_best, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred_best, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad0b6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'xgb_model_without_lstm_paris.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('xgb_model_without_lstm_paris.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8ec9e2",
   "metadata": {},
   "source": [
    "## With LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b07988d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Model: 100%|██████████ [time left: 00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  12.751246042354705\n",
      "Mean Squared Error (MSE):  320.0072665197678\n",
      "R-squared (R2):  0.9820498593043347\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  51.67993841760089\n",
      "Mean Squared Error (MSE):  11072.456506496326\n",
      "R-squared (R2):  0.440815503007571\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  48.05060716729184\n",
      "Mean Squared Error (MSE):  8962.187259438984\n",
      "R-squared (R2):  0.47480467797723147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'month', 'day', 'year', 'weekday', 'date']\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the data excluding specific features\n",
    "def scale_features(X, excluded_columns):\n",
    "    # Determine the columns that are numerical and not in excluded columns\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    \n",
    "    # Scale all numerical columns\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    \n",
    "    return X_scaled, scaler\n",
    "\n",
    "# Example datasets (replace with actual data)\n",
    "# train_data_lstm_paris, valid_data_lstm_paris, test_data_lstm_paris are assumed to be loaded DataFrames\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train_raw, y_train = prepare_data(train_data_lstm_paris, 'price')\n",
    "X_valid_raw, y_valid = prepare_data(valid_data_lstm_paris, 'price')\n",
    "X_test_raw, y_test = prepare_data(test_data_lstm_paris, 'price')\n",
    "\n",
    "# Apply scaling\n",
    "X_train_scaled, scaler = scale_features(X_train_raw, excluded_columns)\n",
    "X_valid_scaled, _ = scale_features(X_valid_raw, excluded_columns)\n",
    "X_test_scaled, _ = scale_features(X_test_raw, excluded_columns)\n",
    "\n",
    "# Define the model with default parameters\n",
    "xgb = XGBRegressor(objective='reg:squarederror', eval_metric='rmse')\n",
    "\n",
    "# Wrap the fitting process with tqdm for visualization\n",
    "with tqdm(total=1, desc=\"Fitting Model\", bar_format=\"{l_bar}{bar} [time left: {remaining}]\") as pbar:\n",
    "    xgb.fit(X_train_scaled, y_train)\n",
    "    pbar.update()  # Simulate completion\n",
    "\n",
    "# Predict on the training, validation, and test sets\n",
    "y_train_pred = xgb.predict(X_train_scaled)\n",
    "y_valid_pred = xgb.predict(X_valid_scaled)\n",
    "y_test_pred = xgb.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print_metrics(y_train, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eb980a",
   "metadata": {},
   "source": [
    "#### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bca3cd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 08:39:05,711] A new study created in memory with name: no-name-abb5546a-0606-4ae3-ba97-5ca007898589\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e4a2e5c63a4277a3b965453ebf35f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 08:39:08,644] Trial 0 finished with value: 97.57684261056518 and parameters: {'n_estimators': 624, 'max_depth': 4, 'learning_rate': 0.05453880744368767, 'subsample': 0.9293605867251573, 'colsample_bytree': 0.7004051261202859, 'gamma': 0.02601420012062039, 'reg_alpha': 8.556079544001246, 'reg_lambda': 7.000034239109044}. Best is trial 0 with value: 97.57684261056518.\n",
      "[I 2024-08-02 08:39:14,706] Trial 1 finished with value: 99.623348364953 and parameters: {'n_estimators': 429, 'max_depth': 10, 'learning_rate': 0.13927171216143386, 'subsample': 0.6054705913737048, 'colsample_bytree': 0.6124874099518458, 'gamma': 0.6137475048159097, 'reg_alpha': 8.158355353754809, 'reg_lambda': 4.642023452605577}. Best is trial 0 with value: 97.57684261056518.\n",
      "[I 2024-08-02 08:39:18,279] Trial 2 finished with value: 97.56752958335777 and parameters: {'n_estimators': 293, 'max_depth': 9, 'learning_rate': 0.03881348028103489, 'subsample': 0.6703158241618912, 'colsample_bytree': 0.8857258565881581, 'gamma': 3.0926499866433916, 'reg_alpha': 3.2444702161639416, 'reg_lambda': 9.531586354802332}. Best is trial 2 with value: 97.56752958335777.\n",
      "[I 2024-08-02 08:39:22,995] Trial 3 finished with value: 102.02553467085964 and parameters: {'n_estimators': 864, 'max_depth': 5, 'learning_rate': 0.10596183169993506, 'subsample': 0.5385993926438388, 'colsample_bytree': 0.97024800312791, 'gamma': 3.068240483633014, 'reg_alpha': 9.888782576262196, 'reg_lambda': 2.372407117365901}. Best is trial 2 with value: 97.56752958335777.\n",
      "[I 2024-08-02 08:39:24,964] Trial 4 finished with value: 99.04175207846414 and parameters: {'n_estimators': 391, 'max_depth': 3, 'learning_rate': 0.03087295566899257, 'subsample': 0.6493356086105062, 'colsample_bytree': 0.9847263886735438, 'gamma': 2.6156930985205475, 'reg_alpha': 7.435203986326223, 'reg_lambda': 7.316339932890275}. Best is trial 2 with value: 97.56752958335777.\n",
      "[I 2024-08-02 08:39:29,052] Trial 5 finished with value: 106.8210327262107 and parameters: {'n_estimators': 753, 'max_depth': 4, 'learning_rate': 0.14232886819922175, 'subsample': 0.63365564101472, 'colsample_bytree': 0.732394226508007, 'gamma': 0.009642933334117476, 'reg_alpha': 1.3078889715863584, 'reg_lambda': 1.8929442990220036}. Best is trial 2 with value: 97.56752958335777.\n",
      "[I 2024-08-02 08:39:32,265] Trial 6 finished with value: 97.98714073447697 and parameters: {'n_estimators': 614, 'max_depth': 4, 'learning_rate': 0.08688442506593831, 'subsample': 0.6572902419874755, 'colsample_bytree': 0.5727738623629586, 'gamma': 3.6355075244634887, 'reg_alpha': 4.536606223077602, 'reg_lambda': 8.531901479148688}. Best is trial 2 with value: 97.56752958335777.\n",
      "[I 2024-08-02 08:39:36,706] Trial 7 finished with value: 98.46421165554982 and parameters: {'n_estimators': 528, 'max_depth': 9, 'learning_rate': 0.26372416103038215, 'subsample': 0.6514948107053613, 'colsample_bytree': 0.5584943821882622, 'gamma': 1.1358445320009092, 'reg_alpha': 1.860979385853928, 'reg_lambda': 5.997931722526484}. Best is trial 2 with value: 97.56752958335777.\n",
      "[I 2024-08-02 08:39:41,019] Trial 8 finished with value: 106.62318930987921 and parameters: {'n_estimators': 762, 'max_depth': 5, 'learning_rate': 0.24303081340258284, 'subsample': 0.8081665458271985, 'colsample_bytree': 0.6058443376554212, 'gamma': 0.8684358306875262, 'reg_alpha': 7.290720916558872, 'reg_lambda': 0.8903997655709683}. Best is trial 2 with value: 97.56752958335777.\n",
      "[I 2024-08-02 08:39:44,956] Trial 9 finished with value: 102.25616418942406 and parameters: {'n_estimators': 866, 'max_depth': 3, 'learning_rate': 0.06421133976388821, 'subsample': 0.7713767951063761, 'colsample_bytree': 0.6633168089149732, 'gamma': 0.5149551187124318, 'reg_alpha': 4.043764520131264, 'reg_lambda': 3.671486706028343}. Best is trial 2 with value: 97.56752958335777.\n",
      "[I 2024-08-02 08:39:46,208] Trial 10 finished with value: 100.27134128746205 and parameters: {'n_estimators': 100, 'max_depth': 8, 'learning_rate': 0.2081892783453877, 'subsample': 0.8906861065274969, 'colsample_bytree': 0.8538708783228914, 'gamma': 4.887923448782193, 'reg_alpha': 2.5693098997599044, 'reg_lambda': 9.601104891815252}. Best is trial 2 with value: 97.56752958335777.\n",
      "[I 2024-08-02 08:39:48,086] Trial 11 finished with value: 98.28513107011699 and parameters: {'n_estimators': 187, 'max_depth': 7, 'learning_rate': 0.021048032418006124, 'subsample': 0.9766216311475706, 'colsample_bytree': 0.8033623443865003, 'gamma': 1.8201099995784986, 'reg_alpha': 0.018274372799689687, 'reg_lambda': 7.388128253750536}. Best is trial 2 with value: 97.56752958335777.\n",
      "[I 2024-08-02 08:39:50,621] Trial 12 finished with value: 97.56995514650407 and parameters: {'n_estimators': 362, 'max_depth': 6, 'learning_rate': 0.05756050256655413, 'subsample': 0.9785504367820317, 'colsample_bytree': 0.8629225370119985, 'gamma': 3.8859655591756974, 'reg_alpha': 5.770732601716788, 'reg_lambda': 9.850908473019222}. Best is trial 2 with value: 97.56752958335777.\n",
      "[I 2024-08-02 08:39:53,399] Trial 13 finished with value: 100.22941333905628 and parameters: {'n_estimators': 299, 'max_depth': 7, 'learning_rate': 0.19728289415200978, 'subsample': 0.8475152170820633, 'colsample_bytree': 0.8854292824596366, 'gamma': 4.090802242104771, 'reg_alpha': 5.714650995105556, 'reg_lambda': 9.69522038635203}. Best is trial 2 with value: 97.56752958335777.\n",
      "[I 2024-08-02 08:39:55,436] Trial 14 finished with value: 100.00691863198452 and parameters: {'n_estimators': 274, 'max_depth': 6, 'learning_rate': 0.011177973545790969, 'subsample': 0.7239455331223923, 'colsample_bytree': 0.9033583333246482, 'gamma': 4.164856711371126, 'reg_alpha': 6.061800072569534, 'reg_lambda': 8.588632940824247}. Best is trial 2 with value: 97.56752958335777.\n",
      "[I 2024-08-02 08:40:02,007] Trial 15 finished with value: 99.72451653020597 and parameters: {'n_estimators': 416, 'max_depth': 10, 'learning_rate': 0.10419659395283715, 'subsample': 0.7237654127695523, 'colsample_bytree': 0.7983787716592501, 'gamma': 3.147264903153812, 'reg_alpha': 2.964135814221753, 'reg_lambda': 9.781514467247742}. Best is trial 2 with value: 97.56752958335777.\n",
      "[I 2024-08-02 08:40:05,081] Trial 16 finished with value: 97.19147964111188 and parameters: {'n_estimators': 291, 'max_depth': 8, 'learning_rate': 0.06122566167542927, 'subsample': 0.5045153982569346, 'colsample_bytree': 0.9304925283604331, 'gamma': 2.11292578474439, 'reg_alpha': 3.707584426130121, 'reg_lambda': 8.386077221956645}. Best is trial 16 with value: 97.19147964111188.\n",
      "[I 2024-08-02 08:40:06,789] Trial 17 finished with value: 98.76572709987936 and parameters: {'n_estimators': 157, 'max_depth': 8, 'learning_rate': 0.08662387685585954, 'subsample': 0.504439881008224, 'colsample_bytree': 0.9406929134577255, 'gamma': 1.8807598766689662, 'reg_alpha': 3.6191241859857533, 'reg_lambda': 5.645267974660353}. Best is trial 16 with value: 97.19147964111188.\n",
      "[I 2024-08-02 08:40:09,954] Trial 18 finished with value: 106.02348616313758 and parameters: {'n_estimators': 253, 'max_depth': 9, 'learning_rate': 0.29886105989310363, 'subsample': 0.5793095208044379, 'colsample_bytree': 0.8106282203588293, 'gamma': 1.9788314546654289, 'reg_alpha': 0.6135665222470426, 'reg_lambda': 8.271856592859676}. Best is trial 16 with value: 97.19147964111188.\n",
      "[I 2024-08-02 08:40:15,466] Trial 19 finished with value: 100.79308554481501 and parameters: {'n_estimators': 493, 'max_depth': 8, 'learning_rate': 0.17349365788137233, 'subsample': 0.5612747481703521, 'colsample_bytree': 0.9150990569002838, 'gamma': 2.3937242580707405, 'reg_alpha': 3.0985508099204493, 'reg_lambda': 6.548233453164579}. Best is trial 16 with value: 97.19147964111188.\n",
      "[I 2024-08-02 08:40:18,566] Trial 20 finished with value: 99.34044105713531 and parameters: {'n_estimators': 219, 'max_depth': 9, 'learning_rate': 0.04383911417582708, 'subsample': 0.6984687572461348, 'colsample_bytree': 0.9965670963392395, 'gamma': 1.4576577874579626, 'reg_alpha': 5.001822399494223, 'reg_lambda': 4.81865387208474}. Best is trial 16 with value: 97.19147964111188.\n",
      "[I 2024-08-02 08:40:21,009] Trial 21 finished with value: 97.87392806429102 and parameters: {'n_estimators': 346, 'max_depth': 6, 'learning_rate': 0.06449306976808737, 'subsample': 0.9977239621443129, 'colsample_bytree': 0.8529591979099317, 'gamma': 3.3731746586372777, 'reg_alpha': 5.526179767392424, 'reg_lambda': 8.89694621494968}. Best is trial 16 with value: 97.19147964111188.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 08:40:23,876] Trial 22 finished with value: 98.65177693351315 and parameters: {'n_estimators': 343, 'max_depth': 7, 'learning_rate': 0.11824603803670582, 'subsample': 0.8439024119057795, 'colsample_bytree': 0.8546596010590676, 'gamma': 2.759282374611295, 'reg_alpha': 6.850522192219302, 'reg_lambda': 8.12884232721814}. Best is trial 16 with value: 97.19147964111188.\n",
      "[I 2024-08-02 08:40:28,243] Trial 23 finished with value: 98.12268889562387 and parameters: {'n_estimators': 458, 'max_depth': 8, 'learning_rate': 0.0702884094221057, 'subsample': 0.5037065212710001, 'colsample_bytree': 0.9511509465375685, 'gamma': 3.994422300875568, 'reg_alpha': 4.330722664241777, 'reg_lambda': 9.917269772228886}. Best is trial 16 with value: 97.19147964111188.\n",
      "[I 2024-08-02 08:40:32,585] Trial 24 finished with value: 97.71616628068553 and parameters: {'n_estimators': 339, 'max_depth': 9, 'learning_rate': 0.040997967117432504, 'subsample': 0.7633032179246375, 'colsample_bytree': 0.7683526682482491, 'gamma': 4.516855584480671, 'reg_alpha': 2.0152770758321275, 'reg_lambda': 9.165568041884844}. Best is trial 16 with value: 97.19147964111188.\n",
      "[I 2024-08-02 08:40:36,535] Trial 25 finished with value: 97.44375414882315 and parameters: {'n_estimators': 597, 'max_depth': 6, 'learning_rate': 0.08229976419260678, 'subsample': 0.9318011651155198, 'colsample_bytree': 0.8863097420284327, 'gamma': 2.3689669729391603, 'reg_alpha': 3.544403239502058, 'reg_lambda': 7.506055453580658}. Best is trial 16 with value: 97.19147964111188.\n",
      "[I 2024-08-02 08:40:42,657] Trial 26 finished with value: 99.44287706830514 and parameters: {'n_estimators': 591, 'max_depth': 10, 'learning_rate': 0.12525920005237995, 'subsample': 0.9086296529362269, 'colsample_bytree': 0.9330925950352135, 'gamma': 2.2629517038690925, 'reg_alpha': 3.506047878205151, 'reg_lambda': 7.3535007157037064}. Best is trial 16 with value: 97.19147964111188.\n",
      "[I 2024-08-02 08:40:48,862] Trial 27 finished with value: 98.1508176117278 and parameters: {'n_estimators': 713, 'max_depth': 7, 'learning_rate': 0.08644344737464607, 'subsample': 0.8030252907331205, 'colsample_bytree': 0.8941436753027865, 'gamma': 2.853418428620499, 'reg_alpha': 2.1871822444588567, 'reg_lambda': 7.691740482372568}. Best is trial 16 with value: 97.19147964111188.\n",
      "[I 2024-08-02 08:40:53,781] Trial 28 finished with value: 97.29551287205433 and parameters: {'n_estimators': 955, 'max_depth': 5, 'learning_rate': 0.012865786835208618, 'subsample': 0.703438861812749, 'colsample_bytree': 0.8272359601297591, 'gamma': 1.3493208177885054, 'reg_alpha': 1.1997105910167623, 'reg_lambda': 6.5004287795606555}. Best is trial 16 with value: 97.19147964111188.\n",
      "[I 2024-08-02 08:40:58,806] Trial 29 finished with value: 96.35863562344639 and parameters: {'n_estimators': 999, 'max_depth': 5, 'learning_rate': 0.019347930065980526, 'subsample': 0.867928673179658, 'colsample_bytree': 0.7013245410139795, 'gamma': 1.4544725896776396, 'reg_alpha': 0.8152950944585955, 'reg_lambda': 6.580961761067806}. Best is trial 29 with value: 96.35863562344639.\n",
      "[I 2024-08-02 08:41:04,365] Trial 30 finished with value: 96.55471391742238 and parameters: {'n_estimators': 993, 'max_depth': 5, 'learning_rate': 0.01052820004133849, 'subsample': 0.8650810509754981, 'colsample_bytree': 0.7014552661799043, 'gamma': 1.4069893059119185, 'reg_alpha': 1.0520384969391954, 'reg_lambda': 6.393984709420863}. Best is trial 29 with value: 96.35863562344639.\n",
      "[I 2024-08-02 08:41:09,344] Trial 31 finished with value: 97.05852210333674 and parameters: {'n_estimators': 983, 'max_depth': 5, 'learning_rate': 0.011595140110995113, 'subsample': 0.8410260475983218, 'colsample_bytree': 0.6873699439722112, 'gamma': 1.4904417726989636, 'reg_alpha': 1.1415157073398077, 'reg_lambda': 6.059144455666863}. Best is trial 29 with value: 96.35863562344639.\n",
      "[I 2024-08-02 08:41:14,393] Trial 32 finished with value: 96.33216074309128 and parameters: {'n_estimators': 972, 'max_depth': 5, 'learning_rate': 0.023608190301457, 'subsample': 0.8573303147307877, 'colsample_bytree': 0.6757835768581306, 'gamma': 1.5456719401575278, 'reg_alpha': 0.33391483318605863, 'reg_lambda': 4.183273090163402}. Best is trial 32 with value: 96.33216074309128.\n",
      "[I 2024-08-02 08:41:20,145] Trial 33 finished with value: 97.5732093799532 and parameters: {'n_estimators': 1000, 'max_depth': 5, 'learning_rate': 0.02907969128463554, 'subsample': 0.8607687668901045, 'colsample_bytree': 0.6810417627058921, 'gamma': 1.5481342894774393, 'reg_alpha': 0.4168904996755519, 'reg_lambda': 4.037427970199984}. Best is trial 32 with value: 96.33216074309128.\n",
      "[I 2024-08-02 08:41:24,146] Trial 34 finished with value: 97.3788424966887 and parameters: {'n_estimators': 905, 'max_depth': 4, 'learning_rate': 0.041255503285383785, 'subsample': 0.8086618868887543, 'colsample_bytree': 0.5032733871522947, 'gamma': 0.5862918306014495, 'reg_alpha': 0.9870782232184332, 'reg_lambda': 5.31244529863404}. Best is trial 32 with value: 96.33216074309128.\n",
      "[I 2024-08-02 08:41:29,230] Trial 35 finished with value: 96.20178044766747 and parameters: {'n_estimators': 995, 'max_depth': 5, 'learning_rate': 0.026721442284962323, 'subsample': 0.8799842241494112, 'colsample_bytree': 0.7195619902964914, 'gamma': 1.0755472955387533, 'reg_alpha': 1.448544152185509, 'reg_lambda': 4.325468503445595}. Best is trial 35 with value: 96.20178044766747.\n",
      "[I 2024-08-02 08:41:33,616] Trial 36 finished with value: 98.178749782834 and parameters: {'n_estimators': 924, 'max_depth': 4, 'learning_rate': 0.029248218491492513, 'subsample': 0.9313102057275854, 'colsample_bytree': 0.7266533795777295, 'gamma': 0.9503692432861296, 'reg_alpha': 1.7170832133877048, 'reg_lambda': 4.084718987966095}. Best is trial 35 with value: 96.20178044766747.\n",
      "[I 2024-08-02 08:41:37,385] Trial 37 finished with value: 105.51021544716042 and parameters: {'n_estimators': 822, 'max_depth': 3, 'learning_rate': 0.050231121342388066, 'subsample': 0.8840822989330117, 'colsample_bytree': 0.647571361033326, 'gamma': 0.32727879090848155, 'reg_alpha': 0.0944291048933481, 'reg_lambda': 2.5997748880533478}. Best is trial 35 with value: 96.20178044766747.\n",
      "[I 2024-08-02 08:41:41,232] Trial 38 finished with value: 98.46862067916021 and parameters: {'n_estimators': 844, 'max_depth': 4, 'learning_rate': 0.028270772348461953, 'subsample': 0.9520922363223538, 'colsample_bytree': 0.7243958175588672, 'gamma': 1.171897047942299, 'reg_alpha': 0.8247257211673381, 'reg_lambda': 4.464574391619074}. Best is trial 35 with value: 96.20178044766747.\n",
      "[I 2024-08-02 08:41:46,134] Trial 39 finished with value: 98.50414362991395 and parameters: {'n_estimators': 919, 'max_depth': 5, 'learning_rate': 0.03946982527631802, 'subsample': 0.8903043369666059, 'colsample_bytree': 0.7638662901907205, 'gamma': 0.8612129370836431, 'reg_alpha': 1.588852679963327, 'reg_lambda': 3.0239346451077194}. Best is trial 35 with value: 96.20178044766747.\n",
      "[I 2024-08-02 08:41:50,168] Trial 40 finished with value: 98.39508738245685 and parameters: {'n_estimators': 779, 'max_depth': 4, 'learning_rate': 0.024828191906814213, 'subsample': 0.7837410425584312, 'colsample_bytree': 0.637794817693802, 'gamma': 0.3298268050142805, 'reg_alpha': 9.471036127301453, 'reg_lambda': 3.2744990201422666}. Best is trial 35 with value: 96.20178044766747.\n",
      "[I 2024-08-02 08:41:55,458] Trial 41 finished with value: 95.882096227524 and parameters: {'n_estimators': 999, 'max_depth': 5, 'learning_rate': 0.023430610014271117, 'subsample': 0.8344615075550971, 'colsample_bytree': 0.7001417085825367, 'gamma': 1.6618699730457644, 'reg_alpha': 2.637205651237542, 'reg_lambda': 6.303598539161277}. Best is trial 41 with value: 95.882096227524.\n",
      "[I 2024-08-02 08:42:00,373] Trial 42 finished with value: 96.80363775381501 and parameters: {'n_estimators': 950, 'max_depth': 5, 'learning_rate': 0.010016437679546195, 'subsample': 0.8238946497360299, 'colsample_bytree': 0.7084715770237855, 'gamma': 1.6966696420719938, 'reg_alpha': 2.540161909079486, 'reg_lambda': 6.587603218057792}. Best is trial 41 with value: 95.882096227524.\n",
      "[I 2024-08-02 08:42:05,864] Trial 43 finished with value: 96.66564284151686 and parameters: {'n_estimators': 885, 'max_depth': 6, 'learning_rate': 0.051584042793793555, 'subsample': 0.8732924609461057, 'colsample_bytree': 0.6167328801057873, 'gamma': 1.1805166312568147, 'reg_alpha': 0.4417461739952734, 'reg_lambda': 5.131594484395738}. Best is trial 41 with value: 95.882096227524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 08:42:09,828] Trial 44 finished with value: 97.15937050538761 and parameters: {'n_estimators': 689, 'max_depth': 5, 'learning_rate': 0.07468126933916869, 'subsample': 0.9090331825534271, 'colsample_bytree': 0.7542193296039648, 'gamma': 0.9139725331259587, 'reg_alpha': 1.481332831371931, 'reg_lambda': 5.817461317924979}. Best is trial 41 with value: 95.882096227524.\n",
      "[I 2024-08-02 08:42:13,884] Trial 45 finished with value: 102.15476152636401 and parameters: {'n_estimators': 1000, 'max_depth': 3, 'learning_rate': 0.025579241887146636, 'subsample': 0.8260389162381773, 'colsample_bytree': 0.6707057636261622, 'gamma': 1.7094218308557039, 'reg_alpha': 2.3526180963270997, 'reg_lambda': 1.76563851853795}. Best is trial 41 with value: 95.882096227524.\n",
      "[I 2024-08-02 08:42:18,990] Trial 46 finished with value: 96.73429880722763 and parameters: {'n_estimators': 948, 'max_depth': 5, 'learning_rate': 0.05166900061638702, 'subsample': 0.8660625485867441, 'colsample_bytree': 0.7035146279423976, 'gamma': 0.7725597388444985, 'reg_alpha': 0.06071068814984182, 'reg_lambda': 6.948774940007281}. Best is trial 41 with value: 95.882096227524.\n",
      "[I 2024-08-02 08:42:24,933] Trial 47 finished with value: 99.31930541647255 and parameters: {'n_estimators': 852, 'max_depth': 6, 'learning_rate': 0.14748116384522325, 'subsample': 0.9121971092473321, 'colsample_bytree': 0.5918519402479265, 'gamma': 1.3363874542075633, 'reg_alpha': 2.8224446554586913, 'reg_lambda': 4.567604173395118}. Best is trial 41 with value: 95.882096227524.\n",
      "[I 2024-08-02 08:42:29,031] Trial 48 finished with value: 99.89155775498992 and parameters: {'n_estimators': 797, 'max_depth': 4, 'learning_rate': 0.09697018690945179, 'subsample': 0.7913794682562452, 'colsample_bytree': 0.647806822991622, 'gamma': 2.0124920208108126, 'reg_alpha': 0.8133435979830786, 'reg_lambda': 6.25196090691777}. Best is trial 41 with value: 95.882096227524.\n",
      "[I 2024-08-02 08:42:34,425] Trial 49 finished with value: 96.88108862940476 and parameters: {'n_estimators': 968, 'max_depth': 5, 'learning_rate': 0.019608696688300835, 'subsample': 0.947622373478302, 'colsample_bytree': 0.7785537908018119, 'gamma': 1.1365914403355633, 'reg_alpha': 1.9220756621534867, 'reg_lambda': 5.241374814722952}. Best is trial 41 with value: 95.882096227524.\n",
      "Best hyperparameters:  {'n_estimators': 999, 'max_depth': 5, 'learning_rate': 0.023430610014271117, 'subsample': 0.8344615075550971, 'colsample_bytree': 0.7001417085825367, 'gamma': 1.6618699730457644, 'reg_alpha': 2.637205651237542, 'reg_lambda': 6.303598539161277}\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  23.390916560891203\n",
      "Mean Squared Error (MSE):  1535.3604418220275\n",
      "R-squared (R2):  0.9138771558252673\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  45.95052112462682\n",
      "Mean Squared Error (MSE):  9193.376376984172\n",
      "R-squared (R2):  0.5357133674889745\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  43.772743965804885\n",
      "Mean Squared Error (MSE):  8105.707489831314\n",
      "R-squared (R2):  0.5249954579044571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'month', 'day', 'year', 'weekday', 'date']\n",
    "\n",
    "# Prepare the data by separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the data excluding specific features\n",
    "def scale_features(X, excluded_columns):\n",
    "    # Determine numerical features not in excluded columns\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    \n",
    "    # Scale all numerical columns\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    \n",
    "    return X_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets for LSTM data\n",
    "X_train_raw, y_train = prepare_data(train_data_lstm_paris, 'price')\n",
    "X_valid_raw, y_valid = prepare_data(valid_data_lstm_paris, 'price')\n",
    "X_test_raw, y_test = prepare_data(test_data_lstm_paris, 'price')\n",
    "\n",
    "# Apply scaling\n",
    "X_train_scaled, scaler = scale_features(X_train_raw, excluded_columns)\n",
    "X_valid_scaled, _ = scale_features(X_valid_raw, excluded_columns)\n",
    "X_test_scaled, _ = scale_features(X_test_raw, excluded_columns)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10)\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        eval_metric='rmse',  # Use rmse for evaluation\n",
    "        **param\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train, eval_set=[(X_valid_scaled, y_valid)], verbose=False)\n",
    "    preds = model.predict(X_valid_scaled)\n",
    "    mse = mean_squared_error(y_valid, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    eval_metric='rmse',  # Use rmse for evaluation\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_best = best_model.predict(X_train_scaled)\n",
    "y_valid_pred_best = best_model.predict(X_valid_scaled)\n",
    "y_test_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print_metrics(y_train, y_train_pred_best, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred_best, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred_best, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f6d725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'xgb_model_with_lstm_paris.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('xgb_model_with_lstm_paris.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1948251",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c38a99",
   "metadata": {},
   "source": [
    "## Without LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf7fb4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Model: 100%|██████████ [time left: 00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  17.4929370422044\n",
      "Mean Squared Error (MSE):  1311.3673833171097\n",
      "R-squared (R2):  0.9264415796233353\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  49.37201673398731\n",
      "Mean Squared Error (MSE):  9836.268809881709\n",
      "R-squared (R2):  0.5032458223241659\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  48.37610460802901\n",
      "Mean Squared Error (MSE):  9203.99066621083\n",
      "R-squared (R2):  0.4606346975461719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'month', 'day', 'year', 'weekday', 'date']\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the data excluding specific features\n",
    "def scale_features(X, excluded_columns):\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    return X_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train_raw, y_train = prepare_data(train_data_paris, 'price')\n",
    "X_valid_raw, y_valid = prepare_data(valid_data_paris, 'price')\n",
    "X_test_raw, y_test = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Apply scaling\n",
    "X_train_scaled, scaler = scale_features(X_train_raw, excluded_columns)\n",
    "X_valid_scaled, _ = scale_features(X_valid_raw, excluded_columns)\n",
    "X_test_scaled, _ = scale_features(X_test_raw, excluded_columns)\n",
    "\n",
    "# Define the Random Forest Regression model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Wrap the fitting process with tqdm for visualization\n",
    "with tqdm(total=1, desc=\"Fitting Model\", bar_format=\"{l_bar}{bar} [time left: {remaining}]\") as pbar:\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    pbar.update()  # Simulate completion\n",
    "\n",
    "# Predict on the training, validation, and test sets\n",
    "y_train_pred = rf.predict(X_train_scaled)\n",
    "y_valid_pred = rf.predict(X_valid_scaled)\n",
    "y_test_pred = rf.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print_metrics(y_train, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce82fca7",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3fc0c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 08:43:17,377] A new study created in memory with name: no-name-c29ec042-eede-423b-8c03-b032a07a265c\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7c3fd1b6144e4d841d57595df83faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 08:43:18,298] Trial 0 finished with value: 11114.892031116997 and parameters: {'n_estimators': 129, 'max_depth': 7, 'min_samples_split': 12, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 0 with value: 11114.892031116997.\n",
      "[I 2024-08-02 08:43:21,118] Trial 1 finished with value: 10633.21114729911 and parameters: {'n_estimators': 258, 'max_depth': 8, 'min_samples_split': 11, 'min_samples_leaf': 11, 'max_features': 'sqrt'}. Best is trial 1 with value: 10633.21114729911.\n",
      "[I 2024-08-02 08:43:24,308] Trial 2 finished with value: 10596.709784132261 and parameters: {'n_estimators': 237, 'max_depth': 11, 'min_samples_split': 5, 'min_samples_leaf': 13, 'max_features': 'sqrt'}. Best is trial 2 with value: 10596.709784132261.\n",
      "[I 2024-08-02 08:43:24,933] Trial 3 finished with value: 11825.403165456048 and parameters: {'n_estimators': 107, 'max_depth': 5, 'min_samples_split': 9, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 2 with value: 10596.709784132261.\n",
      "[I 2024-08-02 08:43:27,969] Trial 4 finished with value: 10992.492284708938 and parameters: {'n_estimators': 288, 'max_depth': 15, 'min_samples_split': 6, 'min_samples_leaf': 13, 'max_features': 'log2'}. Best is trial 2 with value: 10596.709784132261.\n",
      "[I 2024-08-02 08:44:15,251] Trial 5 finished with value: 9970.41179341478 and parameters: {'n_estimators': 329, 'max_depth': 14, 'min_samples_split': 9, 'min_samples_leaf': 14, 'max_features': None}. Best is trial 5 with value: 9970.41179341478.\n",
      "[I 2024-08-02 08:44:18,253] Trial 6 finished with value: 10373.675402559615 and parameters: {'n_estimators': 187, 'max_depth': 14, 'min_samples_split': 9, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 5 with value: 9970.41179341478.\n",
      "[I 2024-08-02 08:44:24,577] Trial 7 finished with value: 10514.879470737858 and parameters: {'n_estimators': 470, 'max_depth': 11, 'min_samples_split': 10, 'min_samples_leaf': 11, 'max_features': 'sqrt'}. Best is trial 5 with value: 9970.41179341478.\n",
      "[I 2024-08-02 08:44:30,040] Trial 8 finished with value: 10224.517150821774 and parameters: {'n_estimators': 403, 'max_depth': 10, 'min_samples_split': 12, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 5 with value: 9970.41179341478.\n",
      "[I 2024-08-02 08:44:34,483] Trial 9 finished with value: 11018.051475943932 and parameters: {'n_estimators': 436, 'max_depth': 13, 'min_samples_split': 5, 'min_samples_leaf': 13, 'max_features': 'log2'}. Best is trial 5 with value: 9970.41179341478.\n",
      "[I 2024-08-02 08:44:48,595] Trial 10 finished with value: 11655.995300157403 and parameters: {'n_estimators': 362, 'max_depth': 3, 'min_samples_split': 15, 'min_samples_leaf': 15, 'max_features': None}. Best is trial 5 with value: 9970.41179341478.\n",
      "[I 2024-08-02 08:45:39,817] Trial 11 finished with value: 9743.0631789102 and parameters: {'n_estimators': 376, 'max_depth': 11, 'min_samples_split': 13, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 11 with value: 9743.0631789102.\n",
      "[I 2024-08-02 08:46:28,141] Trial 12 finished with value: 9793.902960024061 and parameters: {'n_estimators': 361, 'max_depth': 13, 'min_samples_split': 15, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 11 with value: 9743.0631789102.\n",
      "[I 2024-08-02 08:47:12,987] Trial 13 finished with value: 9736.327288257864 and parameters: {'n_estimators': 378, 'max_depth': 12, 'min_samples_split': 15, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 13 with value: 9736.327288257864.\n",
      "[I 2024-08-02 08:48:14,541] Trial 14 finished with value: 9747.565187878175 and parameters: {'n_estimators': 483, 'max_depth': 12, 'min_samples_split': 14, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 13 with value: 9736.327288257864.\n",
      "[I 2024-08-02 08:48:53,479] Trial 15 finished with value: 9930.962379864759 and parameters: {'n_estimators': 421, 'max_depth': 9, 'min_samples_split': 13, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 13 with value: 9736.327288257864.\n",
      "[I 2024-08-02 08:49:20,429] Trial 16 finished with value: 10086.171327257478 and parameters: {'n_estimators': 379, 'max_depth': 7, 'min_samples_split': 14, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 13 with value: 9736.327288257864.\n",
      "[I 2024-08-02 08:49:58,070] Trial 17 finished with value: 9780.781632212053 and parameters: {'n_estimators': 319, 'max_depth': 11, 'min_samples_split': 13, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 13 with value: 9736.327288257864.\n",
      "[I 2024-08-02 08:50:47,171] Trial 18 finished with value: 9900.167371443944 and parameters: {'n_estimators': 446, 'max_depth': 9, 'min_samples_split': 15, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 13 with value: 9736.327288257864.\n",
      "[I 2024-08-02 08:51:25,006] Trial 19 finished with value: 9813.277420459308 and parameters: {'n_estimators': 282, 'max_depth': 12, 'min_samples_split': 13, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 13 with value: 9736.327288257864.\n",
      "[I 2024-08-02 08:51:56,284] Trial 20 finished with value: 9922.07189054689 and parameters: {'n_estimators': 201, 'max_depth': 15, 'min_samples_split': 11, 'min_samples_leaf': 10, 'max_features': None}. Best is trial 13 with value: 9736.327288257864.\n",
      "[I 2024-08-02 08:53:10,809] Trial 21 finished with value: 9721.7618950851 and parameters: {'n_estimators': 499, 'max_depth': 12, 'min_samples_split': 14, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 21 with value: 9721.7618950851.\n",
      "[I 2024-08-02 08:54:00,956] Trial 22 finished with value: 9833.306091127482 and parameters: {'n_estimators': 394, 'max_depth': 10, 'min_samples_split': 14, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 21 with value: 9721.7618950851.\n",
      "[I 2024-08-02 08:55:15,875] Trial 23 finished with value: 9733.71451730608 and parameters: {'n_estimators': 495, 'max_depth': 12, 'min_samples_split': 14, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 21 with value: 9721.7618950851.\n",
      "[I 2024-08-02 08:56:27,979] Trial 24 finished with value: 9853.695597925544 and parameters: {'n_estimators': 473, 'max_depth': 13, 'min_samples_split': 15, 'min_samples_leaf': 8, 'max_features': None}. Best is trial 21 with value: 9721.7618950851.\n",
      "[I 2024-08-02 08:57:41,658] Trial 25 finished with value: 9785.11564212311 and parameters: {'n_estimators': 484, 'max_depth': 12, 'min_samples_split': 7, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 21 with value: 9721.7618950851.\n",
      "[I 2024-08-02 08:58:50,604] Trial 26 finished with value: 9862.240609734838 and parameters: {'n_estimators': 497, 'max_depth': 14, 'min_samples_split': 14, 'min_samples_leaf': 8, 'max_features': None}. Best is trial 21 with value: 9721.7618950851.\n",
      "[I 2024-08-02 08:59:41,041] Trial 27 finished with value: 9730.50421092237 and parameters: {'n_estimators': 451, 'max_depth': 10, 'min_samples_split': 12, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 21 with value: 9721.7618950851.\n",
      "[I 2024-08-02 08:59:44,963] Trial 28 finished with value: 10643.840102104125 and parameters: {'n_estimators': 452, 'max_depth': 10, 'min_samples_split': 12, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 21 with value: 9721.7618950851.\n",
      "[I 2024-08-02 09:00:26,849] Trial 29 finished with value: 9990.292362731378 and parameters: {'n_estimators': 428, 'max_depth': 8, 'min_samples_split': 12, 'min_samples_leaf': 9, 'max_features': None}. Best is trial 21 with value: 9721.7618950851.\n",
      "[I 2024-08-02 09:00:30,858] Trial 30 finished with value: 10810.509308443818 and parameters: {'n_estimators': 500, 'max_depth': 8, 'min_samples_split': 11, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 21 with value: 9721.7618950851.\n",
      "[I 2024-08-02 09:01:40,640] Trial 31 finished with value: 9772.701085355568 and parameters: {'n_estimators': 461, 'max_depth': 12, 'min_samples_split': 14, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 21 with value: 9721.7618950851.\n",
      "[I 2024-08-02 09:02:35,614] Trial 32 finished with value: 9847.071110598688 and parameters: {'n_estimators': 418, 'max_depth': 10, 'min_samples_split': 13, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 21 with value: 9721.7618950851.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:03:46,518] Trial 33 finished with value: 9792.45292254666 and parameters: {'n_estimators': 445, 'max_depth': 13, 'min_samples_split': 15, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 21 with value: 9721.7618950851.\n",
      "[I 2024-08-02 09:03:51,540] Trial 34 finished with value: 10055.509062944077 and parameters: {'n_estimators': 338, 'max_depth': 11, 'min_samples_split': 12, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 21 with value: 9721.7618950851.\n",
      "[I 2024-08-02 09:04:23,022] Trial 35 finished with value: 10301.012173234209 and parameters: {'n_estimators': 403, 'max_depth': 6, 'min_samples_split': 14, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 21 with value: 9721.7618950851.\n",
      "[I 2024-08-02 09:05:34,195] Trial 36 finished with value: 9710.993483478323 and parameters: {'n_estimators': 463, 'max_depth': 12, 'min_samples_split': 10, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 36 with value: 9710.993483478323.\n",
      "[I 2024-08-02 09:05:39,603] Trial 37 finished with value: 10770.088973965458 and parameters: {'n_estimators': 495, 'max_depth': 14, 'min_samples_split': 10, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 36 with value: 9710.993483478323.\n",
      "[I 2024-08-02 09:05:45,000] Trial 38 finished with value: 10368.761154236263 and parameters: {'n_estimators': 458, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 36 with value: 9710.993483478323.\n",
      "[I 2024-08-02 09:06:04,563] Trial 39 finished with value: 9972.623224107043 and parameters: {'n_estimators': 153, 'max_depth': 11, 'min_samples_split': 11, 'min_samples_leaf': 12, 'max_features': None}. Best is trial 36 with value: 9710.993483478323.\n",
      "[I 2024-08-02 09:06:12,805] Trial 40 finished with value: 10110.11831513474 and parameters: {'n_estimators': 474, 'max_depth': 15, 'min_samples_split': 9, 'min_samples_leaf': 6, 'max_features': 'sqrt'}. Best is trial 36 with value: 9710.993483478323.\n",
      "[I 2024-08-02 09:07:16,541] Trial 41 finished with value: 9655.223640180608 and parameters: {'n_estimators': 434, 'max_depth': 12, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 41 with value: 9655.223640180608.\n",
      "[I 2024-08-02 09:08:22,976] Trial 42 finished with value: 9748.133024096807 and parameters: {'n_estimators': 436, 'max_depth': 13, 'min_samples_split': 10, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 41 with value: 9655.223640180608.\n",
      "[I 2024-08-02 09:09:16,020] Trial 43 finished with value: 9927.145842496058 and parameters: {'n_estimators': 465, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 41 with value: 9655.223640180608.\n",
      "[I 2024-08-02 09:10:04,705] Trial 44 finished with value: 9722.485539261204 and parameters: {'n_estimators': 420, 'max_depth': 11, 'min_samples_split': 11, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 41 with value: 9655.223640180608.\n",
      "[I 2024-08-02 09:10:56,392] Trial 45 finished with value: 9907.677912408528 and parameters: {'n_estimators': 409, 'max_depth': 11, 'min_samples_split': 8, 'min_samples_leaf': 10, 'max_features': None}. Best is trial 41 with value: 9655.223640180608.\n",
      "[I 2024-08-02 09:10:58,031] Trial 46 finished with value: 13318.46624191402 and parameters: {'n_estimators': 433, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 41 with value: 9655.223640180608.\n",
      "[I 2024-08-02 09:11:36,162] Trial 47 finished with value: 9744.125887639264 and parameters: {'n_estimators': 260, 'max_depth': 11, 'min_samples_split': 9, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 41 with value: 9655.223640180608.\n",
      "[I 2024-08-02 09:12:21,153] Trial 48 finished with value: 9934.359603831483 and parameters: {'n_estimators': 348, 'max_depth': 10, 'min_samples_split': 11, 'min_samples_leaf': 8, 'max_features': None}. Best is trial 41 with value: 9655.223640180608.\n",
      "[I 2024-08-02 09:12:27,112] Trial 49 finished with value: 10160.90786797181 and parameters: {'n_estimators': 393, 'max_depth': 14, 'min_samples_split': 11, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 41 with value: 9655.223640180608.\n",
      "Best hyperparameters:  {'n_estimators': 434, 'max_depth': 12, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Performance with tuned hyperparameters:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  32.852702817548625\n",
      "Mean Squared Error (MSE):  4635.189965974909\n",
      "R-squared (R2):  0.7399986789511055\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  47.00110622508079\n",
      "Mean Squared Error (MSE):  9736.33141364503\n",
      "R-squared (R2):  0.5082928904804104\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  45.4013487310062\n",
      "Mean Squared Error (MSE):  8639.222742634876\n",
      "R-squared (R2):  0.4937308004175117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'month', 'day', 'year', 'weekday', 'date']\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the data excluding specific features\n",
    "def scale_features(X, excluded_columns):\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    return X_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train_raw, y_train = prepare_data(train_data_paris, 'price')\n",
    "X_valid_raw, y_valid = prepare_data(valid_data_paris, 'price')\n",
    "X_test_raw, y_test = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Apply scaling\n",
    "X_train, scaler = scale_features(X_train_raw, excluded_columns)\n",
    "X_valid, _ = scale_features(X_valid_raw, excluded_columns)\n",
    "X_test, _ = scale_features(X_test_raw, excluded_columns)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 5, 15),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5, 15),\n",
    "        'max_features': trial.suggest_categorical('max_features', [None, 'sqrt', 'log2'])\n",
    "    }\n",
    "\n",
    "    model = RandomForestRegressor(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    mse = mean_squared_error(y_valid, preds)\n",
    "    return mse\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = RandomForestRegressor(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_best = best_model.predict(X_train)\n",
    "y_valid_pred_best = best_model.predict(X_valid)\n",
    "y_test_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with tuned hyperparameters:\")\n",
    "print_metrics(y_train, y_train_pred_best, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred_best, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred_best, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b68d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'rf_model_without_lstm_paris.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('rf_model_without_lstm_paris.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86e5681",
   "metadata": {},
   "source": [
    "## With LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51451e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Model: 100%|██████████ [time left: 00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  17.7464323334496\n",
      "Mean Squared Error (MSE):  1308.3514661749216\n",
      "R-squared (R2):  0.926610751210023\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  49.26198499711483\n",
      "Mean Squared Error (MSE):  9914.545896826314\n",
      "R-squared (R2):  0.4992926495604282\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  47.87127236006925\n",
      "Mean Squared Error (MSE):  9151.117576817658\n",
      "R-squared (R2):  0.4637331263567215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'month', 'day', 'year', 'weekday', 'date']\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the data excluding specific features\n",
    "def scale_features(X, excluded_columns):\n",
    "    # Determine the columns that are numerical and not in excluded columns\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    \n",
    "    # Scale all numerical columns\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    \n",
    "    return X_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train_raw, y_train = prepare_data(train_data_lstm_paris, 'price')\n",
    "X_valid_raw, y_valid = prepare_data(valid_data_lstm_paris, 'price')\n",
    "X_test_raw, y_test = prepare_data(test_data_lstm_paris, 'price')\n",
    "\n",
    "# Apply scaling\n",
    "X_train, scaler = scale_features(X_train_raw, excluded_columns)\n",
    "X_valid, _ = scale_features(X_valid_raw, excluded_columns)\n",
    "X_test, _ = scale_features(X_test_raw, excluded_columns)\n",
    "\n",
    "# Define the Random Forest model with default parameters\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Wrap the fitting process with tqdm for visualization\n",
    "with tqdm(total=1, desc=\"Fitting Model\", bar_format=\"{l_bar}{bar} [time left: {remaining}]\") as pbar:\n",
    "    rf.fit(X_train, y_train)\n",
    "    pbar.update()  # Simulate completion\n",
    "\n",
    "# Predict on the training, validation, and test sets\n",
    "y_train_pred = rf.predict(X_train)\n",
    "y_valid_pred = rf.predict(X_valid)\n",
    "y_test_pred = rf.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print_metrics(y_train, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ebac99",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "790941dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:14:10,766] A new study created in memory with name: no-name-ec9245d6-27f1-4697-ad13-73d7b72bec19\n",
      "[I 2024-08-02 09:14:15,874] Trial 5 finished with value: 12850.534139832887 and parameters: {'n_estimators': 260, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 12, 'max_features': 'log2'}. Best is trial 5 with value: 12850.534139832887.\n",
      "[I 2024-08-02 09:14:16,929] Trial 7 finished with value: 10731.30505984601 and parameters: {'n_estimators': 154, 'max_depth': 11, 'min_samples_split': 5, 'min_samples_leaf': 13, 'max_features': 'log2'}. Best is trial 7 with value: 10731.30505984601.\n",
      "[I 2024-08-02 09:14:18,742] Trial 13 finished with value: 10606.323756492257 and parameters: {'n_estimators': 194, 'max_depth': 13, 'min_samples_split': 5, 'min_samples_leaf': 11, 'max_features': 'log2'}. Best is trial 13 with value: 10606.323756492257.\n",
      "[I 2024-08-02 09:14:19,678] Trial 9 finished with value: 10638.387962787538 and parameters: {'n_estimators': 298, 'max_depth': 5, 'min_samples_split': 15, 'min_samples_leaf': 6, 'max_features': 'sqrt'}. Best is trial 13 with value: 10606.323756492257.\n",
      "[I 2024-08-02 09:14:21,155] Trial 11 finished with value: 10350.88282630968 and parameters: {'n_estimators': 318, 'max_depth': 8, 'min_samples_split': 13, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 11 with value: 10350.88282630968.\n",
      "[I 2024-08-02 09:14:21,298] Trial 4 finished with value: 10587.202384470516 and parameters: {'n_estimators': 263, 'max_depth': 15, 'min_samples_split': 10, 'min_samples_leaf': 12, 'max_features': 'log2'}. Best is trial 11 with value: 10350.88282630968.\n",
      "[I 2024-08-02 09:14:21,558] Trial 3 finished with value: 10749.471121251874 and parameters: {'n_estimators': 299, 'max_depth': 12, 'min_samples_split': 15, 'min_samples_leaf': 15, 'max_features': 'log2'}. Best is trial 11 with value: 10350.88282630968.\n",
      "[I 2024-08-02 09:14:22,232] Trial 18 finished with value: 10368.570711121836 and parameters: {'n_estimators': 129, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 11 with value: 10350.88282630968.\n",
      "[I 2024-08-02 09:14:22,568] Trial 14 finished with value: 10905.03861619165 and parameters: {'n_estimators': 482, 'max_depth': 6, 'min_samples_split': 14, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 11 with value: 10350.88282630968.\n",
      "[I 2024-08-02 09:14:23,081] Trial 2 finished with value: 10095.023760209979 and parameters: {'n_estimators': 331, 'max_depth': 12, 'min_samples_split': 13, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 2 with value: 10095.023760209979.\n",
      "[I 2024-08-02 09:14:24,771] Trial 17 finished with value: 9953.487882669326 and parameters: {'n_estimators': 199, 'max_depth': 13, 'min_samples_split': 9, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 17 with value: 9953.487882669326.\n",
      "[I 2024-08-02 09:14:25,080] Trial 0 finished with value: 10127.71929675558 and parameters: {'n_estimators': 396, 'max_depth': 9, 'min_samples_split': 13, 'min_samples_leaf': 12, 'max_features': 'sqrt'}. Best is trial 17 with value: 9953.487882669326.\n",
      "[I 2024-08-02 09:14:25,153] Trial 1 finished with value: 10149.427594189225 and parameters: {'n_estimators': 343, 'max_depth': 14, 'min_samples_split': 8, 'min_samples_leaf': 14, 'max_features': 'sqrt'}. Best is trial 17 with value: 9953.487882669326.\n",
      "[I 2024-08-02 09:14:56,743] Trial 6 finished with value: 9652.347499466 and parameters: {'n_estimators': 237, 'max_depth': 8, 'min_samples_split': 13, 'min_samples_leaf': 14, 'max_features': None}. Best is trial 6 with value: 9652.347499466.\n",
      "[I 2024-08-02 09:15:13,015] Trial 10 finished with value: 9387.44044533018 and parameters: {'n_estimators': 275, 'max_depth': 9, 'min_samples_split': 15, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 10 with value: 9387.44044533018.\n",
      "[I 2024-08-02 09:15:16,004] Trial 16 finished with value: 9526.078344643593 and parameters: {'n_estimators': 240, 'max_depth': 14, 'min_samples_split': 7, 'min_samples_leaf': 14, 'max_features': None}. Best is trial 10 with value: 9387.44044533018.\n",
      "[I 2024-08-02 09:15:30,357] Trial 8 finished with value: 9161.901164248471 and parameters: {'n_estimators': 266, 'max_depth': 15, 'min_samples_split': 6, 'min_samples_leaf': 8, 'max_features': None}. Best is trial 8 with value: 9161.901164248471.\n",
      "[I 2024-08-02 09:15:57,884] Trial 12 finished with value: 9565.266021843247 and parameters: {'n_estimators': 465, 'max_depth': 12, 'min_samples_split': 9, 'min_samples_leaf': 15, 'max_features': None}. Best is trial 8 with value: 9161.901164248471.\n",
      "[I 2024-08-02 09:15:58,280] Trial 15 finished with value: 9123.858875146147 and parameters: {'n_estimators': 354, 'max_depth': 15, 'min_samples_split': 9, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 15 with value: 9123.858875146147.\n",
      "[I 2024-08-02 09:16:12,783] Trial 19 finished with value: 9372.711610689856 and parameters: {'n_estimators': 493, 'max_depth': 13, 'min_samples_split': 9, 'min_samples_leaf': 9, 'max_features': None}. Best is trial 15 with value: 9123.858875146147.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'n_estimators': 354, 'max_depth': 15, 'min_samples_split': 9, 'min_samples_leaf': 6, 'max_features': None}\n",
      "Performance with tuned hyperparameters:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  31.139846915918223\n",
      "Mean Squared Error (MSE):  4420.620200323899\n",
      "R-squared (R2):  0.7457592146760521\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  46.45966819498845\n",
      "Mean Squared Error (MSE):  9188.813881845686\n",
      "R-squared (R2):  0.5196827802128215\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  45.70153854494596\n",
      "Mean Squared Error (MSE):  8938.643680533045\n",
      "R-squared (R2):  0.48748542525310967\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'month', 'day', 'year', 'weekday', 'date']\n",
    "\n",
    "# Prepare the data by dropping unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the data excluding specific features\n",
    "def scale_features(X, excluded_columns):\n",
    "    # Determine the columns that are numerical and not in excluded columns\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    \n",
    "    # Scale all numerical columns\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    \n",
    "    return X_scaled, scaler\n",
    "\n",
    "# Subsample the data (20%)\n",
    "def subsample_data(data, sample_size=0.2):\n",
    "    return train_test_split(data, test_size=sample_size, random_state=42)[0]\n",
    "\n",
    "# Example subsampling, assuming train_data_lstm_paris, valid_data_lstm_paris, and test_data_lstm_paris are loaded\n",
    "train_data_sub = subsample_data(train_data_lstm_paris)\n",
    "valid_data_sub = subsample_data(valid_data_lstm_paris)\n",
    "test_data_sub = subsample_data(test_data_lstm_paris)\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train_raw, y_train = prepare_data(train_data_sub, 'price')\n",
    "X_valid_raw, y_valid = prepare_data(valid_data_sub, 'price')\n",
    "X_test_raw, y_test = prepare_data(test_data_sub, 'price')\n",
    "\n",
    "# Apply scaling\n",
    "X_train_scaled, scaler = scale_features(X_train_raw, excluded_columns)\n",
    "X_valid_scaled, _ = scale_features(X_valid_raw, excluded_columns)\n",
    "X_test_scaled, _ = scale_features(X_test_raw, excluded_columns)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 5, 15),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5, 15),\n",
    "        'max_features': trial.suggest_categorical('max_features', [None, 'sqrt', 'log2'])\n",
    "    }\n",
    "\n",
    "    model = RandomForestRegressor(**param)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    preds = model.predict(X_valid_scaled)\n",
    "    mse = mean_squared_error(y_valid, preds)\n",
    "    return mse\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "\n",
    "# Optimize using parallelization with Joblib\n",
    "study.optimize(objective, n_trials=20, n_jobs=-1)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = RandomForestRegressor(**best_params)\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_best = best_model.predict(X_train_scaled)\n",
    "y_valid_pred_best = best_model.predict(X_valid_scaled)\n",
    "y_test_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with tuned hyperparameters:\")\n",
    "print_metrics(y_train, y_train_pred_best, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred_best, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred_best, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74376dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'rf_model_with_lstm_paris.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('rf_model_with_lstm_paris.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b66a802",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f66bd8b",
   "metadata": {},
   "source": [
    "## Without LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0dfd00ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Model: 100%|██████████ [time left: 00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  53.9344262295082\n",
      "Mean Squared Error (MSE):  9695.284618067666\n",
      "R-squared (R2):  0.45616321506848356\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  495062801111567.2\n",
      "Mean Squared Error (MSE):  2.454450628802138e+29\n",
      "R-squared (R2):  -1.2395539684027839e+25\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  229951574270175.97\n",
      "Mean Squared Error (MSE):  5.287772650933226e+28\n",
      "R-squared (R2):  -3.098700551324907e+24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'month', 'day', 'year', 'weekday', 'date']\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the data excluding specific features\n",
    "def scale_features(X, excluded_columns):\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    return X_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train_raw, y_train = prepare_data(train_data_paris, 'price')\n",
    "X_valid_raw, y_valid = prepare_data(valid_data_paris, 'price')\n",
    "X_test_raw, y_test = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Apply scaling\n",
    "X_train, scaler = scale_features(X_train_raw, excluded_columns)\n",
    "X_valid, _ = scale_features(X_valid_raw, excluded_columns)\n",
    "X_test, _ = scale_features(X_test_raw, excluded_columns)\n",
    "\n",
    "# Define the Linear Regression model\n",
    "linear_reg = LinearRegression()\n",
    "\n",
    "# Wrap the fitting process with tqdm for visualization\n",
    "with tqdm(total=1, desc=\"Fitting Model\", bar_format=\"{l_bar}{bar} [time left: {remaining}]\") as pbar:\n",
    "    linear_reg.fit(X_train, y_train)\n",
    "    pbar.update()  # Simulate completion\n",
    "\n",
    "# Predict on the training, validation, and test sets\n",
    "y_train_pred = linear_reg.predict(X_train)\n",
    "y_valid_pred = linear_reg.predict(X_valid)\n",
    "y_test_pred = linear_reg.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print_metrics(y_train, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecde530f",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a581262e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:20:18,779] A new study created in memory with name: no-name-48df6739-0a2a-4968-bf15-2dce9f5551ed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e34918024744da83d67021a7dc0dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:20:18,891] Trial 0 finished with value: 2.454450628802138e+29 and parameters: {'fit_intercept': True}. Best is trial 0 with value: 2.454450628802138e+29.\n",
      "[I 2024-08-02 09:20:18,963] Trial 1 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:19,012] Trial 2 finished with value: 2.454450628802138e+29 and parameters: {'fit_intercept': True}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:19,075] Trial 3 finished with value: 2.454450628802138e+29 and parameters: {'fit_intercept': True}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:19,123] Trial 4 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:19,179] Trial 5 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:19,227] Trial 6 finished with value: 2.454450628802138e+29 and parameters: {'fit_intercept': True}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:19,283] Trial 7 finished with value: 2.454450628802138e+29 and parameters: {'fit_intercept': True}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:19,339] Trial 8 finished with value: 2.454450628802138e+29 and parameters: {'fit_intercept': True}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:19,396] Trial 9 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:19,444] Trial 10 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:19,500] Trial 11 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:19,548] Trial 12 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:19,612] Trial 13 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:19,660] Trial 14 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:19,717] Trial 15 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:19,765] Trial 16 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:19,829] Trial 17 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:19,885] Trial 18 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:19,941] Trial 19 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:19,989] Trial 20 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,045] Trial 21 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,101] Trial 22 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,149] Trial 23 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,198] Trial 24 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,255] Trial 25 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,295] Trial 26 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,327] Trial 27 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,375] Trial 28 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,415] Trial 29 finished with value: 2.454450628802138e+29 and parameters: {'fit_intercept': True}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,455] Trial 30 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,495] Trial 31 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,535] Trial 32 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,575] Trial 33 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,623] Trial 34 finished with value: 2.454450628802138e+29 and parameters: {'fit_intercept': True}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,663] Trial 35 finished with value: 2.454450628802138e+29 and parameters: {'fit_intercept': True}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,704] Trial 36 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,752] Trial 37 finished with value: 2.454450628802138e+29 and parameters: {'fit_intercept': True}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,792] Trial 38 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,831] Trial 39 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,871] Trial 40 finished with value: 2.454450628802138e+29 and parameters: {'fit_intercept': True}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,919] Trial 41 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,951] Trial 42 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:20,999] Trial 43 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:21,040] Trial 44 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:21,080] Trial 45 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:21,128] Trial 46 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:21,168] Trial 47 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:21,208] Trial 48 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "[I 2024-08-02 09:20:21,240] Trial 49 finished with value: 4.344243778321403e+21 and parameters: {'fit_intercept': False}. Best is trial 1 with value: 4.344243778321403e+21.\n",
      "Best hyperparameters:  {'fit_intercept': False}\n",
      "Performance with tuned hyperparameters:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  50.147275845731386\n",
      "Mean Squared Error (MSE):  8963.23286244732\n",
      "R-squared (R2):  0.49722613264783244\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  65910853567.69732\n",
      "Mean Squared Error (MSE):  4.344243778321403e+21\n",
      "R-squared (R2):  -2.1939429344950563e+17\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  6553351751.465487\n",
      "Mean Squared Error (MSE):  4.294641917843576e+19\n",
      "R-squared (R2):  -2516713587566265.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'month', 'day', 'year', 'weekday', 'date']\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the data excluding specific features\n",
    "def scale_features(X, excluded_columns):\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    return X_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train_raw, y_train = prepare_data(train_data_paris, 'price')\n",
    "X_valid_raw, y_valid = prepare_data(valid_data_paris, 'price')\n",
    "X_test_raw, y_test = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Apply scaling\n",
    "X_train, scaler = scale_features(X_train_raw, excluded_columns)\n",
    "X_valid, _ = scale_features(X_valid_raw, excluded_columns)\n",
    "X_test, _ = scale_features(X_test_raw, excluded_columns)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    fit_intercept = trial.suggest_categorical('fit_intercept', [True, False])\n",
    "\n",
    "    model = LinearRegression(fit_intercept=fit_intercept)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    mse = mean_squared_error(y_valid, preds)\n",
    "    return mse\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = LinearRegression(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_best = best_model.predict(X_train)\n",
    "y_valid_pred_best = best_model.predict(X_valid)\n",
    "y_test_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with tuned hyperparameters:\")\n",
    "print_metrics(y_train, y_train_pred_best, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred_best, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred_best, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2dfcdc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'linear_model_without_lstm_paris.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('linear_model_without_lstm_paris.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947128e9",
   "metadata": {},
   "source": [
    "## With LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c06bfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Model: 100%|██████████ [time left: 00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  51.00348796651552\n",
      "Mean Squared Error (MSE):  8939.792465992326\n",
      "R-squared (R2):  0.4985409728353858\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  376341217046853.2\n",
      "Mean Squared Error (MSE):  1.4170329042916717e+29\n",
      "R-squared (R2):  -7.156341786875929e+24\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  47320643809986.0\n",
      "Mean Squared Error (MSE):  2.2392433305915661e+27\n",
      "R-squared (R2):  -1.3122244470609199e+23\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'month', 'day', 'year', 'weekday', 'date']\n",
    "\n",
    "# Prepare the data by separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the data excluding specific features\n",
    "def scale_features(X, excluded_columns):\n",
    "    # Determine numerical features not in excluded columns\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    \n",
    "    # Scale all numerical columns\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    \n",
    "    return X_scaled, scaler\n",
    "\n",
    "# Example datasets (replace with actual data)\n",
    "# Assuming train_data_lstm_paris, valid_data_lstm_paris, and test_data_lstm_paris are loaded DataFrames\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train_raw, y_train = prepare_data(train_data_lstm_paris, 'price')\n",
    "X_valid_raw, y_valid = prepare_data(valid_data_lstm_paris, 'price')\n",
    "X_test_raw, y_test = prepare_data(test_data_lstm_paris, 'price')\n",
    "\n",
    "# Apply scaling\n",
    "X_train_scaled, scaler = scale_features(X_train_raw, excluded_columns)\n",
    "X_valid_scaled, _ = scale_features(X_valid_raw, excluded_columns)\n",
    "X_test_scaled, _ = scale_features(X_test_raw, excluded_columns)\n",
    "\n",
    "# Define the Linear Regression model\n",
    "linear_reg = LinearRegression()\n",
    "\n",
    "# Wrap the fitting process with tqdm for visualization\n",
    "with tqdm(total=1, desc=\"Fitting Model\", bar_format=\"{l_bar}{bar} [time left: {remaining}]\") as pbar:\n",
    "    linear_reg.fit(X_train_scaled, y_train)\n",
    "    pbar.update()  # Simulate completion\n",
    "\n",
    "# Predict on the training, validation, and test sets\n",
    "y_train_pred = linear_reg.predict(X_train_scaled)\n",
    "y_valid_pred = linear_reg.predict(X_valid_scaled)\n",
    "y_test_pred = linear_reg.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print_metrics(y_train, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84939f87",
   "metadata": {},
   "source": [
    "#### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9690fe5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:24:18,143] A new study created in memory with name: no-name-686c9972-d671-458a-acf4-60342411612a\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0616f87dde474d1a89d355841f5bb58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:24:18,263] Trial 0 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:18,335] Trial 1 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:18,391] Trial 2 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:18,455] Trial 3 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:18,511] Trial 4 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:18,580] Trial 5 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:18,632] Trial 6 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:18,696] Trial 7 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:18,760] Trial 8 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:18,816] Trial 9 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:18,880] Trial 10 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:18,936] Trial 11 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:19,000] Trial 12 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:19,057] Trial 13 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:19,121] Trial 14 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:19,177] Trial 15 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:19,241] Trial 16 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:19,312] Trial 17 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:19,371] Trial 18 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:19,436] Trial 19 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:19,476] Trial 20 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:19,524] Trial 21 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:19,565] Trial 22 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:19,613] Trial 23 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:19,653] Trial 24 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:19,701] Trial 25 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:19,741] Trial 26 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:19,789] Trial 27 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:19,837] Trial 28 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:19,877] Trial 29 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:19,925] Trial 30 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:19,965] Trial 31 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:20,009] Trial 32 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:20,053] Trial 33 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:20,093] Trial 34 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:20,141] Trial 35 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:20,189] Trial 36 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:20,238] Trial 37 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:20,278] Trial 38 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:20,326] Trial 39 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:20,374] Trial 40 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:20,414] Trial 41 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:20,462] Trial 42 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:20,510] Trial 43 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:20,550] Trial 44 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:20,598] Trial 45 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:20,638] Trial 46 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:20,686] Trial 47 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:20,734] Trial 48 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "[I 2024-08-02 09:24:20,774] Trial 49 finished with value: 1.4170329042916717e+29 and parameters: {}. Best is trial 0 with value: 1.4170329042916717e+29.\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  51.00348796651552\n",
      "Mean Squared Error (MSE):  8939.792465992326\n",
      "R-squared (R2):  0.4985409728353858\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  376341217046853.2\n",
      "Mean Squared Error (MSE):  1.4170329042916717e+29\n",
      "R-squared (R2):  -7.156341786875929e+24\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  47320643809986.0\n",
      "Mean Squared Error (MSE):  2.2392433305915661e+27\n",
      "R-squared (R2):  -1.3122244470609199e+23\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'month', 'day', 'year', 'weekday', 'date']\n",
    "\n",
    "# Prepare the data by dropping unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the data excluding specific features\n",
    "def scale_features(X, excluded_columns):\n",
    "    # Determine numerical features not in excluded columns\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    \n",
    "    # Scale all numerical columns\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    \n",
    "    return X_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets for LSTM data\n",
    "X_train_raw, y_train = prepare_data(train_data_lstm_paris, 'price')\n",
    "X_valid_raw, y_valid = prepare_data(valid_data_lstm_paris, 'price')\n",
    "X_test_raw, y_test = prepare_data(test_data_lstm_paris, 'price')\n",
    "\n",
    "# Apply scaling\n",
    "X_train_scaled, scaler = scale_features(X_train_raw, excluded_columns)\n",
    "X_valid_scaled, _ = scale_features(X_valid_raw, excluded_columns)\n",
    "X_test_scaled, _ = scale_features(X_test_raw, excluded_columns)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    # No hyperparameters to tune for Linear Regression, just using it to follow the structure\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    preds = model.predict(X_valid_scaled)\n",
    "    mse = mean_squared_error(y_valid, preds)\n",
    "    return mse\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Since there are no hyperparameters to tune, we use a default Linear Regression model\n",
    "best_model = LinearRegression()\n",
    "\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_best = best_model.predict(X_train_scaled)\n",
    "y_valid_pred_best = best_model.predict(X_valid_scaled)\n",
    "y_test_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print_metrics(y_train, y_train_pred_best, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred_best, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred_best, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03e4bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'linear_model_with_lstm_paris.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('linear_model_with_lstm_paris.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034e777f",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a548a18",
   "metadata": {},
   "source": [
    "## Without LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d3e7036c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Model: 100%|██████████ [time left: 00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  50.14610111451753\n",
      "Mean Squared Error (MSE):  8972.437860949662\n",
      "R-squared (R2):  0.4967097974407686\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  55.58703037931301\n",
      "Mean Squared Error (MSE):  11099.154447406334\n",
      "R-squared (R2):  0.43946719564237724\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  92.9724953948484\n",
      "Mean Squared Error (MSE):  14355.693829114145\n",
      "R-squared (R2):  0.1587384836774952\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'month', 'day', 'year', 'weekday', 'date']\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the data excluding specific features\n",
    "def scale_features(X, excluded_columns):\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    return X_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train_raw, y_train = prepare_data(train_data_paris, 'price')\n",
    "X_valid_raw, y_valid = prepare_data(valid_data_paris, 'price')\n",
    "X_test_raw, y_test = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Apply scaling\n",
    "X_train, scaler = scale_features(X_train_raw, excluded_columns)\n",
    "X_valid, _ = scale_features(X_valid_raw, excluded_columns)\n",
    "X_test, _ = scale_features(X_test_raw, excluded_columns)\n",
    "\n",
    "# Define the Ridge Regression model\n",
    "ridge_reg = Ridge()\n",
    "\n",
    "# Wrap the fitting process with tqdm for visualization\n",
    "with tqdm(total=1, desc=\"Fitting Model\", bar_format=\"{l_bar}{bar} [time left: {remaining}]\") as pbar:\n",
    "    ridge_reg.fit(X_train, y_train)\n",
    "    pbar.update()  # Simulate completion\n",
    "\n",
    "# Predict on the training, validation, and test sets\n",
    "y_train_pred = ridge_reg.predict(X_train)\n",
    "y_valid_pred = ridge_reg.predict(X_valid)\n",
    "y_test_pred = ridge_reg.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print_metrics(y_train, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377df321",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "732031c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:32:11,200] A new study created in memory with name: no-name-2d49012b-2e4e-4a8d-85d3-7aafc155f0ca\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2ab065d3884667a8ee9d9dde20906a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:32:11,265] Trial 0 finished with value: 11094.530922547714 and parameters: {'alpha': 3.346737911633354, 'fit_intercept': True}. Best is trial 0 with value: 11094.530922547714.\n",
      "[I 2024-08-02 09:32:11,313] Trial 1 finished with value: 11094.340676572021 and parameters: {'alpha': 0.32573156564818534, 'fit_intercept': True}. Best is trial 1 with value: 11094.340676572021.\n",
      "[I 2024-08-02 09:32:11,353] Trial 2 finished with value: 11100.844849220328 and parameters: {'alpha': 1.1934497606859977, 'fit_intercept': False}. Best is trial 1 with value: 11094.340676572021.\n",
      "[I 2024-08-02 09:32:11,393] Trial 3 finished with value: 11096.184342179928 and parameters: {'alpha': 0.363178053450487, 'fit_intercept': False}. Best is trial 1 with value: 11094.340676572021.\n",
      "[I 2024-08-02 09:32:11,434] Trial 4 finished with value: 11100.793466475641 and parameters: {'alpha': 0.9827486535764222, 'fit_intercept': False}. Best is trial 1 with value: 11094.340676572021.\n",
      "[I 2024-08-02 09:32:11,478] Trial 5 finished with value: 11098.191942432773 and parameters: {'alpha': 0.6322434793140028, 'fit_intercept': True}. Best is trial 1 with value: 11094.340676572021.\n",
      "[I 2024-08-02 09:32:11,515] Trial 6 finished with value: 11093.093016590406 and parameters: {'alpha': 4.759542686403127, 'fit_intercept': False}. Best is trial 6 with value: 11093.093016590406.\n",
      "[I 2024-08-02 09:32:11,557] Trial 7 finished with value: 11096.4232676473 and parameters: {'alpha': 2.570577094974363, 'fit_intercept': True}. Best is trial 6 with value: 11093.093016590406.\n",
      "[I 2024-08-02 09:32:11,597] Trial 8 finished with value: 11097.03052030773 and parameters: {'alpha': 3.0983096709261018, 'fit_intercept': False}. Best is trial 6 with value: 11093.093016590406.\n",
      "[I 2024-08-02 09:32:11,630] Trial 9 finished with value: 11100.021512264131 and parameters: {'alpha': 0.6996693334808779, 'fit_intercept': False}. Best is trial 6 with value: 11093.093016590406.\n",
      "[I 2024-08-02 09:32:11,679] Trial 10 finished with value: 11086.508660733229 and parameters: {'alpha': 7.783097012813877, 'fit_intercept': False}. Best is trial 10 with value: 11086.508660733229.\n",
      "[I 2024-08-02 09:32:11,720] Trial 11 finished with value: 11085.51851951769 and parameters: {'alpha': 8.260597344740386, 'fit_intercept': False}. Best is trial 11 with value: 11085.51851951769.\n",
      "[I 2024-08-02 09:32:11,769] Trial 12 finished with value: 11083.88894271653 and parameters: {'alpha': 9.056278358584922, 'fit_intercept': False}. Best is trial 12 with value: 11083.88894271653.\n",
      "[I 2024-08-02 09:32:11,818] Trial 13 finished with value: 11082.633484407897 and parameters: {'alpha': 9.676670013144145, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:11,866] Trial 14 finished with value: 11084.985034170768 and parameters: {'alpha': 8.519805052435117, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:11,906] Trial 15 finished with value: 11087.366856822433 and parameters: {'alpha': 0.14947657555945398, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:11,956] Trial 16 finished with value: 11100.217889234977 and parameters: {'alpha': 1.6970123716261027, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:12,003] Trial 17 finished with value: 11092.147976106146 and parameters: {'alpha': 5.174519987194767, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:12,052] Trial 18 finished with value: 11090.372873368162 and parameters: {'alpha': 5.123054588983026, 'fit_intercept': True}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:12,099] Trial 19 finished with value: 11083.010774915863 and parameters: {'alpha': 0.10084654546219289, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:12,145] Trial 20 finished with value: 11085.807484763993 and parameters: {'alpha': 0.1299165284086893, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:12,193] Trial 21 finished with value: 11093.978773912058 and parameters: {'alpha': 0.2818083356691465, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:12,242] Trial 22 finished with value: 11099.715214997863 and parameters: {'alpha': 1.9511901447961826, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:12,300] Trial 23 finished with value: 11091.334252145554 and parameters: {'alpha': 0.21574162555591958, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:12,351] Trial 24 finished with value: 11083.327210351847 and parameters: {'alpha': 0.10381185604017677, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:12,406] Trial 25 finished with value: 11083.025074632855 and parameters: {'alpha': 0.1009788188603214, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:12,464] Trial 26 finished with value: 11090.47789677928 and parameters: {'alpha': 0.20809668504874815, 'fit_intercept': True}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:12,504] Trial 27 finished with value: 11098.148227927632 and parameters: {'alpha': 0.4780331432782981, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:12,556] Trial 28 finished with value: 11088.198629169758 and parameters: {'alpha': 0.16115485711885832, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:12,607] Trial 29 finished with value: 11083.555031548389 and parameters: {'alpha': 0.10180770604756437, 'fit_intercept': True}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:12,643] Trial 30 finished with value: 11090.206592487637 and parameters: {'alpha': 0.19385387387462566, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:12,683] Trial 31 finished with value: 11084.468283148259 and parameters: {'alpha': 0.11515273971990123, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:12,724] Trial 32 finished with value: 11083.258511762113 and parameters: {'alpha': 0.10316193645439789, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:12,761] Trial 33 finished with value: 11094.215125576653 and parameters: {'alpha': 0.28907685226289753, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:12,802] Trial 34 finished with value: 11087.276443009187 and parameters: {'alpha': 0.1482631592919279, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:12,841] Trial 35 finished with value: 11091.964352780828 and parameters: {'alpha': 0.22936803391814714, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:12,883] Trial 36 finished with value: 11095.09348530817 and parameters: {'alpha': 0.36039259771484294, 'fit_intercept': True}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:12,922] Trial 37 finished with value: 11089.05799569165 and parameters: {'alpha': 0.17429462513394012, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:12,962] Trial 38 finished with value: 11100.846234799363 and parameters: {'alpha': 1.0574956290110813, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:13,003] Trial 39 finished with value: 11097.148446394192 and parameters: {'alpha': 0.5008911154390996, 'fit_intercept': True}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:13,039] Trial 40 finished with value: 11100.401249557059 and parameters: {'alpha': 0.7913791866323246, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:13,079] Trial 41 finished with value: 11083.929316095313 and parameters: {'alpha': 0.10966517668849071, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:32:13,121] Trial 42 finished with value: 11085.658495699592 and parameters: {'alpha': 0.12818830965324587, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:13,159] Trial 43 finished with value: 11083.300278095243 and parameters: {'alpha': 0.10355655830564303, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:13,200] Trial 44 finished with value: 11086.871725906105 and parameters: {'alpha': 0.14295750658924228, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:13,233] Trial 45 finished with value: 11100.644214318063 and parameters: {'alpha': 1.425151762136201, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:13,273] Trial 46 finished with value: 11092.941562519582 and parameters: {'alpha': 0.25290187546573983, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:13,319] Trial 47 finished with value: 11095.589978800492 and parameters: {'alpha': 3.6955393146145457, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:13,356] Trial 48 finished with value: 11089.046096350032 and parameters: {'alpha': 0.17410446009255062, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "[I 2024-08-02 09:32:13,399] Trial 49 finished with value: 11097.517209457332 and parameters: {'alpha': 0.4342887318920486, 'fit_intercept': False}. Best is trial 13 with value: 11082.633484407897.\n",
      "Best hyperparameters:  {'alpha': 9.676670013144145, 'fit_intercept': False}\n",
      "Performance with tuned hyperparameters:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  50.045804605747364\n",
      "Mean Squared Error (MSE):  8984.435353663626\n",
      "R-squared (R2):  0.4960368230906764\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  55.127845686263676\n",
      "Mean Squared Error (MSE):  11082.633484407897\n",
      "R-squared (R2):  0.44030154223734386\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  90.57267465280752\n",
      "Mean Squared Error (MSE):  13919.17860855722\n",
      "R-squared (R2):  0.18431881861044153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'month', 'day', 'year', 'weekday', 'date']\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the data excluding specific features\n",
    "def scale_features(X, excluded_columns):\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    return X_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train_raw, y_train = prepare_data(train_data_paris, 'price')\n",
    "X_valid_raw, y_valid = prepare_data(valid_data_paris, 'price')\n",
    "X_test_raw, y_test = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Apply scaling\n",
    "X_train, scaler = scale_features(X_train_raw, excluded_columns)\n",
    "X_valid, _ = scale_features(X_valid_raw, excluded_columns)\n",
    "X_test, _ = scale_features(X_test_raw, excluded_columns)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    alpha = trial.suggest_float('alpha', 0.1, 10.0, log=True)\n",
    "    fit_intercept = trial.suggest_categorical('fit_intercept', [True, False])\n",
    "\n",
    "    model = Ridge(alpha=alpha, fit_intercept=fit_intercept)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    mse = mean_squared_error(y_valid, preds)\n",
    "    return mse\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = Ridge(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_best = best_model.predict(X_train)\n",
    "y_valid_pred_best = best_model.predict(X_valid)\n",
    "y_test_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with tuned hyperparameters:\")\n",
    "print_metrics(y_train, y_train_pred_best, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred_best, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred_best, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f45b6153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'ridge_model_without_lstm_paris.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('ridge_model_without_lstm_paris.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2419306",
   "metadata": {},
   "source": [
    "## With LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ecd7d2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Model: 100%|██████████ [time left: 00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  50.29217949446014\n",
      "Mean Squared Error (MSE):  8907.158204693262\n",
      "R-squared (R2):  0.5003715237106459\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  56.08656541069187\n",
      "Mean Squared Error (MSE):  11252.815101084097\n",
      "R-squared (R2):  0.43170697953460435\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  92.64900985594313\n",
      "Mean Squared Error (MSE):  14326.068749791848\n",
      "R-squared (R2):  0.1604745501782523\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'month', 'day', 'year', 'weekday', 'date']\n",
    "\n",
    "# Prepare the data by dropping unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the data excluding specific features\n",
    "def scale_features(X, excluded_columns):\n",
    "    # Determine numerical features not in excluded columns\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    \n",
    "    # Scale all numerical columns\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    \n",
    "    return X_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets for LSTM data\n",
    "X_train_raw, y_train = prepare_data(train_data_lstm_paris, 'price')\n",
    "X_valid_raw, y_valid = prepare_data(valid_data_lstm_paris, 'price')\n",
    "X_test_raw, y_test = prepare_data(test_data_lstm_paris, 'price')\n",
    "\n",
    "# Apply scaling\n",
    "X_train_scaled, scaler = scale_features(X_train_raw, excluded_columns)\n",
    "X_valid_scaled, _ = scale_features(X_valid_raw, excluded_columns)\n",
    "X_test_scaled, _ = scale_features(X_test_raw, excluded_columns)\n",
    "\n",
    "# Define the Ridge Regression model\n",
    "ridge = Ridge()\n",
    "\n",
    "# Wrap the fitting process with tqdm for visualization\n",
    "with tqdm(total=1, desc=\"Fitting Model\", bar_format=\"{l_bar}{bar} [time left: {remaining}]\") as pbar:\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "    pbar.update()  # Simulate completion\n",
    "\n",
    "# Predict on the training, validation, and test sets\n",
    "y_train_pred = ridge.predict(X_train_scaled)\n",
    "y_valid_pred = ridge.predict(X_valid_scaled)\n",
    "y_test_pred = ridge.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print_metrics(y_train, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a21c7",
   "metadata": {},
   "source": [
    "#### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe538d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:30:25,061] A new study created in memory with name: no-name-a64bc1e2-bf95-4707-b2db-186880309a01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f35ba0ad9543ef904c7c39cad81509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:30:25,132] Trial 0 finished with value: 11235.589830026598 and parameters: {'alpha': 2.251063215994805}. Best is trial 0 with value: 11235.589830026598.\n",
      "[I 2024-08-02 09:30:25,180] Trial 1 finished with value: 11193.329094417508 and parameters: {'alpha': 7.487150948399996}. Best is trial 1 with value: 11193.329094417508.\n",
      "[I 2024-08-02 09:30:25,228] Trial 2 finished with value: 11258.589523913757 and parameters: {'alpha': 0.1403094126852901}. Best is trial 1 with value: 11193.329094417508.\n",
      "[I 2024-08-02 09:30:25,260] Trial 3 finished with value: 11186.66607128433 and parameters: {'alpha': 8.752468928952034}. Best is trial 3 with value: 11186.66607128433.\n",
      "[I 2024-08-02 09:30:25,292] Trial 4 finished with value: 11259.837950912002 and parameters: {'alpha': 0.20736833837329458}. Best is trial 3 with value: 11186.66607128433.\n",
      "[I 2024-08-02 09:30:25,332] Trial 5 finished with value: 11260.366826166068 and parameters: {'alpha': 0.26166284369053167}. Best is trial 3 with value: 11186.66607128433.\n",
      "[I 2024-08-02 09:30:25,372] Trial 6 finished with value: 11259.873035516799 and parameters: {'alpha': 0.46391448549329917}. Best is trial 3 with value: 11186.66607128433.\n",
      "[I 2024-08-02 09:30:25,404] Trial 7 finished with value: 11259.665702784618 and parameters: {'alpha': 0.1954456573647906}. Best is trial 3 with value: 11186.66607128433.\n",
      "[I 2024-08-02 09:30:25,444] Trial 8 finished with value: 11205.479305950317 and parameters: {'alpha': 5.538130977527186}. Best is trial 3 with value: 11186.66607128433.\n",
      "[I 2024-08-02 09:30:25,476] Trial 9 finished with value: 11235.908580259778 and parameters: {'alpha': 2.2250053106236023}. Best is trial 3 with value: 11186.66607128433.\n",
      "[I 2024-08-02 09:30:25,516] Trial 10 finished with value: 11253.146315639955 and parameters: {'alpha': 0.9777395607003427}. Best is trial 3 with value: 11186.66607128433.\n",
      "[I 2024-08-02 09:30:25,557] Trial 11 finished with value: 11187.218715973459 and parameters: {'alpha': 8.64183825992582}. Best is trial 3 with value: 11186.66607128433.\n",
      "[I 2024-08-02 09:30:25,597] Trial 12 finished with value: 11186.35937646257 and parameters: {'alpha': 8.814318013005767}. Best is trial 12 with value: 11186.35937646257.\n",
      "[I 2024-08-02 09:30:25,645] Trial 13 finished with value: 11221.908980911543 and parameters: {'alpha': 3.5197162108225495}. Best is trial 12 with value: 11186.35937646257.\n",
      "[I 2024-08-02 09:30:25,685] Trial 14 finished with value: 11180.800903568363 and parameters: {'alpha': 9.992405982516965}. Best is trial 14 with value: 11180.800903568363.\n",
      "[I 2024-08-02 09:30:25,725] Trial 15 finished with value: 11218.709789344004 and parameters: {'alpha': 3.8652098193079922}. Best is trial 14 with value: 11180.800903568363.\n",
      "[I 2024-08-02 09:30:25,773] Trial 16 finished with value: 11252.697732352693 and parameters: {'alpha': 1.0078822889721588}. Best is trial 14 with value: 11180.800903568363.\n",
      "[I 2024-08-02 09:30:25,818] Trial 17 finished with value: 11244.350587666533 and parameters: {'alpha': 1.5810189177395506}. Best is trial 14 with value: 11180.800903568363.\n",
      "[I 2024-08-02 09:30:25,861] Trial 18 finished with value: 11209.240246127747 and parameters: {'alpha': 5.01918349030877}. Best is trial 14 with value: 11180.800903568363.\n",
      "[I 2024-08-02 09:30:25,901] Trial 19 finished with value: 11259.34928645979 and parameters: {'alpha': 0.5196668482948594}. Best is trial 14 with value: 11180.800903568363.\n",
      "[I 2024-08-02 09:30:25,949] Trial 20 finished with value: 11224.892222211423 and parameters: {'alpha': 3.215608983649738}. Best is trial 14 with value: 11180.800903568363.\n",
      "[I 2024-08-02 09:30:25,989] Trial 21 finished with value: 11180.767244732619 and parameters: {'alpha': 9.999874851538525}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,029] Trial 22 finished with value: 11181.905305452889 and parameters: {'alpha': 9.749601961560874}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,077] Trial 23 finished with value: 11204.088680615323 and parameters: {'alpha': 5.7395407347839855}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,117] Trial 24 finished with value: 11202.381582171054 and parameters: {'alpha': 5.994092458267345}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,165] Trial 25 finished with value: 11182.477007359512 and parameters: {'alpha': 9.625630225827939}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,205] Trial 26 finished with value: 11214.819861564718 and parameters: {'alpha': 4.314334523095976}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,246] Trial 27 finished with value: 11233.781920638741 and parameters: {'alpha': 2.4015606516826566}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,285] Trial 28 finished with value: 11198.008168977474 and parameters: {'alpha': 6.684438336932724}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,334] Trial 29 finished with value: 11236.730785739526 and parameters: {'alpha': 2.158429174368075}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,374] Trial 30 finished with value: 11246.727952697564 and parameters: {'alpha': 1.41327986817238}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,414] Trial 31 finished with value: 11180.841257190268 and parameters: {'alpha': 9.983456933613919}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,454] Trial 32 finished with value: 11198.437483020809 and parameters: {'alpha': 6.614168290180358}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,494] Trial 33 finished with value: 11182.007472623383 and parameters: {'alpha': 9.727361596306649}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,534] Trial 34 finished with value: 11194.919523117464 and parameters: {'alpha': 7.206618156906062}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,582] Trial 35 finished with value: 11180.82612524424 and parameters: {'alpha': 9.986811991701757}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,614] Trial 36 finished with value: 11212.644562501373 and parameters: {'alpha': 4.580292649823774}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,662] Trial 37 finished with value: 11227.59887858564 and parameters: {'alpha': 2.9538054338090243}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,710] Trial 38 finished with value: 11191.912367786988 and parameters: {'alpha': 7.7438834979384294}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,750] Trial 39 finished with value: 11257.637406456975 and parameters: {'alpha': 0.10120350280666356}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,790] Trial 40 finished with value: 11209.443516270205 and parameters: {'alpha': 4.992174319385159}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,830] Trial 41 finished with value: 11181.420092898077 and parameters: {'alpha': 9.855736637918808}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,878] Trial 42 finished with value: 11193.696207944675 and parameters: {'alpha': 7.421680700771381}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,918] Trial 43 finished with value: 11194.58490024049 and parameters: {'alpha': 7.264974350766149}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:26,966] Trial 44 finished with value: 11200.868730269005 and parameters: {'alpha': 6.22657678695798}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:27,006] Trial 45 finished with value: 11190.034182357667 and parameters: {'alpha': 8.09436242434171}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:27,047] Trial 46 finished with value: 11181.075504673912 and parameters: {'alpha': 9.931624837679532}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:27,087] Trial 47 finished with value: 11258.834894132275 and parameters: {'alpha': 0.5668447843154379}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:27,127] Trial 48 finished with value: 11216.867306216536 and parameters: {'alpha': 4.073827211730745}. Best is trial 21 with value: 11180.767244732619.\n",
      "[I 2024-08-02 09:30:27,167] Trial 49 finished with value: 11204.658283864246 and parameters: {'alpha': 5.656404736318103}. Best is trial 21 with value: 11180.767244732619.\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  50.14040087769615\n",
      "Mean Squared Error (MSE):  8933.572953881983\n",
      "R-squared (R2):  0.49888984340527376\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  55.49765544130991\n",
      "Mean Squared Error (MSE):  11180.767244732619\n",
      "R-squared (R2):  0.4353455618392309\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  90.03333561332668\n",
      "Mean Squared Error (MSE):  13825.93008231174\n",
      "R-squared (R2):  0.18978329824604223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'month', 'day', 'year', 'weekday', 'date']\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the data excluding specific features\n",
    "def scale_features(X, excluded_columns):\n",
    "    # Determine numerical features not in excluded columns\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    \n",
    "    # Scale all numerical columns\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    \n",
    "    return X_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets for LSTM data\n",
    "X_train_raw, y_train = prepare_data(train_data_lstm_paris, 'price')\n",
    "X_valid_raw, y_valid = prepare_data(valid_data_lstm_paris, 'price')\n",
    "X_test_raw, y_test = prepare_data(test_data_lstm_paris, 'price')\n",
    "\n",
    "# Apply scaling\n",
    "X_train_scaled, scaler = scale_features(X_train_raw, excluded_columns)\n",
    "X_valid_scaled, _ = scale_features(X_valid_raw, excluded_columns)\n",
    "X_test_scaled, _ = scale_features(X_test_raw, excluded_columns)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'alpha': trial.suggest_float('alpha', 0.1, 10.0, log=True)\n",
    "    }\n",
    "\n",
    "    model = Ridge(**param)\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    preds = model.predict(X_valid_scaled)\n",
    "    mse = mean_squared_error(y_valid, preds)\n",
    "    return mse\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters and train the final model\n",
    "best_params = study.best_params\n",
    "best_model = Ridge(**best_params)\n",
    "\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_best = best_model.predict(X_train_scaled)\n",
    "y_valid_pred_best = best_model.predict(X_valid_scaled)\n",
    "y_test_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print_metrics(y_train, y_train_pred_best, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred_best, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred_best, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d562a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'ridge_model_with_lstm_paris.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('ridge_model_with_lstm_paris.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517f8bdb",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd331b2",
   "metadata": {},
   "source": [
    "## Without LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13dcc3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Model: 100%|██████████ [time left: 00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  50.30543834579549\n",
      "Mean Squared Error (MSE):  9478.353291174948\n",
      "R-squared (R2):  0.4683315257489571\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  52.829419569466594\n",
      "Mean Squared Error (MSE):  10990.65464715735\n",
      "R-squared (R2):  0.44494668487680433\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  50.54472231663865\n",
      "Mean Squared Error (MSE):  9308.868421544232\n",
      "R-squared (R2):  0.4544887306197011\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'month', 'day', 'year', 'weekday', 'date']\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the data excluding specific features\n",
    "def scale_features(X, excluded_columns):\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    return X_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train_raw, y_train = prepare_data(train_data_paris, 'price')\n",
    "X_valid_raw, y_valid = prepare_data(valid_data_paris, 'price')\n",
    "X_test_raw, y_test = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Apply scaling\n",
    "X_train, scaler = scale_features(X_train_raw, excluded_columns)\n",
    "X_valid, _ = scale_features(X_valid_raw, excluded_columns)\n",
    "X_test, _ = scale_features(X_test_raw, excluded_columns)\n",
    "\n",
    "# Define the Lasso Regression model with a default alpha value\n",
    "lasso_reg = Lasso()\n",
    "\n",
    "# Wrap the fitting process with tqdm for visualization\n",
    "with tqdm(total=1, desc=\"Fitting Model\", bar_format=\"{l_bar}{bar} [time left: {remaining}]\") as pbar:\n",
    "    lasso_reg.fit(X_train, y_train)\n",
    "    pbar.update()  # Simulate completion\n",
    "\n",
    "# Predict on the training, validation, and test sets\n",
    "y_train_pred = lasso_reg.predict(X_train)\n",
    "y_valid_pred = lasso_reg.predict(X_valid)\n",
    "y_test_pred = lasso_reg.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print_metrics(y_train, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d9410a",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b8380fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:33:47,150] A new study created in memory with name: no-name-2fb150ae-3f6b-4517-940d-c7d4eace59a9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48aac95ce6b242a6a0853a07ed394b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:33:47,594] Trial 0 finished with value: 10809.263169951872 and parameters: {'alpha': 0.06482466683077619}. Best is trial 0 with value: 10809.263169951872.\n",
      "[I 2024-08-02 09:33:48,162] Trial 1 finished with value: 10804.516458280583 and parameters: {'alpha': 0.049017718450403015}. Best is trial 1 with value: 10804.516458280583.\n",
      "[I 2024-08-02 09:33:49,247] Trial 2 finished with value: 10820.619732248313 and parameters: {'alpha': 0.023853834247951993}. Best is trial 1 with value: 10804.516458280583.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.112e+07, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:33:50,191] Trial 3 finished with value: 10848.082400172734 and parameters: {'alpha': 0.00010858581932040283}. Best is trial 1 with value: 10804.516458280583.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.113e+07, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:33:51,125] Trial 4 finished with value: 10821.552640477672 and parameters: {'alpha': 0.00013393253973727738}. Best is trial 1 with value: 10804.516458280583.\n",
      "[I 2024-08-02 09:33:51,844] Trial 5 finished with value: 10813.28447228366 and parameters: {'alpha': 0.030101430024695695}. Best is trial 1 with value: 10804.516458280583.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.079e+06, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:33:52,911] Trial 6 finished with value: 10815.677319175606 and parameters: {'alpha': 0.0005254180937850412}. Best is trial 1 with value: 10804.516458280583.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.085e+06, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:33:53,990] Trial 7 finished with value: 10817.277967464936 and parameters: {'alpha': 0.0011498515951615378}. Best is trial 1 with value: 10804.516458280583.\n",
      "[I 2024-08-02 09:33:54,112] Trial 8 finished with value: 10875.34109365557 and parameters: {'alpha': 0.5336761395855574}. Best is trial 1 with value: 10804.516458280583.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.106e+06, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:33:55,146] Trial 9 finished with value: 10815.555719878905 and parameters: {'alpha': 0.00040343922349809754}. Best is trial 1 with value: 10804.516458280583.\n",
      "[I 2024-08-02 09:33:55,269] Trial 10 finished with value: 11059.773792309003 and parameters: {'alpha': 0.38505539648231246}. Best is trial 1 with value: 10804.516458280583.\n",
      "[I 2024-08-02 09:33:55,450] Trial 11 finished with value: 10823.853599060174 and parameters: {'alpha': 0.09036176283451831}. Best is trial 1 with value: 10804.516458280583.\n",
      "[I 2024-08-02 09:33:56,477] Trial 12 finished with value: 10819.357508565188 and parameters: {'alpha': 0.0057820535611989325}. Best is trial 1 with value: 10804.516458280583.\n",
      "[I 2024-08-02 09:33:56,766] Trial 13 finished with value: 10809.873069496101 and parameters: {'alpha': 0.06640815618153328}. Best is trial 1 with value: 10804.516458280583.\n",
      "[I 2024-08-02 09:33:57,604] Trial 14 finished with value: 10819.373972458627 and parameters: {'alpha': 0.007521843131931382}. Best is trial 1 with value: 10804.516458280583.\n",
      "[I 2024-08-02 09:33:57,699] Trial 15 finished with value: 11091.690916156871 and parameters: {'alpha': 0.22307247262702182}. Best is trial 1 with value: 10804.516458280583.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.061e+06, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:33:58,498] Trial 16 finished with value: 10821.74658288955 and parameters: {'alpha': 0.002805167767679536}. Best is trial 1 with value: 10804.516458280583.\n",
      "[I 2024-08-02 09:33:58,632] Trial 17 finished with value: 10838.27068511922 and parameters: {'alpha': 0.10595541997055782}. Best is trial 1 with value: 10804.516458280583.\n",
      "[I 2024-08-02 09:33:59,304] Trial 18 finished with value: 10821.683562156515 and parameters: {'alpha': 0.022461199147311803}. Best is trial 1 with value: 10804.516458280583.\n",
      "[I 2024-08-02 09:33:59,402] Trial 19 finished with value: 10978.225149465101 and parameters: {'alpha': 0.1842826040233184}. Best is trial 1 with value: 10804.516458280583.\n",
      "[I 2024-08-02 09:33:59,721] Trial 20 finished with value: 10803.548746187049 and parameters: {'alpha': 0.04296228039610662}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:33:59,796] Trial 21 finished with value: 10974.707819489367 and parameters: {'alpha': 0.9128376939219491}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:00,119] Trial 22 finished with value: 10803.934765820477 and parameters: {'alpha': 0.04612528674181026}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:00,818] Trial 23 finished with value: 10823.958377072175 and parameters: {'alpha': 0.01506642325002567}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:01,139] Trial 24 finished with value: 10803.710002209835 and parameters: {'alpha': 0.04462340641461279}. Best is trial 20 with value: 10803.548746187049.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.924e+05, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:34:01,940] Trial 25 finished with value: 10821.592051866646 and parameters: {'alpha': 0.0029787774310981218}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:02,639] Trial 26 finished with value: 10818.804146751112 and parameters: {'alpha': 0.011201872047509992}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:02,741] Trial 27 finished with value: 10945.365196989356 and parameters: {'alpha': 0.17077391381821547}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:03,314] Trial 28 finished with value: 10814.94350567641 and parameters: {'alpha': 0.028558583985711455}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:03,623] Trial 29 finished with value: 10803.845467220694 and parameters: {'alpha': 0.04557510328395289}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:03,760] Trial 30 finished with value: 10845.852570441883 and parameters: {'alpha': 0.11302187091393934}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:04,071] Trial 31 finished with value: 10804.078141596416 and parameters: {'alpha': 0.04693170254595016}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:04,397] Trial 32 finished with value: 10803.83008343081 and parameters: {'alpha': 0.04546291778177257}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:05,156] Trial 33 finished with value: 10824.778015815671 and parameters: {'alpha': 0.015920242599919816}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:05,491] Trial 34 finished with value: 10803.694170402478 and parameters: {'alpha': 0.04451500188875798}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:05,599] Trial 35 finished with value: 11104.032104814678 and parameters: {'alpha': 0.31359663863335757}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:06,459] Trial 36 finished with value: 10819.362004007493 and parameters: {'alpha': 0.005417658152690475}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:07,093] Trial 37 finished with value: 10815.658292380966 and parameters: {'alpha': 0.02791524000676807}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:07,260] Trial 38 finished with value: 10813.662529190566 and parameters: {'alpha': 0.07419169543819283}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:07,973] Trial 39 finished with value: 10824.458865412596 and parameters: {'alpha': 0.0155979791506758}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:08,365] Trial 40 finished with value: 10803.730500685404 and parameters: {'alpha': 0.04260296721777182}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:08,855] Trial 41 finished with value: 10807.879931821906 and parameters: {'alpha': 0.036162308927407155}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:08,990] Trial 42 finished with value: 10876.001854379818 and parameters: {'alpha': 0.1348117681606915}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:09,218] Trial 43 finished with value: 10809.887630094958 and parameters: {'alpha': 0.06644799701293337}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:09,939] Trial 44 finished with value: 10824.457021018541 and parameters: {'alpha': 0.017515129788143038}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:10,623] Trial 45 finished with value: 10818.773850707381 and parameters: {'alpha': 0.009379103460792253}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:11,130] Trial 46 finished with value: 10808.129950720131 and parameters: {'alpha': 0.03582919082891979}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:11,394] Trial 47 finished with value: 10807.178214207159 and parameters: {'alpha': 0.058708284708897295}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:11,530] Trial 48 finished with value: 10820.18057151135 and parameters: {'alpha': 0.08537120731135871}. Best is trial 20 with value: 10803.548746187049.\n",
      "[I 2024-08-02 09:34:12,231] Trial 49 finished with value: 10822.487832230468 and parameters: {'alpha': 0.021347774696404608}. Best is trial 20 with value: 10803.548746187049.\n",
      "Best hyperparameters:  {'alpha': 0.04296228039610662}\n",
      "Performance with tuned hyperparameters:\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  49.98291731587792\n",
      "Mean Squared Error (MSE):  8987.785493988878\n",
      "R-squared (R2):  0.4958489039510817\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  53.31251961726104\n",
      "Mean Squared Error (MSE):  10803.548746187049\n",
      "R-squared (R2):  0.4543959628267341\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  87.68067502464835\n",
      "Mean Squared Error (MSE):  13438.289539325287\n",
      "R-squared (R2):  0.2124995162751212\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'month', 'day', 'year', 'weekday', 'date']\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the data excluding specific features\n",
    "def scale_features(X, excluded_columns):\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    return X_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets\n",
    "X_train_raw, y_train = prepare_data(train_data_paris, 'price')\n",
    "X_valid_raw, y_valid = prepare_data(valid_data_paris, 'price')\n",
    "X_test_raw, y_test = prepare_data(test_data_paris, 'price')\n",
    "\n",
    "# Apply scaling\n",
    "X_train, scaler = scale_features(X_train_raw, excluded_columns)\n",
    "X_valid, _ = scale_features(X_valid_raw, excluded_columns)\n",
    "X_test, _ = scale_features(X_test_raw, excluded_columns)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'alpha': trial.suggest_float('alpha', 0.0001, 1.0, log=True)\n",
    "    }\n",
    "\n",
    "    model = Lasso(**param)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    mse = mean_squared_error(y_valid, preds)\n",
    "    return mse\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = Lasso(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_best = best_model.predict(X_train)\n",
    "y_valid_pred_best = best_model.predict(X_valid)\n",
    "y_test_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print(\"Performance with tuned hyperparameters:\")\n",
    "print_metrics(y_train, y_train_pred_best, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred_best, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred_best, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8155aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'lasso_model_without_lstm_paris.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('lasso_model_without_lstm_paris.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1856bdf4",
   "metadata": {},
   "source": [
    "## With LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb2486c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Model: 100%|██████████ [time left: 00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  50.38535458579421\n",
      "Mean Squared Error (MSE):  9447.573925974453\n",
      "R-squared (R2):  0.4700580300933139\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  52.86823962727356\n",
      "Mean Squared Error (MSE):  11020.839248155218\n",
      "R-squared (R2):  0.4434222931651721\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  50.54463623600311\n",
      "Mean Squared Error (MSE):  9271.685560854856\n",
      "R-squared (R2):  0.45666769251016115\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'month', 'day', 'year', 'weekday', 'date']\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the data excluding specific features\n",
    "def scale_features(X, excluded_columns):\n",
    "    # Determine numerical features not in excluded columns\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    \n",
    "    # Scale all numerical columns\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    \n",
    "    return X_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets for LSTM data\n",
    "X_train_raw, y_train = prepare_data(train_data_lstm_paris, 'price')\n",
    "X_valid_raw, y_valid = prepare_data(valid_data_lstm_paris, 'price')\n",
    "X_test_raw, y_test = prepare_data(test_data_lstm_paris, 'price')\n",
    "\n",
    "# Apply scaling\n",
    "X_train_scaled, scaler = scale_features(X_train_raw, excluded_columns)\n",
    "X_valid_scaled, _ = scale_features(X_valid_raw, excluded_columns)\n",
    "X_test_scaled, _ = scale_features(X_test_raw, excluded_columns)\n",
    "\n",
    "# Define the Lasso Regression model\n",
    "lasso = Lasso()\n",
    "\n",
    "# Wrap the fitting process with tqdm for visualization\n",
    "with tqdm(total=1, desc=\"Fitting Model\", bar_format=\"{l_bar}{bar} [time left: {remaining}]\") as pbar:\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "    pbar.update()  # Simulate completion\n",
    "\n",
    "# Predict on the training, validation, and test sets\n",
    "y_train_pred = lasso.predict(X_train_scaled)\n",
    "y_valid_pred = lasso.predict(X_valid_scaled)\n",
    "y_test_pred = lasso.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print_metrics(y_train, y_train_pred, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18b231b",
   "metadata": {},
   "source": [
    "#### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "55d72c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:35:45,540] A new study created in memory with name: no-name-f04bdf37-c455-41b8-ad3f-3cbef5bec440\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1d8bb27a164123ae9d0d9dde1b9573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.965e+06, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:35:46,630] Trial 0 finished with value: 10869.22752189175 and parameters: {'alpha': 0.05186566872092902}. Best is trial 0 with value: 10869.22752189175.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.538e+07, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:35:47,859] Trial 1 finished with value: 10921.473217862655 and parameters: {'alpha': 0.00041062335573671866}. Best is trial 0 with value: 10869.22752189175.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.540e+07, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:35:49,123] Trial 2 finished with value: 10921.412201189889 and parameters: {'alpha': 0.0003545843163208201}. Best is trial 0 with value: 10869.22752189175.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.445e+07, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:35:50,080] Trial 3 finished with value: 10901.161446889017 and parameters: {'alpha': 0.024999458971504657}. Best is trial 0 with value: 10869.22752189175.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.095e+07, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:35:51,115] Trial 4 finished with value: 10912.162476521362 and parameters: {'alpha': 0.010808235204608743}. Best is trial 0 with value: 10869.22752189175.\n",
      "[I 2024-08-02 09:35:51,267] Trial 5 finished with value: 10976.208955075796 and parameters: {'alpha': 0.431312082815869}. Best is trial 0 with value: 10869.22752189175.\n",
      "[I 2024-08-02 09:35:51,829] Trial 6 finished with value: 10888.32258900445 and parameters: {'alpha': 0.10983368196129378}. Best is trial 0 with value: 10869.22752189175.\n",
      "[I 2024-08-02 09:35:52,135] Trial 7 finished with value: 10871.230814027671 and parameters: {'alpha': 0.08826287892694966}. Best is trial 0 with value: 10869.22752189175.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.306e+07, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:35:53,061] Trial 8 finished with value: 10917.827766513537 and parameters: {'alpha': 0.005808238282535255}. Best is trial 0 with value: 10869.22752189175.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.234e+07, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:35:53,817] Trial 9 finished with value: 10895.522264344758 and parameters: {'alpha': 0.02897687291574119}. Best is trial 0 with value: 10869.22752189175.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+07, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:35:54,741] Trial 10 finished with value: 10923.150581563868 and parameters: {'alpha': 0.0020319885728913834}. Best is trial 0 with value: 10869.22752189175.\n",
      "[I 2024-08-02 09:35:54,830] Trial 11 finished with value: 11007.627184561165 and parameters: {'alpha': 0.9240401364019715}. Best is trial 0 with value: 10869.22752189175.\n",
      "[I 2024-08-02 09:35:55,078] Trial 12 finished with value: 10877.258835002871 and parameters: {'alpha': 0.09687130668435386}. Best is trial 0 with value: 10869.22752189175.\n",
      "[I 2024-08-02 09:35:55,231] Trial 13 finished with value: 10901.350170390082 and parameters: {'alpha': 0.12183102614997286}. Best is trial 0 with value: 10869.22752189175.\n",
      "[I 2024-08-02 09:35:55,335] Trial 14 finished with value: 11163.655837805698 and parameters: {'alpha': 0.26049209109173943}. Best is trial 0 with value: 10869.22752189175.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.198e+07, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:35:56,100] Trial 15 finished with value: 10894.638538384374 and parameters: {'alpha': 0.02962673447458337}. Best is trial 0 with value: 10869.22752189175.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.423e+07, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:35:57,072] Trial 16 finished with value: 10924.195977956935 and parameters: {'alpha': 0.0030729539651085246}. Best is trial 0 with value: 10869.22752189175.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.995e+06, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:35:57,905] Trial 17 finished with value: 10870.379400029762 and parameters: {'alpha': 0.048581878436495717}. Best is trial 0 with value: 10869.22752189175.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.489e+07, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:35:58,962] Trial 18 finished with value: 10922.607791593373 and parameters: {'alpha': 0.0015273786263086282}. Best is trial 0 with value: 10869.22752189175.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.090e+07, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:35:59,835] Trial 19 finished with value: 10912.00455310294 and parameters: {'alpha': 0.010938610819101448}. Best is trial 0 with value: 10869.22752189175.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.714e+06, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:36:00,581] Trial 20 finished with value: 10874.506449470375 and parameters: {'alpha': 0.04354016654594001}. Best is trial 0 with value: 10869.22752189175.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.202e+05, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:36:01,540] Trial 21 finished with value: 10866.292667586804 and parameters: {'alpha': 0.07669702718918757}. Best is trial 21 with value: 10866.292667586804.\n",
      "[I 2024-08-02 09:36:01,642] Trial 22 finished with value: 11165.353043504958 and parameters: {'alpha': 0.2570062222960267}. Best is trial 21 with value: 10866.292667586804.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.213e+05, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:36:02,619] Trial 23 finished with value: 10865.924629014722 and parameters: {'alpha': 0.0680419702299028}. Best is trial 23 with value: 10865.924629014722.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.551e+07, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:36:03,424] Trial 24 finished with value: 10960.815363103635 and parameters: {'alpha': 0.00010257443684489034}. Best is trial 23 with value: 10865.924629014722.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.850e+07, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:36:04,205] Trial 25 finished with value: 10912.34845310172 and parameters: {'alpha': 0.016728398790281648}. Best is trial 23 with value: 10865.924629014722.\n",
      "[I 2024-08-02 09:36:04,310] Trial 26 finished with value: 11169.897504953891 and parameters: {'alpha': 0.24809123769241365}. Best is trial 23 with value: 10865.924629014722.\n",
      "[I 2024-08-02 09:36:04,414] Trial 27 finished with value: 10989.092224622626 and parameters: {'alpha': 0.8229009321033285}. Best is trial 23 with value: 10865.924629014722.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.460e+06, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:36:05,334] Trial 28 finished with value: 10866.79242930652 and parameters: {'alpha': 0.06130285273131932}. Best is trial 23 with value: 10865.924629014722.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.805e+05, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:36:06,292] Trial 29 finished with value: 10866.237408642979 and parameters: {'alpha': 0.06517945714123284}. Best is trial 23 with value: 10865.924629014722.\n",
      "[I 2024-08-02 09:36:06,429] Trial 30 finished with value: 10965.461455851464 and parameters: {'alpha': 0.1618868720543296}. Best is trial 23 with value: 10865.924629014722.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.414e+06, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:36:07,325] Trial 31 finished with value: 10866.726439218764 and parameters: {'alpha': 0.06159557247237647}. Best is trial 23 with value: 10865.924629014722.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.718e+06, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:36:08,219] Trial 32 finished with value: 10867.05394972752 and parameters: {'alpha': 0.06011822950520885}. Best is trial 23 with value: 10865.924629014722.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.707e+07, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:36:09,001] Trial 33 finished with value: 10909.02507302917 and parameters: {'alpha': 0.019820769055092956}. Best is trial 23 with value: 10865.924629014722.\n",
      "[I 2024-08-02 09:36:09,113] Trial 34 finished with value: 10927.387891173881 and parameters: {'alpha': 0.4626784883577544}. Best is trial 23 with value: 10865.924629014722.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.269e+07, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:36:10,000] Trial 35 finished with value: 10916.719060695574 and parameters: {'alpha': 0.0066827620690544445}. Best is trial 23 with value: 10865.924629014722.\n",
      "[I 2024-08-02 09:36:10,129] Trial 36 finished with value: 10995.11352283044 and parameters: {'alpha': 0.17554619930102872}. Best is trial 23 with value: 10865.924629014722.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.010e+07, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:36:10,878] Trial 37 finished with value: 10888.076183238802 and parameters: {'alpha': 0.033649603437025576}. Best is trial 23 with value: 10865.924629014722.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.919e+07, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:36:11,618] Trial 38 finished with value: 10911.097093880458 and parameters: {'alpha': 0.015066640223383969}. Best is trial 23 with value: 10865.924629014722.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.820e+05, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:36:12,552] Trial 39 finished with value: 10865.89365879067 and parameters: {'alpha': 0.07305288839126688}. Best is trial 39 with value: 10865.89365879067.\n",
      "[I 2024-08-02 09:36:12,681] Trial 40 finished with value: 10933.094769706864 and parameters: {'alpha': 0.5977607394020548}. Best is trial 39 with value: 10865.89365879067.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.584e+05, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:36:13,690] Trial 41 finished with value: 10865.832381476448 and parameters: {'alpha': 0.07172484372168912}. Best is trial 41 with value: 10865.832381476448.\n",
      "[I 2024-08-02 09:36:14,018] Trial 42 finished with value: 10881.436217987633 and parameters: {'alpha': 0.10213477740776895}. Best is trial 41 with value: 10865.832381476448.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.331e+04, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:36:15,083] Trial 43 finished with value: 10866.559578173426 and parameters: {'alpha': 0.07811888808829202}. Best is trial 41 with value: 10865.832381476448.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.149e+06, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:36:15,859] Trial 44 finished with value: 10884.432291976918 and parameters: {'alpha': 0.0360019956498177}. Best is trial 41 with value: 10865.832381476448.\n",
      "[I 2024-08-02 09:36:15,996] Trial 45 finished with value: 10982.812950501471 and parameters: {'alpha': 0.17009760125895296}. Best is trial 41 with value: 10865.832381476448.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.471e+07, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:36:16,791] Trial 46 finished with value: 10901.910445018853 and parameters: {'alpha': 0.024501682005368845}. Best is trial 41 with value: 10865.832381476448.\n",
      "[I 2024-08-02 09:36:16,895] Trial 47 finished with value: 11133.255473505438 and parameters: {'alpha': 0.3396636415610627}. Best is trial 41 with value: 10865.832381476448.\n",
      "[I 2024-08-02 09:36:17,063] Trial 48 finished with value: 10914.953733510687 and parameters: {'alpha': 0.13230734329593635}. Best is trial 41 with value: 10865.832381476448.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.480e+04, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-02 09:36:18,104] Trial 49 finished with value: 10866.548853239987 and parameters: {'alpha': 0.0780757127644565}. Best is trial 41 with value: 10865.832381476448.\n",
      "Training Performance Metrics:\n",
      "Mean Absolute Error (MAE):  50.02148988294239\n",
      "Mean Squared Error (MSE):  8961.977437784955\n",
      "R-squared (R2):  0.49729655307786613\n",
      "\n",
      "Validation Performance Metrics:\n",
      "Mean Absolute Error (MAE):  53.46785004222779\n",
      "Mean Squared Error (MSE):  10865.832381476448\n",
      "R-squared (R2):  0.45125049612296075\n",
      "\n",
      "Test Performance Metrics:\n",
      "Mean Absolute Error (MAE):  83.49933684603104\n",
      "Mean Squared Error (MSE):  12747.109230497052\n",
      "R-squared (R2):  0.2530035421744379\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.584e+05, tolerance: 1.022e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Excluded columns for scaling\n",
    "excluded_columns = ['id', 'host_id', 'longitude', 'latitude', 'month', 'day', 'year', 'weekday', 'date']\n",
    "\n",
    "# Prepare the data by dropping the unnecessary columns and separating features and target\n",
    "def prepare_data(data, target_column):\n",
    "    X = data.drop(columns=['id', 'host_id', 'date', target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Scale the data excluding specific features\n",
    "def scale_features(X, excluded_columns):\n",
    "    # Determine numerical features not in excluded columns\n",
    "    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.difference(excluded_columns)\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = X.copy()\n",
    "    \n",
    "    # Scale all numerical columns\n",
    "    X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    \n",
    "    return X_scaled, scaler\n",
    "\n",
    "# Prepare the training, validation, and test sets for LSTM data\n",
    "X_train_raw, y_train = prepare_data(train_data_lstm_paris, 'price')\n",
    "X_valid_raw, y_valid = prepare_data(valid_data_lstm_paris, 'price')\n",
    "X_test_raw, y_test = prepare_data(test_data_lstm_paris, 'price')\n",
    "\n",
    "# Apply scaling\n",
    "X_train_scaled, scaler = scale_features(X_train_raw, excluded_columns)\n",
    "X_valid_scaled, _ = scale_features(X_valid_raw, excluded_columns)\n",
    "X_test_scaled, _ = scale_features(X_test_raw, excluded_columns)\n",
    "\n",
    "# Function to optimize using Optuna\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'alpha': trial.suggest_float('alpha', 0.0001, 1.0, log=True)\n",
    "    }\n",
    "\n",
    "    model = Lasso(**param)\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    preds = model.predict(X_valid_scaled)\n",
    "    mse = mean_squared_error(y_valid, preds)\n",
    "    return mse\n",
    "\n",
    "# Create a study\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters and train the final model\n",
    "best_params = study.best_params\n",
    "best_model = Lasso(**best_params)\n",
    "\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the training, validation, and test sets with the best model\n",
    "y_train_pred_best = best_model.predict(X_train_scaled)\n",
    "y_valid_pred_best = best_model.predict(X_valid_scaled)\n",
    "y_test_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"{dataset_name} Performance Metrics:\")\n",
    "    print(\"Mean Absolute Error (MAE): \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Mean Squared Error (MSE): \", mean_squared_error(y_true, y_pred))\n",
    "    print(\"R-squared (R2): \", r2_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Print metrics for training, validation, and test data\n",
    "print_metrics(y_train, y_train_pred_best, \"Training\")\n",
    "print_metrics(y_valid, y_valid_pred_best, \"Validation\")\n",
    "print_metrics(y_test, y_test_pred_best, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d8cc86ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'lasso_model_with_lstm_paris.joblib')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('lasso_model_with_lstm_paris.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
