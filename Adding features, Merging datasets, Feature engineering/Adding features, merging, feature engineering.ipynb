{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23eaeab8",
   "metadata": {},
   "source": [
    "# Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69d7aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#insert file_path for the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/Data Cleaning Listings/cleaned_dataset.xlsx'\n",
    "#read data, and quickly check if it is correctly read in\n",
    "dataframe1= pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb6ff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "#reading in the data\n",
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Data/2. airbnb_data/Paris/Q1 airbnb_data Paris/calendar/calendar.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed27bfe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "      <th>available</th>\n",
       "      <th>price</th>\n",
       "      <th>adjusted_price</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>maximum_nights</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5396</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>f</td>\n",
       "      <td>123.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5396</td>\n",
       "      <td>2023-03-15</td>\n",
       "      <td>f</td>\n",
       "      <td>123.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5396</td>\n",
       "      <td>2023-03-16</td>\n",
       "      <td>f</td>\n",
       "      <td>124.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5396</td>\n",
       "      <td>2023-03-17</td>\n",
       "      <td>f</td>\n",
       "      <td>124.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5396</td>\n",
       "      <td>2023-03-18</td>\n",
       "      <td>f</td>\n",
       "      <td>124.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id       date available  price  adjusted_price  minimum_nights  \\\n",
       "0        5396 2023-03-14         f  123.0           123.0             1.0   \n",
       "1        5396 2023-03-15         f  123.0           123.0             1.0   \n",
       "2        5396 2023-03-16         f  124.0           124.0             1.0   \n",
       "3        5396 2023-03-17         f  124.0           124.0             1.0   \n",
       "4        5396 2023-03-18         f  124.0           124.0             1.0   \n",
       "\n",
       "   maximum_nights  weekday  \n",
       "0          1125.0        1  \n",
       "1          1125.0        2  \n",
       "2          1125.0        3  \n",
       "3          1125.0        4  \n",
       "4          1125.0        5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['price'] = data['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "data['adjusted_price'] = data['adjusted_price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "data['date']=pd.to_datetime(data['date'])\n",
    "data['weekday'] = pd.Series(data.date).dt.dayofweek\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d802d7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listing_id        0.0\n",
      "date              0.0\n",
      "available         0.0\n",
      "price             0.0\n",
      "adjusted_price    0.0\n",
      "minimum_nights    0.0\n",
      "maximum_nights    0.0\n",
      "weekday           0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#removing the missing values from the dataset after seeing that there are not that many missing values\n",
    "data.dropna(inplace=True)\n",
    "print(data.isnull().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d882c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the columns for which you want to calculate z-scores\n",
    "columns_of_interest = ['price']\n",
    "\n",
    "# Calculate z-scores for the specified columns\n",
    "z_scores = (data[columns_of_interest] - data[columns_of_interest].mean()) / data[columns_of_interest].std()\n",
    "\n",
    "# Define a threshold for the z-score (e.g., 3)\n",
    "threshold = 3\n",
    "\n",
    "# Filter out rows where any z-score exceeds the threshold\n",
    "data = data[(np.abs(z_scores) < threshold).all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69950585",
   "metadata": {},
   "source": [
    "# Adding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25ec6b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import great_circle\n",
    "\n",
    "# Define the monuments and their coordinates\n",
    "monuments = {\n",
    "    \"Eiffel Tower\": (48.8584, 2.2945),\n",
    "    \"Louvre Museum\": (48.8606, 2.3376),\n",
    "    \"Notre-Dame Cathedral\": (48.8529, 2.3500),\n",
    "    \"Sacré-Cœur Basilica\": (48.8867, 2.3431),\n",
    "    \"Arc de Triomphe\": (48.8738, 2.2950)\n",
    "}\n",
    "\n",
    "# Calculate distances and add as new columns\n",
    "for monument, coords in monuments.items():\n",
    "    dataframe1[monument + ' Distance'] = dataframe1.apply(lambda row: great_circle((row['latitude'], row['longitude']), coords).kilometers,axis=1)\n",
    "\n",
    "# Now data_copy_clean has new columns with distances to each monument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7acc7143",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/france_holidays.csv'\n",
    "holidays_data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert the 'date' column in both datasets to datetime objects\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "holidays_data['date'] = pd.to_datetime(holidays_data['date'])\n",
    "\n",
    "# Merge the Airbnb data with the public holidays data based on the 'date' column\n",
    "data['is_holiday'] = data['date'].isin(holidays_data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a7a8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/anton/Documents/2de_master/Thesis/Code/analysis/Code/Airbnb_Spatiotemporal_Analysis/Paris Airbnb_Spatiotemporal_Analysis/paris_school_holidays.csv'\n",
    "school_holidays_data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert the 'date' column in both datasets to datetime objects\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "school_holidays_data['date'] = pd.to_datetime(school_holidays_data['date'])\n",
    "\n",
    "# Merge the Airbnb data with the public holidays data based on the 'date' column\n",
    "data['is_school_holiday'] = data['date'].isin(school_holidays_data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8db5761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopandas in c:\\users\\anton\\anaconda3\\lib\\site-packages (0.14.2)\n",
      "Requirement already satisfied: fiona>=1.8.21 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from geopandas) (1.9.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\anton\\anaconda3\\lib\\site-packages (from geopandas) (23.1)\n",
      "Requirement already satisfied: pandas>=1.4.0 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from geopandas) (2.0.3)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from geopandas) (3.6.1)\n",
      "Requirement already satisfied: shapely>=1.8.0 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from geopandas) (2.0.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from fiona>=1.8.21->geopandas) (22.1.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\anton\\anaconda3\\lib\\site-packages (from fiona>=1.8.21->geopandas) (2024.2.2)\n",
      "Requirement already satisfied: click~=8.0 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from fiona>=1.8.21->geopandas) (8.0.4)\n",
      "Requirement already satisfied: click-plugins>=1.0 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from fiona>=1.8.21->geopandas) (1.1.1)\n",
      "Requirement already satisfied: cligj>=0.5 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from fiona>=1.8.21->geopandas) (0.7.2)\n",
      "Requirement already satisfied: six in c:\\users\\anton\\anaconda3\\lib\\site-packages (from fiona>=1.8.21->geopandas) (1.16.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\anton\\anaconda3\\lib\\site-packages (from fiona>=1.8.21->geopandas) (68.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from pandas>=1.4.0->geopandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from pandas>=1.4.0->geopandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from pandas>=1.4.0->geopandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from pandas>=1.4.0->geopandas) (1.24.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\anton\\anaconda3\\lib\\site-packages (from click~=8.0->fiona>=1.8.21->geopandas) (0.4.6)\n",
      "Requirement already satisfied: rtree in c:\\users\\anton\\anaconda3\\lib\\site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install geopandas\n",
    "!pip install rtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e49bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Assuming 'data' is your original DataFrame with latitude and longitude\n",
    "# First, ensure 'data' is loaded correctly, e.g., data = pd.read_excel('path/to/your/data.xlsx')\n",
    "\n",
    "# Convert DataFrame to GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(dataframe1, geometry=gpd.points_from_xy(dataframe1.longitude, dataframe1.latitude))\n",
    "\n",
    "# Create a spatial index for the GeoDataFrame\n",
    "sindex = gdf.sindex\n",
    "\n",
    "def count_nearby_listings(row, gdf, radius_meters=500):\n",
    "    # Convert the radius in meters to degrees (approximation)\n",
    "    # Note: This approximation works well for small distances (e.g., within a few kilometers)\n",
    "    radius_degrees = radius_meters / 111320  # Rough conversion factor for degrees to meters\n",
    "    \n",
    "    # Find possible matches with rtree index, which returns indices of possible matches\n",
    "    possible_matches_index = list(sindex.intersection(row.geometry.buffer(radius_degrees).bounds))\n",
    "    \n",
    "    # Filter actual matches from possible matches\n",
    "    possible_matches = gdf.iloc[possible_matches_index]\n",
    "    actual_matches = possible_matches[possible_matches.geometry.distance(row.geometry) <= radius_degrees]\n",
    "    \n",
    "    # Exclude the row itself from the count\n",
    "    nearby_count = actual_matches.shape[0] - 1\n",
    "    return nearby_count\n",
    "\n",
    "# Apply the function to count nearby listings for each listing\n",
    "gdf['nearby_airbnbs_count'] = gdf.apply(count_nearby_listings, axis=1, gdf=gdf)\n",
    "\n",
    "# If you want to work with a regular DataFrame (excluding the geometry column)\n",
    "dataframe_clean = pd.DataFrame(gdf.drop(columns='geometry'))\n",
    "\n",
    "# Display the first few rows of the updated DataFrame to verify the new column\n",
    "#print(dataframe_clean.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34891b21",
   "metadata": {},
   "source": [
    "# Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db65e8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have loaded your calendar dataset into a DataFrame named 'calendar_data'\n",
    "# Let's assume your 'calendar_data' has columns: 'listing_id', 'date', 'price', 'availability'\n",
    "\n",
    "\n",
    "# Extract season from date\n",
    "def get_season(month):\n",
    "    if month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Autumn'\n",
    "    else:\n",
    "        return 'Winter'\n",
    "\n",
    "data['season'] = data['date'].dt.month.map(get_season)\n",
    "\n",
    "# Group by listing_id and season, and randomly select one row from each group\n",
    "simplified_calendar_data = data.groupby(['listing_id', 'season']).apply(lambda x: x.sample(1)).reset_index(drop=True)\n",
    "\n",
    "# Now 'simplified_calendar_data' contains only one random day per season for each listing_id\n",
    "# Now you can use 'simplified_calendar_data' for further analysis\n",
    "\n",
    "#lastly we also drop the minimum_nights and maximum nights\n",
    "columns_to_drop = [\"minimum_nights\" , \"maximum_nights\", \"available\",\"adjusted_price\"]\n",
    "simplified_calendar_data = simplified_calendar_data.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c785d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming so that both datasets have the same column name\n",
    "simplified_calendar_data.rename(columns={'listing_id': 'id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d6739f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have two DataFrames: simplified_calendar_data and listings_data\n",
    "# Let's assume 'dataframe1' has columns: 'listing_id', and other relevant features\n",
    "\n",
    "# Merge the two datasets on 'id'\n",
    "merged_data = pd.merge(dataframe_clean, simplified_calendar_data, on='id', how='inner')\n",
    "\n",
    "# Now 'merged_data' contains the combined information from both datasets\n",
    "# You can proceed with your analysis using this merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cff3deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data.drop('price_x', axis=1)\n",
    "merged_data.rename(columns={'price_y': 'price'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d070275e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     467\n",
       "1     467\n",
       "2     467\n",
       "3     467\n",
       "4    1082\n",
       "5    1082\n",
       "6    1082\n",
       "7    1082\n",
       "8     743\n",
       "9     743\n",
       "Name: nearby_airbnbs_count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data[\"nearby_airbnbs_count\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f059025",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfe4966f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with NaN values: ['first_review', 'last_review', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'reviews_per_month', 'days_since_first_review', 'days_since_first_first_review', 'days_since_last_review', 'days_since_last_last_review']\n"
     ]
    }
   ],
   "source": [
    "# Creating a copy of the dataset to work with\n",
    "data_copy = merged_data.copy()\n",
    "#looking into which columns currently still have empty values\n",
    "columns_with_nan = data_copy.columns[data_copy.isna().any()].tolist()\n",
    "# Print the columns\n",
    "print(\"Columns with NaN values:\", columns_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e3207d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"host_id\", \"days_since_host_host\",\"host_since\"]\n",
    "data_copy = data_copy.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c98ba004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to many missing values in these columns, thus removing them\n",
    "columns_to_drop = ['first_review', 'last_review','days_since_first_review', 'days_since_first_first_review', 'days_since_last_review', \n",
    "                   'days_since_last_last_review']\n",
    "\n",
    "#filling in 0 in the fields where nothing is filled in\n",
    "columns_to_fill = ['review_scores_rating', 'review_scores_accuracy', \n",
    "                   'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', \n",
    "                   'review_scores_location', 'review_scores_value', 'reviews_per_month']\n",
    "\n",
    "# Fill NaN values with 0 in the specified columns\n",
    "for column in columns_to_fill:\n",
    "    data_copy[column] = data_copy[column].fillna(0)\n",
    "\n",
    "data_copy = data_copy.drop(columns_to_drop, axis=1)\n",
    "\n",
    "#looking into which columns currently still have empty values\n",
    "columns_with_nan = data_copy.columns[data_copy.isna().any()].tolist()\n",
    "# Print the columns\n",
    "#print(\"Columns with NaN values:\", columns_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f4322c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Splitting the columns up in one-hot and label encoding\n",
    "categorical_columns_one_hot = ['neighbourhood', 'property_type', 'room_type','season']  # For one-hot encoding\n",
    "categorical_columns_label = ['host_response_time', 'host_response_rate']  # For label encoding\n",
    "\n",
    "# One-Hot Encoding\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(data_copy[categorical_columns_one_hot])\n",
    "\n",
    "# Manually create feature names for the one-hot encoded columns\n",
    "one_hot_feature_names = []\n",
    "for i, column in enumerate(categorical_columns_one_hot):\n",
    "    categories = one_hot_encoder.categories_[i]\n",
    "    one_hot_feature_names.extend([f\"{column}_{category}\" for category in categories])\n",
    "\n",
    "one_hot_encoded_df = pd.DataFrame(one_hot_encoded, columns=one_hot_feature_names, index=data_copy.index)\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_columns_label:\n",
    "    data_copy[col] = label_encoder.fit_transform(data_copy[col])\n",
    "\n",
    "# Concatenate the one-hot encoded columns back to the original dataframe\n",
    "data_copy = pd.concat([data_copy, one_hot_encoded_df], axis=1)\n",
    "\n",
    "# Drop the original string columns\n",
    "data_copy.drop(categorical_columns_one_hot, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2d7f59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'date' is the name of your date column\n",
    "data_copy['year'] = pd.to_datetime(data_copy['date']).dt.year\n",
    "data_copy['month'] = pd.to_datetime(data_copy['date']).dt.month\n",
    "data_copy['day'] = pd.to_datetime(data_copy['date']).dt.day\n",
    "data_copy['day_of_year'] = pd.to_datetime(data_copy['date']).dt.dayofyear\n",
    "\n",
    "#drop the date column then\n",
    "data_copy = data_copy.drop('date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9904e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns with exclusively 0s and 1s to boolean\n",
    "for col in data_copy.columns:\n",
    "    unique_values = data_copy[col].unique()\n",
    "    if set(unique_values).issubset({0, 1}):\n",
    "        data_copy[col] = data_copy[col].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac894b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159831 entries, 0 to 159830\n",
      "Columns: 179 entries, id to day_of_year\n",
      "dtypes: bool(133), float64(17), int32(7), int64(22)\n",
      "memory usage: 72.1 MB\n"
     ]
    }
   ],
   "source": [
    "data_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d28a4bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading the cleaned_dataset\n",
    "data_copy.to_excel('Merged_Engineerd_Dataset.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
